{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MahdiRahbar/NLP_SLU/blob/main/sentiment/WelshSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "44XAmeUWJ--p"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from typing import List\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(\"..\",\"data\",\"train-v2.tsv\")\n",
    "handle = open(fn, \"r\")\n",
    "res = list()\n",
    "tweets = list()\n",
    "for i in handle:\n",
    "    entry = i.strip().split(\"\\t\")\n",
    "    res.append(int(entry[0]))\n",
    "    tweets.append(entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i tried\n",
    "def reformat_tweet(tweet):\n",
    "    #get rid of mentions, links, hashtags and numbers\n",
    "    tweet2 = re.sub(\"(@USER|\\{URL\\}|#\\w*|[0-9]+)\", \"\", tweet).strip()\n",
    "    #lowercase everything\n",
    "    tweet2 =  tweet2.lower()\n",
    "    #tokenize\n",
    "    tweet2 = tweet2.split(\" \")\n",
    "    \n",
    "    clean_tweet = []\n",
    "    for word in tweet2:\n",
    "        #keep only face emojis and alphas\n",
    "        relevant_piece = re.match(\"[\\w\\U00010000-\\U0010ffff]+\", word)\n",
    "        if relevant_piece:\n",
    "            clean_tweet.append(relevant_piece.group(0))\n",
    "    return clean_tweet\n",
    "\n",
    "def make_bag_of_words(tweets: List[list]): # list of list here\n",
    "    bag_of_words = defaultdict(int)  # key-word, val-count\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            bag_of_words[word] += 1\n",
    "    return bag_of_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: pseudocode from the book\n",
    "class NaiveBayes():\n",
    "    def __init__(self, xVec: List[list], yVec: list, vocabs: defaultdict(int)):\n",
    "        self.xVec = xVec #assuming xVec = yVec no missing data\n",
    "        self.yVec = yVec\n",
    "        self.classes = set(yVec)\n",
    "\n",
    "        self.vocabs = vocabs #frequency list of all words appear in all docs\n",
    "        self.prior = defaultdict(float) #dictionary holds prior probability for all classes\n",
    "        self.log_like = defaultdict(lambda: defaultdict(float)) #dictionary holds log likelihood of a word, separated by class\n",
    "\n",
    "    def _sep_by_class(self):\n",
    "        big_doc = defaultdict(lambda: defaultdict(int))\n",
    "        for i in range( len(self.xVec)) :\n",
    "            c = self.yVec[i]\n",
    "            for w in self.xVec[i]:\n",
    "                big_doc[c][w] += 1\n",
    "        return big_doc #big doc contains word count separated by class {c_1: {word1: count, word2:count,etc}, c_2: {word1: count, etc},...}\n",
    "\n",
    "    def train(self, transform_func=math.log, smoothing=lambda x, y: (x/y) ):\n",
    "        N_all = len(self.xVec)\n",
    "        big_doc = self._sep_by_class()\n",
    "\n",
    "        for c in self.classes:\n",
    "            N_c = sum(1 for i in self.yVec if i == c)\n",
    "            self.prior[c] = transform_func(N_c / N_all) #calculate prior\n",
    "\n",
    "            words_in_c = big_doc[c]\n",
    "            count_all_words_in_c = sum(words_in_c[w] for w in self.vocabs) #all words in a class\n",
    "\n",
    "            for word in self.vocabs:\n",
    "                count_word_in_c = words_in_c[word]\n",
    "                \n",
    "                # calculate likelihood\n",
    "\n",
    "                self.log_like[word][c] = transform_func(smoothing(\n",
    "                    count_word_in_c, count_all_words_in_c))\n",
    "                    \n",
    "    def test(self,tweet, impute_for_never_seen=0):\n",
    "        bestClass = None\n",
    "        bestLogPrior = None\n",
    "        for c in self.classes:\n",
    "            probability = self.prior[c]\n",
    "            for w in tweet:\n",
    "                if w in self.vocabs:\n",
    "                    probability += self.log_like[w][c]\n",
    "                else:\n",
    "                    if impute_for_never_seen:\n",
    "                        probability = impute_for_never_seen\n",
    "            if bestLogPrior == None:\n",
    "                bestLogPrior = probability\n",
    "                bestClass = c\n",
    "            elif bestLogPrior < probability:\n",
    "                bestLogPrior = probability\n",
    "                bestClass = c\n",
    "        return bestClass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#divide train-test\n",
    "import random\n",
    "tweets_reformatted = list(map(lambda x: reformat_tweet(x), tweets))\n",
    "random.seed(42)\n",
    "train_indexes = random.sample(range(80000), 64000)\n",
    "test_indexes = [i for i in range(80000) if i not in train_indexes]\n",
    "trainX = [tweets_reformatted[i] for i in train_indexes]\n",
    "trainY = [res[i] for i in train_indexes]\n",
    "vocabs = make_bag_of_words(trainX)\n",
    "testX = [tweets_reformatted[i] for i in test_indexes]\n",
    "testY = [res[i] for i in test_indexes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#smoothing add 1. im just experimenting some stuff here.\n",
    "def smoothing_add_alpha_prob(c_w, count_all_words, alpha = 1): \n",
    "    return (c_w+alpha) * (count_all_words+len(vocabs))\n",
    "nb = NaiveBayes(trainX, trainY, vocabs)\n",
    "nb.train(smoothing = smoothing_add_alpha_prob)\n",
    "#sum([nb.test(trainX[i]) == trainY[i] for i in range(len(trainX))])/len(trainX)\n",
    "sum([nb.test(testX[i]) == testY[i] for i in range(len(testX))])/len(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.723375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def smoothing_add_alpha_prob(c_w, count_all_words, alpha=1):\n",
    "    return (c_w+alpha) * (count_all_words+len(vocabs))\n",
    "\n",
    "\n",
    "nb3 = NaiveBayes(trainX, trainY, vocabs)\n",
    "nb3.train(smoothing=lambda x, y: smoothing_add_alpha_prob(x, y, alpha=0.4))\n",
    "#sum([nb.test(trainX[i]) == trainY[i] for i in range(len(trainX))])/len(trainX)\n",
    "sum([nb3.test(testX[i]) == testY[i] for i in range(len(testX))])/len(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7049375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#smoothing add alpha. im just experimenting some stuff here.\n",
    "nb2 = NaiveBayes(trainX, trainY, vocabs)\n",
    "nb2.train(smoothing=lambda x, y: smoothing_add_alpha_prob(x, y, alpha=0.5) )\n",
    "# i am imputing the likelihood for words/ characters that were never seen before with N_1/N_v (ref: lecture)\n",
    "all_count_ones = sum(nb2.vocabs[i] for i in nb2.vocabs if nb2.vocabs[i] == 1) #count all vocabs that appear once\n",
    "impute_for_unknown = all_count_ones/len(vocabs)\n",
    "\n",
    "sum([nb2.test(testX[i], impute_for_never_seen=impute_for_unknown) == testY[i]\n",
    "    for i in range(len(testX))]) /len(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1CmrgFtVJ--t"
   },
   "outputs": [],
   "source": [
    "# this should return either 0 (negative sentiment) or 1 (positive sentiment)\n",
    "TRAIN_X = list(\n",
    "    map(lambda x: reformat_tweet(x), tweets))\n",
    "TRAIN_Y = res\n",
    "PROVIDED_VOCABS = make_bag_of_words( TRAIN_X ) #this builds a vocabulary size based on 80k tweets\n",
    "TRAINED_NB = NaiveBayes(TRAIN_X, TRAIN_Y, vocabs)\n",
    "TRAINED_NB.train(smoothing=lambda x,\n",
    "                 y: smoothing_add_alpha_prob(x, y, alpha=0.5))\n",
    "\n",
    "\n",
    "def predict_from_scratch(tweet, trained_model= TRAINED_NB,vocabs=PROVIDED_VOCABS):\n",
    "    return trained_model.test(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_UPXq_SdJ--u"
   },
   "outputs": [],
   "source": [
    "# this should return either 0 (negative sentiment) or 1 (positive sentiment)\n",
    "def predict_anything_goes(tweet):\n",
    "  # do something complicated here\n",
    "  return random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xGmPR8pmJ--u"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    total = 0\n",
    "    correct_from_scratch = 0\n",
    "    correct_anything_goes = 0\n",
    "    testfile = open(os.path.join(\".\", \"data\", 'test.tsv'), 'r')\n",
    "    testfile = testfile.readlines()\n",
    "    \n",
    "    for line in testfile:\n",
    "        total += 1\n",
    "        pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "        test_sample = list(map(lambda x: reformat_tweet(x), [pieces[1]]))\n",
    "        if predict_from_scratch(test_sample[0]) == int(pieces[0]):\n",
    "            correct_from_scratch += 1\n",
    "\n",
    "    return (correct_from_scratch/total)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "HNuZwhmFJ--v",
    "outputId": "efced462-d5ba-44f0-afa0-6fe235193587"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
