{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiRahbar/NLP_SLU/blob/main/sentiment/WelshSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44XAmeUWJ--p"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from typing import List\n",
        "import os\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fn = os.path.join(\"..\",\"data\",\"train-v2.tsv\")\n",
        "handle = open(fn, \"r\")\n",
        "res = list()\n",
        "tweets = list()\n",
        "for i in handle:\n",
        "    entry = i.strip().split(\"\\t\")\n",
        "    res.append(int(entry[0]))\n",
        "    tweets.append(entry[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i tried\n",
        "def reformat_tweet(tweet):\n",
        "    #get rid of mentions, links, hashtags and numbers\n",
        "    tweet2 = re.sub(\"(@USER|\\{URL\\}|#\\w*|[0-9]+)\", \"\", tweet).strip()\n",
        "    #lowercase everything\n",
        "    tweet2 =  tweet2.lower()\n",
        "    #tokenize\n",
        "    tweet2 = tweet2.split(\" \")\n",
        "    \n",
        "    clean_tweet = []\n",
        "    for word in tweet2:\n",
        "        #keep only face emojis and alpha\n",
        "        relevant_piece = re.match(\"[\\w\\U00010000-\\U0010ffff]+\", word)\n",
        "        if relevant_piece:\n",
        "            clean_tweet.append(relevant_piece.group(0))\n",
        "    return clean_tweet\n",
        "\n",
        "def make_bag_of_words(tweets: List[list]): # list of list here\n",
        "    bag_of_words = defaultdict(int)  # key-word, val-count\n",
        "    for tweet in tweets:\n",
        "        for word in tweet:\n",
        "            bag_of_words[word] += 1\n",
        "    return bag_of_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NaiveBayes():\n",
        "    def __init__(self, xVec: List[list], yVec: list, vocabs: defaultdict(int)):\n",
        "        self.xVec = xVec #assuming xVec = yVec no missing data\n",
        "        self.yVec = yVec\n",
        "        self.classes = set(yVec)\n",
        "\n",
        "        self.vocabs = vocabs\n",
        "        self.prior = defaultdict(float)\n",
        "        self.log_like = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def _sep_by_class(self):\n",
        "        big_doc = defaultdict(lambda: defaultdict(int))\n",
        "        for i in range( len(self.xVec)) :\n",
        "            c = self.yVec[i]\n",
        "            for w in self.xVec[i]:\n",
        "                big_doc[c][w] += 1\n",
        "        return big_doc #big doc contains word count separated by class {c_1: {word1: count, word2:count,etc}, c_2: {word1: count, etc},...}\n",
        "\n",
        "    def train(self, smoothing=lambda x: x):\n",
        "        N_all = len(self.xVec)\n",
        "        big_doc = self._sep_by_class()\n",
        "\n",
        "        for c in self.classes:\n",
        "            N_c = sum(1 for i in self.yVec if i == c)\n",
        "            self.prior[c] = math.log(N_c / N_all) #calculate prior\n",
        "\n",
        "            words_in_c = big_doc[c]\n",
        "            count_all_words_in_c = sum(words_in_c[w] for w in self.vocabs)\n",
        "\n",
        "            for word in self.vocabs:\n",
        "                count_word_in_c = words_in_c[word]\n",
        "                \n",
        "                # calculate likelihood\n",
        "                \n",
        "                self.log_like[word][c] = smoothing(count_word_in_c, count_all_words_in_c)\n",
        "    def test(self,tweet, impute_for_never_seen=None):\n",
        "        bestClass = None\n",
        "        bestLogPrior = None\n",
        "        for c in self.classes:\n",
        "            probability = self.prior[c]\n",
        "            for w in tweet:\n",
        "                if w in self.vocabs:\n",
        "                    probability += self.log_like[w][c]\n",
        "                else:\n",
        "                    if impute_for_never_seen:\n",
        "                        probability = impute_for_never_seen\n",
        "            if bestLogPrior == None:\n",
        "                bestLogPrior = probability\n",
        "                bestClass = c\n",
        "            elif bestLogPrior < probability:\n",
        "                bestLogPrior = probability\n",
        "                bestClass = c\n",
        "        return bestClass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#test the above method w small example from txtbook\n",
        "#just want to make sure my math makes sense\n",
        "\n",
        "#def test_nb():\n",
        "#    trainingX = [\"just plain boring\", \"entirely predictable and lacks energy\",\n",
        "#                \"no surprises and very few laughs\", \"very powerful\", \"the most fun film of the summer\"]\n",
        "#    trainingY = [0,0,0,1,1]\n",
        "#    tweets_reformatted  = reformat_tweets(trainingX)\n",
        "#    vocabs = make_bag_of_words(tweets_reformatted)\n",
        "#    nb = NaiveBayes(tweets_reformatted, trainingY, vocabs)\n",
        "#    print(nb.log_like)\n",
        "#    nb.train()\n",
        "#    print(nb.log_like)\n",
        "#    return( nb.test([\"the\", \"most\", \"fun\"]) )\n",
        "\n",
        "#divide train-test\n",
        "tweets_reformatted = list(map(lambda x: reformat_tweet(x), tweets))\n",
        "train_indexes = range(64000)\n",
        "trainX = [tweets_reformatted[i] for i in train_indexes]\n",
        "trainY = [res[i] for i in train_indexes]\n",
        "vocabs = make_bag_of_words(trainX)\n",
        "testX = [tweets_reformatted[i] for i in range(64000,80000)]\n",
        "testY = [res[i] for i in range(64000, 80000)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#smoothing add one. im just experimenting some stuff here.\n",
        "def smoothing_add_one_prob(c_w, count_all_words): return (\n",
        "    math.log((c_w+1) * (count_all_words+len(vocabs))))\n",
        "nb = NaiveBayes(trainX, trainY, vocabs)\n",
        "nb.train(smoothing = smoothing_add_one_prob)\n",
        "#sum([nb.test(trainX[i]) == trainY[i] for i in range(len(trainX))])/len(trainX)\n",
        "sum([nb.test(testX[i]) == testY[i] for i in range(len(testX))])/len(testX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#smoothing add one. im just experimenting some stuff here.\n",
        "nb2 = NaiveBayes(trainX, trainY, vocabs)\n",
        "nb2.train(smoothing=smoothing_add_one_prob)\n",
        "all_count_ones = sum(nb2.vocabs[i] for i in nb2.vocabs if nb2.vocabs[i] == 1)\n",
        "impute_for_unknown = all_count_ones/len(vocabs)\n",
        "#sum([nb.test(trainX[i]) == trainY[i] for i in range(len(trainX))])/len(trainX)\n",
        "sum([nb.test(testX[i], impute_for_unknown) == testY[i]\n",
        "    for i in range(len(testX))]) /len(testX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#with smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CmrgFtVJ--t"
      },
      "outputs": [],
      "source": [
        "# this should return either 0 (negative sentiment) or 1 (positive sentiment)\n",
        "PROVIDED_VOCABS = make_bag_of_words = make_bag_of_words( list(\n",
        "    map(lambda x: reformat_tweet(x), tweets)) ) #this builds a vocabulary size based on 80k tweets\n",
        "PRETRAINED_NB = NaiveBayes(trainX, trainY, vocabs)\n",
        "PRETRAINED_NB.train(smoothing=smoothing_add_one_prob)\n",
        "\n",
        "def predict_from_scratch(tweet, trained_model= PRETRAINED_NB,vocabs=PROVIDED_VOCABS):\n",
        "  return trained_model.test(tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UPXq_SdJ--u"
      },
      "outputs": [],
      "source": [
        "# this should return either 0 (negative sentiment) or 1 (positive sentiment)\n",
        "def predict_anything_goes(tweet):\n",
        "  # do something complicated here\n",
        "  return random.randint(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGmPR8pmJ--u"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "  total = 0\n",
        "  correct_from_scratch = 0\n",
        "  correct_anything_goes = 0\n",
        "  testfile = open('test.tsv', 'r')\n",
        "  for line in testfile:\n",
        "    total += 1\n",
        "    pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "    if predict_from_scratch(pieces[1]) == int(pieces[0]):\n",
        "      correct_from_scratch += 1\n",
        "    if predict_anything_goes(pieces[1]) == int(pieces[0]):\n",
        "      correct_anything_goes += 1\n",
        "  return (correct_from_scratch/total, correct_anything_goes/total)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNuZwhmFJ--v",
        "outputId": "efced462-d5ba-44f0-afa0-6fe235193587"
      },
      "outputs": [],
      "source": [
        "evaluate()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
