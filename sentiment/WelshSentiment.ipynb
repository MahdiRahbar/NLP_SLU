{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiRahbar/NLP_SLU/blob/main/sentiment/WelshSentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44XAmeUWJ--p"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "fn = os.path.join(\"..\",\"data\",\"train-v2.tsv\")\n",
        "handle = open(fn, \"r\")\n",
        "res = list()\n",
        "tweets = list()\n",
        "for i in handle:\n",
        "    entry = i.strip().split(\"\\t\")\n",
        "    res.append(int(entry[0]))\n",
        "    tweets.append(entry[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i tried\n",
        "def reformat_tweets(tweets):\n",
        "    #get rid of mentions, links, hashtags and numbers\n",
        "    tweets2 = map(lambda x: re.sub(\"(@USER|\\{URL\\}|#\\w*|[0-9]+)\", \"\", x).strip(), tweets)\n",
        "    #lowercase everything\n",
        "    tweets2 = map(lambda x: x.lower(), tweets2)\n",
        "    #tokenize\n",
        "    tweets2 = [t.split(\" \") for t in tweets2]\n",
        "    \n",
        "    tweets_reformatted = []\n",
        "    for tweet in tweets2:\n",
        "        clean_tweet = []\n",
        "        for word in tweet:\n",
        "            #keep only face emojis and alpha\n",
        "            relevant_piece = re.match(\"[\\w\\U00010000-\\U0010ffff]+\", word)\n",
        "            if relevant_piece:\n",
        "                clean_tweet.append(relevant_piece.group(0))\n",
        "        tweets_reformatted.append(clean_tweet)\n",
        "    return tweets_reformatted\n",
        "\n",
        "def make_bag_of_words(tweets): # list of list here\n",
        "    bag_of_words = defaultdict(int)  # key-word, val-count\n",
        "    for tweet in tweets:\n",
        "        for word in tweet:\n",
        "            bag_of_words[word] += 1\n",
        "    return bag_of_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class NaiveBayes():\n",
        "    def __init__(self, xVec:list, yVec:list, vocabs:defaultdict(int)):\n",
        "        self.xVec = xVec #assuming xVec = yVec no missing data\n",
        "        self.yVec = yVec\n",
        "        self.classes = set(yVec)\n",
        "\n",
        "        self.vocabs = vocabs\n",
        "        self.prior = defaultdict(float)\n",
        "        self.log_like = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def _sep_by_class(self):\n",
        "        big_doc = defaultdict(lambda: defaultdict(int))\n",
        "        for i in range( len(self.xVec)) :\n",
        "            c = self.yVec[i]\n",
        "            for w in self.xVec[i]:\n",
        "                big_doc[c][w] += 1\n",
        "        return big_doc #big doc contains word count separated by class {c_1: {word1: count, word2:count,etc}, c_2: {word1: count, etc},...}\n",
        "\n",
        "    def train(self):\n",
        "        N_all = len(self.xVec)\n",
        "        big_doc = self._sep_by_class()\n",
        "\n",
        "        for c in self.classes:\n",
        "            N_c = sum(1 for i in self.yVec if i == c)\n",
        "            self.prior[c] = N_c / N_all #calculate prior\n",
        "\n",
        "            words_in_c = big_doc[c]\n",
        "            count_all_words_in_c = sum(words_in_c[w] for w in self.vocabs)\n",
        "\n",
        "            for word in self.vocabs:\n",
        "                count_word_in_c = words_in_c[word]\n",
        "                \n",
        "                # calculate likelihood\n",
        "                self.log_like[word][c] = math.log(\n",
        "                    (count_word_in_c+1) / (count_all_words_in_c + len(self.vocabs)) )\n",
        "    def test(self,tweet):\n",
        "        bestClass = None\n",
        "        bestLogPrior = None\n",
        "        for c in self.classes:\n",
        "            probability = self.prior[c]\n",
        "            for w in tweet:\n",
        "                if w in self.vocabs:\n",
        "                    probability += self.log_like[w][c]\n",
        "            if bestLogPrior == None:\n",
        "                bestLogPrior = probability\n",
        "                bestClass = c\n",
        "            elif bestLogPrior < probability:\n",
        "                bestLogPrior = probability\n",
        "                bestClass = c\n",
        "        return bestClass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'int' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/fc/kpcnsyl97sd418q6f75lby8r0000gn/T/ipykernel_41750/539067482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvocabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/fc/kpcnsyl97sd418q6f75lby8r0000gn/T/ipykernel_41750/4072073987.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mN_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mbig_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sep_by_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/fc/kpcnsyl97sd418q6f75lby8r0000gn/T/ipykernel_41750/4072073987.py\u001b[0m in \u001b[0;36m_sep_by_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxVec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myVec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myVec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mbig_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbig_doc\u001b[0m \u001b[0;31m#big doc contains word count separated by class {c_1: {word1: count, word2:count,etc}, c_2: {word1: count, etc},...}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ],
      "source": [
        "#divide train-test\n",
        "\n",
        "train_indexes = range(64000)\n",
        "tweets_reformatted = reformat_tweets(tweets)\n",
        "\n",
        "trainX = [tweets_reformatted[i] for i in train_indexes]\n",
        "trainY = [res[i] for i in train_indexes]\n",
        "vocabs = make_bag_of_words(trainX)\n",
        "nb = NaiveBayes(trainX, trainY, vocabs)\n",
        "nb.train()\n",
        "nb.test(trainX[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 233,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nb.test_bayes(trainX[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 231,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CmrgFtVJ--t"
      },
      "outputs": [],
      "source": [
        "# this should return either 0 (negative sentiment) or 1 (positive sentiment)\n",
        "\n",
        "def predict_from_scratch(tweet):\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UPXq_SdJ--u"
      },
      "outputs": [],
      "source": [
        "# this should return either 0 (negative sentiment) or 1 (positive sentiment)\n",
        "def predict_anything_goes(tweet):\n",
        "  # do something complicated here\n",
        "  return random.randint(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGmPR8pmJ--u"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "  total = 0\n",
        "  correct_from_scratch = 0\n",
        "  correct_anything_goes = 0\n",
        "  testfile = open('test.tsv', 'r')\n",
        "  for line in testfile:\n",
        "    total += 1\n",
        "    pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "    if predict_from_scratch(pieces[1]) == int(pieces[0]):\n",
        "      correct_from_scratch += 1\n",
        "    if predict_anything_goes(pieces[1]) == int(pieces[0]):\n",
        "      correct_anything_goes += 1\n",
        "  return (correct_from_scratch/total, correct_anything_goes/total)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNuZwhmFJ--v",
        "outputId": "efced462-d5ba-44f0-afa0-6fe235193587"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.5096, 0.4987)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
