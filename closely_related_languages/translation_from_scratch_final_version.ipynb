{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873cf942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-11-15 22:06:49.461446: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 22:06:51.653175: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-15 22:06:52.736737: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 22:07:15.805346: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-11-15 22:07:15.812472: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-11-15 22:07:15.812499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from typing import Any, Tuple \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import einops \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score \n",
    "from sklearn import preprocessing \n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "import tensorflow as tf \n",
    "# import tensorflow_text  as tf_text \n",
    "\n",
    "from Bio import pairwise2\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "from difflib import ndiff\n",
    "\n",
    "\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b3c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'train')\n",
    "test_path = os.path.join('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fade8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Word level \n",
    "\n",
    "# with open(os.path.join(train_path, 'train-source.txt')) as f: \n",
    "#     train_source = f.read()\n",
    "    \n",
    "# with open(os.path.join(train_path, 'train-target.txt')) as f: \n",
    "#     train_target = f.read()\n",
    "    \n",
    "# train_source = train_source.split('</s>')\n",
    "# train_target = train_target.split('</s>')\n",
    "\n",
    "# for i in range(len(train_source)): \n",
    "#     train_source[i] = str(train_source[i].lower()+'E').replace('<s>','S').strip().split('\\n')\n",
    "#     train_target[i] = str(train_target[i].lower()+'E').replace('<s>','S').strip().split('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "# with open(os.path.join(test_path, 'test-source.txt')) as f: \n",
    "#     test_source = f.read()\n",
    "    \n",
    "# with open(os.path.join(test_path, 'test-target.txt')) as f: \n",
    "#     test_target = f.read()\n",
    "    \n",
    "# test_source = test_source.split('</s>')\n",
    "# test_target = test_target.split('</s>')\n",
    "\n",
    "# for i in range(len(test_source)): \n",
    "#     test_source[i] = str(test_source[i].lower()+'E').replace('<s>','S').strip().split('\\n')\n",
    "#     test_target[i] = str(test_target[i].lower()+'E').replace('<s>','S').strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5441760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level \n",
    "\n",
    "with open(os.path.join(train_path, 'train-source.txt')) as f: \n",
    "    train_source = f.read()\n",
    "    \n",
    "with open(os.path.join(train_path, 'train-target.txt')) as f: \n",
    "    train_target = f.read()\n",
    "    \n",
    "train_source = train_source.strip().split('</s>')\n",
    "train_target = train_target.strip().split('</s>')\n",
    "\n",
    "for i in range(len(train_source)): \n",
    "    train_source[i] = str(train_source[i].lower()).replace('<s>','').strip().split('\\n')\n",
    "    train_target[i] = str(train_target[i].lower()).replace('<s>','').strip().split('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(os.path.join(test_path, 'test-source.txt')) as f: \n",
    "    test_source = f.read()\n",
    "    \n",
    "with open(os.path.join(test_path, 'test-target.txt')) as f: \n",
    "    test_target = f.read()\n",
    "    \n",
    "test_source = test_source.split('</s>')\n",
    "test_target = test_target.split('</s>')\n",
    "\n",
    "for i in range(len(test_source)): \n",
    "    test_source[i] = str(test_source[i].lower()).replace('<s>','').strip().split('\\n')\n",
    "    test_target[i] = str(test_target[i].lower()).replace('<s>','').strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a24e507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scéal',\n",
       " 'chathail',\n",
       " 'freeman',\n",
       " '-',\n",
       " 'téid',\n",
       " 'mo',\n",
       " 'dhearbhráthair',\n",
       " \"'un\",\n",
       " 'na',\n",
       " 'dubh-charraice']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba61ebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(',\n",
       " 'bhí',\n",
       " 'sé',\n",
       " 'follasach',\n",
       " 'go',\n",
       " 'rabh',\n",
       " 'an',\n",
       " 'poll',\n",
       " 'sin',\n",
       " 'ag',\n",
       " 'foscladh',\n",
       " 'ar',\n",
       " 'an',\n",
       " 'fhairrge',\n",
       " 'ar',\n",
       " 'dhóigh',\n",
       " 'éigin',\n",
       " ',',\n",
       " 'ná',\n",
       " 'líonadh',\n",
       " 'agus',\n",
       " 'thráigheadh',\n",
       " 'an',\n",
       " 't-uisce',\n",
       " 'ann',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed56add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Character level \n",
    "\n",
    "# train_source_char = []\n",
    "# train_target_char = []\n",
    "\n",
    "# for i in range(len(train_source)): \n",
    "#     train_source_char.append(' '.join(train_source[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "#     train_target_char.append(' '.join(train_target[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    \n",
    "# for i in range(len(train_source_char)):\n",
    "#     train_source_char[i] = [c for c in train_source_char[i]] + ['S', 'E']\n",
    "#     train_target_char[i] = [c for c in train_target_char[i]] + ['S', 'E']\n",
    "    \n",
    "# char_pool = [] \n",
    "# for i in range(len(train_source_char)): \n",
    "#     char_pool += train_source_char[i]+ train_target_char[i]  \n",
    "    \n",
    "    \n",
    "# # Character level - test set\n",
    "\n",
    "# test_source_char = []\n",
    "# test_target_char = []\n",
    "\n",
    "# for i in range(len(test_source)): \n",
    "#     test_source_char.append(' '.join(test_source[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "#     test_target_char.append(' '.join(test_target[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    \n",
    "# for i in range(len(test_source_char)):\n",
    "#     test_source_char[i] = [c for c in test_source_char[i]] + ['S', 'E']\n",
    "#     test_target_char[i] = [c for c in test_target_char[i]] + ['S', 'E']\n",
    "                                                                \n",
    "                                                                \n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7946f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character level \n",
    "\n",
    "train_source_char = []\n",
    "train_target_char = []\n",
    "\n",
    "for i in range(len(train_source)): \n",
    "    train_source_char.append(' '.join(train_source[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    train_target_char.append(' '.join(train_target[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    \n",
    "for i in range(len(train_source_char)):\n",
    "    train_source_char[i] = [c for c in train_source_char[i]]\n",
    "    train_target_char[i] = [c for c in train_target_char[i]] \n",
    "    \n",
    "char_pool = [] \n",
    "for i in range(len(train_source_char)): \n",
    "    char_pool += train_source_char[i]+ train_target_char[i]  \n",
    "    \n",
    "    \n",
    "# Character level - test set\n",
    "\n",
    "test_source_char = []\n",
    "test_target_char = []\n",
    "\n",
    "for i in range(len(test_source)): \n",
    "    test_source_char.append(' '.join(test_source[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    test_target_char.append(' '.join(test_target[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    \n",
    "for i in range(len(test_source_char)):\n",
    "    test_source_char[i] = [c for c in test_source_char[i]]\n",
    "    test_target_char[i] = [c for c in test_target_char[i]]\n",
    "                                                                \n",
    "                                                                \n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dc9504f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61119d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = True):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "            \n",
    "\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[rows-1][cols-1]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return distance[row][col]\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1273786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(list1, list2): \n",
    "    if len(list1)< len(list2):\n",
    "        return len(list1)\n",
    "    else:\n",
    "        return len(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57966931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(list1, list2): \n",
    "    if len(list1)> len(list2):\n",
    "        return len(list1)\n",
    "    else:\n",
    "        return len(list2)\n",
    "    \n",
    "def get_len(input_list):\n",
    "    return len(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9f66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1 , c2  \n",
    "def align_words(train_source):\n",
    "    sentence_num = get_len(train_source)\n",
    "    tagged_list = []\n",
    "    for i in range(sentence_num): \n",
    "        c1, c2 = 0, 0 \n",
    "        l1 = get_len(train_source[i])\n",
    "        l2 = get_len(train_target[i])\n",
    "        temp_tagged_list = []\n",
    "        while c1<l1 and c2<l2-1:\n",
    "            score_list = []\n",
    "            for k in range(2):\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1], train_target[i][c2+k]))\n",
    "\n",
    "            if c1<l1-1:\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1+1], train_target[i][c2]))\n",
    "            if score_list[0]==1: \n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2]))\n",
    "                c1 += 1 \n",
    "                c2 += 1\n",
    "            elif score_list[1]==1: \n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif score_list[0]>0.4 and  score_list[1]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif c1<l1-1 and score_list[0]>0.4 and  score_list[2]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2]))\n",
    "                temp_tagged_list.append((train_source[i][c1+1], train_target[i][c2]))\n",
    "                c1 += 2\n",
    "                c2 += 1\n",
    "            elif score_list[0]>0.3 and  score_list[1]<0.6:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]))\n",
    "                c1 += 1\n",
    "                c2 += 1\n",
    "            elif score_list[0]<0.6 and  score_list[1]>0.3:\n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2+k-1]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            else:\n",
    "                temp_tagged_list.append((train_source[i][c1], \"ε\")) # <NULL> = ε\n",
    "                c1 += 1\n",
    "\n",
    "        tagged_list.append(temp_tagged_list)      \n",
    "    return tagged_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d082b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1 , c2  \n",
    "def align_words(train_source, train_target):\n",
    "    sentence_num = get_len(train_source)\n",
    "    tagged_list = []\n",
    "    for i in range(sentence_num): \n",
    "        c1, c2 = 0, 0 \n",
    "        l1 = get_len(train_source[i])\n",
    "        l2 = get_len(train_target[i])\n",
    "        temp_tagged_list = []\n",
    "        while c1<l1 and c2<l2-1:\n",
    "            score_list = []\n",
    "            for k in range(2):\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1], train_target[i][c2+k]))\n",
    "\n",
    "            if c1<l1-1:\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1+1], train_target[i][c2]))\n",
    "            if score_list[0]==1: \n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2]))\n",
    "                c1 += 1 \n",
    "                c2 += 1\n",
    "            elif score_list[1]==1: \n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif score_list[0]>0.4 and  score_list[1]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]+ ' ' + train_target[i][c2+1]))\n",
    "#                 temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif c1<l1-1 and score_list[0]>0.4 and  score_list[2]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1]+ ' ' +train_source[i][c1+1], train_target[i][c2]))\n",
    "#                 temp_tagged_list.append((train_source[i][c1+1], train_target[i][c2]))\n",
    "                c1 += 2\n",
    "                c2 += 1\n",
    "            elif score_list[0]>0.3 and  score_list[1]<0.6:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]))\n",
    "                c1 += 1\n",
    "                c2 += 1\n",
    "            elif score_list[0]<0.6 and  score_list[1]>0.3:\n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2+k-1]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            else:\n",
    "                temp_tagged_list.append((train_source[i][c1], \"ε\")) # <NULL> = ε\n",
    "                c1 += 1\n",
    "#             temp_tagged_list.append((' ', ' '))\n",
    "        \n",
    "        tagged_list.append(temp_tagged_list[:-1])      \n",
    "    return tagged_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a844a411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scéal',\n",
       " 'chathail',\n",
       " 'freeman',\n",
       " '-',\n",
       " 'téid',\n",
       " 'mo',\n",
       " 'dhearbhráthair',\n",
       " \"'un\",\n",
       " 'na',\n",
       " 'dubh-charraice']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05dae902",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_list = align_words(train_source, train_target)\n",
    "tagged_list_test = align_words(test_source, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f90c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_list_sent = tagged_list.copy()\n",
    "tagged_list_test_sent = tagged_list_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2809c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45172"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_list_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "501ddc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_list_temp = [] \n",
    "tagged_list_test_temp = [] \n",
    "for i in range(len(tagged_list)):\n",
    "    tagged_list_temp += tagged_list[i]\n",
    "    \n",
    "for i in range(len(tagged_list_test)):\n",
    "    tagged_list_test_temp += tagged_list_test[i]\n",
    "    \n",
    "tagged_list =  tagged_list_temp\n",
    "tagged_list_test = tagged_list_test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a150b609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cinnte', 'cinnte'),\n",
       " ('go', 'go'),\n",
       " ('leór', 'leor'),\n",
       " (',', ','),\n",
       " ('thiocfadh', 'thiocfadh'),\n",
       " ('dóbhtha', 'dóibh'),\n",
       " ('bás', 'bás'),\n",
       " ('a', 'a'),\n",
       " ('fhagháil', 'fháil'),\n",
       " ('ar', 'ar')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4311e694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scéal', 'scéal'), ('chathail', 'chathail'), ('freeman', 'freeman')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b2f4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_pad(str1, str2):\n",
    "    result = \"\"\n",
    "    pos, removed = 0, 0\n",
    "    for x in ndiff(str1, str2):\n",
    "        if pos<len(str1) and str1[pos] == x[2]:\n",
    "            pos += 1\n",
    "            result += x[2]\n",
    "            if x[0] == \"-\":\n",
    "                removed += 1\n",
    "            continue\n",
    "        else:\n",
    "            if removed > 0:\n",
    "                removed -=1\n",
    "            else:\n",
    "                result += \"ε\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f60f6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cha2thaεεil'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_pad('cha2thail','cha3tha89il')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f21ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_levenshtein(sentence_list):\n",
    "    padded_sentence_list = [] \n",
    "    for i in range(len(sentence_list)):\n",
    "        \n",
    "        if len(sentence_list[i][0])>len(sentence_list[i][1]):\n",
    "             padded_sentence_list.append((sentence_list[i][0], \n",
    "             levenshtein_pad(sentence_list[i][1], sentence_list[i][0])))\n",
    "        else: \n",
    "            padded_sentence_list.append((levenshtein_pad(sentence_list[i][0], sentence_list[i][1]),\n",
    "            sentence_list[i][1]))\n",
    "    return padded_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2dcd671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(input_string, max_len):\n",
    "    input_string += \"ε\" * (max_len - len(input_string))\n",
    "    return input_string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "808baf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(sentence_list):\n",
    "    padded_sentence_list = [] \n",
    "    for i in range(len(sentence_list)):\n",
    "        \n",
    "        if len(sentence_list[i][0])>len(sentence_list[i][1]):\n",
    "            max_len = len(sentence_list[i][0])\n",
    "        else: \n",
    "            max_len = len(sentence_list[i][1])\n",
    "        padded_sentence_list.append((pad(sentence_list[i][0], max_len),\n",
    "                                 pad(sentence_list[i][1], max_len)))\n",
    "    return padded_sentence_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d14dcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cinnte', 'cinnte')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "945715ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_list = pad_list_levenshtein(tagged_list)\n",
    "padded_list_test = pad_list_levenshtein(tagged_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "161a9906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cinnte', 'cinnte')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbe22696",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_list_sent = [] \n",
    "padded_list_test_sent = []\n",
    "for i in range(len(tagged_list_sent)): \n",
    "    padded_list_sent.append(pad_list_levenshtein(tagged_list_sent[i]))\n",
    "for i in range(len(tagged_list_test_sent)): \n",
    "    padded_list_test_sent.append(pad_list_levenshtein(tagged_list_test_sent[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79e13306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('leór', 'leor')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a340c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771279"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be8ffd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_char_pair_set(sent_word_list:list): \n",
    "    sent_word_char_list = [] \n",
    "    for i in range(len(sent_word_list)):\n",
    "        try:\n",
    "            string_list = []\n",
    "            for k in range(len(sent_word_list[i][0])):\n",
    "                string_list.append((sent_word_list[i][0][k], sent_word_list[i][1][k]))\n",
    "#             string_list.append((' ', ' '))\n",
    "            sent_word_char_list.append(string_list)\n",
    "        except: \n",
    "            pass\n",
    "    return sent_word_char_list  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e69bf07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dubεεεεεεεhairt', \"dúirt m'athair\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_list[1386]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a6e8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_char_list = make_char_pair_set(padded_list)\n",
    "padded_char_list_test = make_char_pair_set(padded_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa7d92ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 'b'), ('á', 'á'), ('s', 's')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_char_list[6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28629660",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_train = padded_char_list\n",
    "sentence_list_test = padded_char_list_test\n",
    "global_list_of_words = [w for sentence in sentence_list_train for w,t in sentence]\n",
    "# global_list_of_chars \n",
    "# global_list_of_tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c831c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train, Xy_test = (sentence_list_train, sentence_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "395c26e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3127478\n",
      "88253\n"
     ]
    }
   ],
   "source": [
    "train_tagged_words = [ tup for sent in Xy_train for tup in sent]\n",
    "test_tagged_words = [ tup for sent in Xy_test for tup in sent]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f08d3fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f', 'i', 'k', 'n', 'h', 'l', 'r', '…', 'í', 'e', ' ', ')', '!', 's', 'p', '\"', '’', '6', 'z', '0', '-', 'u', 'o', '.', '_', '5', '£', 'ú', ';', 'g', 't', 'é', 'y', '1', 'w', 'x', '4', 'm', 'd', 'á', '`', '>', 'j', '?', '8', ',', 'a', ':', '^', '‘', '2', 'b', 'c', '(', '—', '[', 'ó', 'q', '7', 'ε', '3', \"'\", '9', 'v'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "# print(len(tags))\n",
    "print(tags)\n",
    " \n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde1d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5a269f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', 'e')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b374e307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3127478"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9fec79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(train_tagged_words)\n",
    "best_tags = dict((w, cfd[w].max()) for w in global_list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "24681a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = [(t,w) for sent in sentence_list_train for (w,t) in sent]\n",
    "wordgiventag = nltk.ConditionalFreqDist(flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "id": "6d8e226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is P(w|t), unsmoothed!\n",
    "def P(w,t):\n",
    "    return wordgiventag[t][w] / wordgiventag[t].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "id": "a118cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigrams = [(x,y) for sent in sentence_list_train for x,y in nltk.bigrams([t for (w,t) in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "1f73e4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('n', 'a')"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_bigrams[2134]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "1262c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "tag_bigram_counts = nltk.ConditionalFreqDist(tag_bigrams)\n",
    "# this is count of noun tags following adjective tags (normal order in English)\n",
    "print(tag_bigram_counts['a']['a'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "83cc5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188797\n"
     ]
    }
   ],
   "source": [
    "print(tag_bigram_counts['ε']['ε'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "dae94882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766320"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "b1fc1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "for i in range(len(sentence_list_train)-1,-1,-1):\n",
    "    \n",
    "    try: \n",
    "        sentence_list_train[i][0][1]\n",
    "    except: \n",
    "        del sentence_list_train[i]\n",
    "        \n",
    "    counter +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "45986092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is P(t2|t1), unsmoothed again!\n",
    "def tagP(t2,t1):\n",
    "    return tag_bigram_counts[t1][t2] / (tag_bigram_counts[t1].N()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "e81035da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766320"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "a94bd8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_start = nltk.FreqDist(sent[0][1] for sent in sentence_list_train)\n",
    "def initP(t):\n",
    "    return sentence_start[t] / (sentence_start.N()+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "id": "f3775cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(V,tag_list,t,i):\n",
    "    ans=-1\n",
    "    best=None\n",
    "    for s in tag_list:\n",
    "        temp=V[(s,i-1)]*tagP(t,s)\n",
    "        if temp > ans:\n",
    "            ans = temp\n",
    "            best = s\n",
    "    return (best,ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "fb4ccf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_viterbi(sentence,tag_list,V,B):\n",
    "    output_list = []\n",
    "    for i in range(len(sentence)-1,-1,-1):\n",
    "        max_value = 0 \n",
    "        temp_label = ''\n",
    "        for t in tag_list:\n",
    "            if V[(t,i)] > max_value: \n",
    "                max_value = V[(t,i)]\n",
    "                temp_label = t \n",
    "                if i>0:\n",
    "                    tag_list_temp = list(B[(t,i)])\n",
    "                else: \n",
    "                    break\n",
    "        output_list.append(temp_label)\n",
    "    output_list.reverse()\n",
    "    return output_list\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "id": "0bf0e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, labels ):\n",
    "    V = dict()    # keys are (t,i) where t is a tag (row label) and i is position in sentence (column label)\n",
    "    B = dict()    # same keys as V; this stores the \"backpointers\" to remember best tag sequence\n",
    "    tag_list = list(sentence_start.keys()).copy()\n",
    "    for t in tag_list:\n",
    "        V[(t,0)] = initP(t)*P(sentence[0],t)\n",
    "    for i in range(1,len(sentence)):\n",
    "        for t in tag_list:\n",
    "            pair = argmax(V,tag_list,t,i)\n",
    "            B[(t,i)] = pair[0]\n",
    "            V[(t,i)] = pair[1]*P(sentence[i],t)\n",
    "\n",
    "    output_list = output_viterbi(sentence,tag_list,V,B)\n",
    "    \n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "e06ee946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('s', 's'), ('c', 'c'), ('é', 'é'), ('a', 'a'), ('l', 'l')],\n",
       " [('c', 'c'),\n",
       "  ('h', 'h'),\n",
       "  ('a', 'a'),\n",
       "  ('t', 't'),\n",
       "  ('h', 'h'),\n",
       "  ('a', 'a'),\n",
       "  ('i', 'i'),\n",
       "  ('l', 'l')],\n",
       " [('f', 'f'),\n",
       "  ('r', 'r'),\n",
       "  ('e', 'e'),\n",
       "  ('e', 'e'),\n",
       "  ('m', 'm'),\n",
       "  ('a', 'a'),\n",
       "  ('n', 'n')],\n",
       " [('-', '-')],\n",
       " [('t', 't'), ('é', 'é'), ('i', 'a'), ('d', 'n'), ('ε', 'n')]]"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "f772502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5838535596264097\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "counter = 0\n",
    "total = 0\n",
    "final_outputs = []\n",
    "for i in range(len(Xy_test)-1):  # len(Xy_test)-1\n",
    "    test_run = [Xy_test[i]]\n",
    "    # list of tagged words\n",
    "    test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "    # list of untagged words\n",
    "    test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "    test_tagged_labels = [tup[1] for sent in test_run for tup in sent]\n",
    "    \n",
    "    if len(test_tagged_words)==0 or len(test_tagged_labels)==0: \n",
    "        continue\n",
    "    \n",
    "    output = viterbi(test_tagged_words, test_tagged_labels)\n",
    "#     output = list(filter(lambda a: a != 'ε', output))\n",
    "#     test_tagged_labels = list(filter(lambda a: a != 'ε', test_tagged_labels))\n",
    "    test_tagged_labels = list(filter(lambda a: a != ' ', test_tagged_labels))\n",
    "    \n",
    "    #there may be several references\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([test_tagged_labels], output, weights = [1])\n",
    "    total += BLEUscore\n",
    "    \n",
    "    final_outputs.append(output)\n",
    "    \n",
    "print(total/(len(Xy_test)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a8c59a",
   "metadata": {},
   "source": [
    "## Viterbi and HMM on Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e741c5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('c', 'c')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f23e5301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "771279\n",
      "21448\n"
     ]
    }
   ],
   "source": [
    "sentence_list_train = tagged_list_sent.copy()\n",
    "sentence_list_test = tagged_list_test_sent.copy()\n",
    "\n",
    "global_list_of_words = [w for sentence in sentence_list_train for w,t in sentence]\n",
    "\n",
    "Xy_train, Xy_test = (sentence_list_train.copy(), sentence_list_test.copy())\n",
    "\n",
    "train_tagged_words = [ tup for sent in Xy_train for tup in sent]\n",
    "test_tagged_words = [ tup for sent in Xy_test for tup in sent]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))\n",
    "\n",
    "\n",
    "#use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "# print(len(tags))\n",
    "# print(tags)\n",
    " \n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "735374a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scéal', 'scéal'),\n",
       " ('chathail', 'chathail'),\n",
       " ('freeman', 'freeman'),\n",
       " ('-', '-'),\n",
       " ('téid', 'téann'),\n",
       " ('mo', 'mo'),\n",
       " ('dhearbhráthair', 'dheartháir'),\n",
       " (\"'un\", 'chun')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6a12e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(train_tagged_words)\n",
    "best_tags = dict((w, cfd[w].max()) for w in global_list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "935e22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = [(t,w) for sent in sentence_list_train for (w,t) in sent]\n",
    "wordgiventag = nltk.ConditionalFreqDist(flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49d8fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is P(w|t), unsmoothed!\n",
    "def P(w,t):\n",
    "    return wordgiventag[t][w] / wordgiventag[t].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2f3293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigrams = [(x,y) for sent in sentence_list_train for x,y in nltk.bigrams([t for (w,t) in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa14e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigram_counts = nltk.ConditionalFreqDist(tag_bigrams)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6eaba14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0 \n",
    "for i in range(len(sentence_list_train)-1,-1,-1):\n",
    "    \n",
    "    try: \n",
    "        sentence_list_train[i][0][1]\n",
    "    except: \n",
    "        del sentence_list_train[i]\n",
    "        \n",
    "    counter +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3f6f4bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_bigram_counts['scéal'].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee789fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is P(t2|t1), unsmoothed again!\n",
    "def tagP(t2,t1):\n",
    "    if tag_bigram_counts[t1].N():\n",
    "        return tag_bigram_counts[t1][t2] / (tag_bigram_counts[t1].N())\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d4bbe7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_start = nltk.FreqDist(sent[0][1] for sent in sentence_list_train)\n",
    "def initP(t):\n",
    "    if sentence_start.N():\n",
    "        return sentence_start[t] / (sentence_start.N())\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1896a908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(V,tag_list,t,i):\n",
    "    ans=-1\n",
    "    best=None\n",
    "    for s in tag_list:\n",
    "        temp=V[(s,i-1)]*tagP(t,s)\n",
    "        if temp > ans:\n",
    "            ans = temp\n",
    "            best = s\n",
    "    return (best,ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "681414da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_viterbi(sentence,tag_list,V,B):\n",
    "    output_list = []\n",
    "    for i in range(len(sentence)-1,-1,-1):\n",
    "        max_value = 0 \n",
    "        temp_label = ''\n",
    "        for t in tag_list:\n",
    "            if V[(t,i)] > max_value: \n",
    "                max_value = V[(t,i)]\n",
    "                temp_label = t \n",
    "                if i>0:\n",
    "                    tag_list_temp = list(B[(t,i)])\n",
    "                else: \n",
    "                    break\n",
    "        output_list.append(temp_label)\n",
    "    output_list.reverse()\n",
    "    return output_list.reverse()\n",
    "                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "831bf32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, labels ):\n",
    "    V = dict()    # keys are (t,i) where t is a tag (row label) and i is position in sentence (column label)\n",
    "    B = dict()    # same keys as V; this stores the \"backpointers\" to remember best tag sequence\n",
    "    tag_list = list(sentence_start.keys()).copy()\n",
    "    \n",
    "    for t in tag_list:\n",
    "        V[(t,0)] = initP(t)*P(sentence[0],t)\n",
    "        \n",
    "    for i in range(1,len(sentence)):\n",
    "        for t in tag_list:\n",
    "            pair = argmax(V,tag_list,t,i)\n",
    "            B[(t,i)] = pair[0]\n",
    "            V[(t,i)] = pair[1]*P(sentence[i],t)\n",
    "    \n",
    "    output_list = output_viterbi(sentence,tag_list,V,B)\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "519604c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.742359"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "counter = 0\n",
    "total = 0\n",
    "final_outputs = []\n",
    "for i in range(5):  # len(Xy_test)-1\n",
    "    try:\n",
    "        test_run = [Xy_test[i]]\n",
    "        # list of tagged words\n",
    "#         test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "        # list of untagged words\n",
    "        test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "        test_tagged_labels = [tup[1] for sent in test_run for tup in sent]\n",
    "\n",
    "        if len(test_tagged_words)==0 or len(test_tagged_labels)==0: \n",
    "            continue\n",
    "\n",
    "        output = viterbi(test_tagged_words, test_tagged_labels)\n",
    "\n",
    "        #there may be several references\n",
    "        BLEUscore = nltk.translate.bleu_score.sentence_bleu([test_tagged_labels], output, weights = [1])\n",
    "        total += BLEUscore\n",
    "\n",
    "        final_outputs.append(output)\n",
    "        counter += 1 \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(total/(counter+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04ba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
