{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "873cf942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from typing import Any, Tuple \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import einops \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score \n",
    "from sklearn import preprocessing \n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_text  as tf_text \n",
    "\n",
    "from Bio import pairwise2\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "import pathlib\n",
    "\n",
    "\n",
    "use_builtins = True\n",
    "\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b4e67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########################################################################################\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    # Disable first GPU\n",
    "    GPUs = tf.config.set_visible_devices(physical_devices[3], 'GPU')\n",
    "#     for gpu in gpus:\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "    pass\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37b3c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'train')\n",
    "test_path = os.path.join('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fade8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level \n",
    "\n",
    "with open(os.path.join(train_path, 'train-source.txt')) as f: \n",
    "    train_source = f.read()\n",
    "    \n",
    "with open(os.path.join(train_path, 'train-target.txt')) as f: \n",
    "    train_target = f.read()\n",
    "    \n",
    "train_source = train_source.split('</s>')\n",
    "train_target = train_target.split('</s>')\n",
    "\n",
    "for i in range(len(train_source)): \n",
    "    train_source[i] = str(train_source[i].lower()+'E').replace('<s>','S').strip().split('\\n')\n",
    "    train_target[i] = str(train_target[i].lower()+'E').replace('<s>','S').strip().split('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(os.path.join(test_path, 'test-source.txt')) as f: \n",
    "    test_source = f.read()\n",
    "    \n",
    "with open(os.path.join(test_path, 'test-target.txt')) as f: \n",
    "    test_target = f.read()\n",
    "    \n",
    "test_source = test_source.split('</s>')\n",
    "test_target = test_target.split('</s>')\n",
    "\n",
    "for i in range(len(test_source)): \n",
    "    test_source[i] = str(test_source[i].lower()+'E').replace('<s>','S').strip().split('\\n')\n",
    "    test_target[i] = str(test_target[i].lower()+'E').replace('<s>','S').strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5441760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level \n",
    "\n",
    "with open(os.path.join(train_path, 'train-source.txt')) as f: \n",
    "    train_source = f.read()\n",
    "    \n",
    "with open(os.path.join(train_path, 'train-target.txt')) as f: \n",
    "    train_target = f.read()\n",
    "    \n",
    "train_source = train_source.strip().split('</s>')\n",
    "train_target = train_target.strip().split('</s>')\n",
    "\n",
    "for i in range(len(train_source)): \n",
    "    train_source[i] = str(train_source[i].lower()).replace('<s>','').strip().split('\\n')\n",
    "    train_target[i] = str(train_target[i].lower()).replace('<s>','').strip().split('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "with open(os.path.join(test_path, 'test-source.txt')) as f: \n",
    "    test_source = f.read()\n",
    "    \n",
    "with open(os.path.join(test_path, 'test-target.txt')) as f: \n",
    "    test_target = f.read()\n",
    "    \n",
    "test_source = test_source.split('</s>')\n",
    "test_target = test_target.split('</s>')\n",
    "\n",
    "for i in range(len(test_source)): \n",
    "    test_source[i] = str(test_source[i].lower()).replace('<s>','').strip().split('\\n')\n",
    "    test_target[i] = str(test_target[i].lower()).replace('<s>','').strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8a24e507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scéal',\n",
       " 'chathail',\n",
       " 'freeman',\n",
       " '-',\n",
       " 'téid',\n",
       " 'mo',\n",
       " 'dhearbhráthair',\n",
       " \"'un\",\n",
       " 'na',\n",
       " 'dubh-charraice']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba61ebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(',\n",
       " 'bhí',\n",
       " 'sé',\n",
       " 'follasach',\n",
       " 'go',\n",
       " 'rabh',\n",
       " 'an',\n",
       " 'poll',\n",
       " 'sin',\n",
       " 'ag',\n",
       " 'foscladh',\n",
       " 'ar',\n",
       " 'an',\n",
       " 'fhairrge',\n",
       " 'ar',\n",
       " 'dhóigh',\n",
       " 'éigin',\n",
       " ',',\n",
       " 'ná',\n",
       " 'líonadh',\n",
       " 'agus',\n",
       " 'thráigheadh',\n",
       " 'an',\n",
       " 't-uisce',\n",
       " 'ann',\n",
       " '.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f7946f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character level \n",
    "\n",
    "train_source_char = []\n",
    "train_target_char = []\n",
    "\n",
    "for i in range(len(train_source)): \n",
    "    train_source_char.append(' '.join(train_source[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    train_target_char.append(' '.join(train_target[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    \n",
    "for i in range(len(train_source_char)):\n",
    "    train_source_char[i] = [c for c in train_source_char[i]]\n",
    "    train_target_char[i] = [c for c in train_target_char[i]] \n",
    "    \n",
    "char_pool = [] \n",
    "for i in range(len(train_source_char)): \n",
    "    char_pool += train_source_char[i]+ train_target_char[i]  \n",
    "    \n",
    "    \n",
    "# Character level - test set\n",
    "\n",
    "test_source_char = []\n",
    "test_target_char = []\n",
    "\n",
    "for i in range(len(test_source)): \n",
    "    test_source_char.append(' '.join(test_source[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    test_target_char.append(' '.join(test_target[i]).replace('S','').replace('E','').lower().strip(' '))\n",
    "    \n",
    "for i in range(len(test_source_char)):\n",
    "    test_source_char[i] = [c for c in test_source_char[i]]\n",
    "    test_target_char[i] = [c for c in test_target_char[i]]\n",
    "                                                                \n",
    "                                                                \n",
    "                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6dc9504f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61119d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = True):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "            \n",
    "\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[rows-1][cols-1]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return distance[row][col]\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1273786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min(list1, list2): \n",
    "    if len(list1)< len(list2):\n",
    "        return len(list1)\n",
    "    else:\n",
    "        return len(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57966931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(list1, list2): \n",
    "    if len(list1)> len(list2):\n",
    "        return len(list1)\n",
    "    else:\n",
    "        return len(list2)\n",
    "    \n",
    "def get_len(input_list):\n",
    "    return len(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff9f66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1 , c2  \n",
    "def align_words(train_source):\n",
    "    sentence_num = get_len(train_source)\n",
    "    tagged_list = []\n",
    "    for i in range(sentence_num): \n",
    "        c1, c2 = 0, 0 \n",
    "        l1 = get_len(train_source[i])\n",
    "        l2 = get_len(train_target[i])\n",
    "        temp_tagged_list = []\n",
    "        while c1<l1 and c2<l2-1:\n",
    "            score_list = []\n",
    "            for k in range(2):\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1], train_target[i][c2+k]))\n",
    "\n",
    "            if c1<l1-1:\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1+1], train_target[i][c2]))\n",
    "            if score_list[0]==1: \n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2]))\n",
    "                c1 += 1 \n",
    "                c2 += 1\n",
    "            elif score_list[1]==1: \n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif score_list[0]>0.4 and  score_list[1]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif c1<l1-1 and score_list[0]>0.4 and  score_list[2]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2]))\n",
    "                temp_tagged_list.append((train_source[i][c1+1], train_target[i][c2]))\n",
    "                c1 += 2\n",
    "                c2 += 1\n",
    "            elif score_list[0]>0.3 and  score_list[1]<0.6:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]))\n",
    "                c1 += 1\n",
    "                c2 += 1\n",
    "            elif score_list[0]<0.6 and  score_list[1]>0.3:\n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2+k-1]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            else:\n",
    "                temp_tagged_list.append((train_source[i][c1], \"ε\")) # <NULL> = ε\n",
    "                c1 += 1\n",
    "\n",
    "        tagged_list.append(temp_tagged_list)      \n",
    "    return tagged_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "58d082b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c1 , c2  \n",
    "def align_words(train_source, train_target):\n",
    "    sentence_num = get_len(train_source)\n",
    "    tagged_list = []\n",
    "    for i in range(sentence_num): \n",
    "        c1, c2 = 0, 0 \n",
    "        l1 = get_len(train_source[i])\n",
    "        l2 = get_len(train_target[i])\n",
    "        temp_tagged_list = []\n",
    "        while c1<l1 and c2<l2-1:\n",
    "            score_list = []\n",
    "            for k in range(2):\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1], train_target[i][c2+k]))\n",
    "\n",
    "            if c1<l1-1:\n",
    "                score_list.append(levenshtein_ratio_and_distance(train_source[i][c1+1], train_target[i][c2]))\n",
    "            if score_list[0]==1: \n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2]))\n",
    "                c1 += 1 \n",
    "                c2 += 1\n",
    "            elif score_list[1]==1: \n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif score_list[0]>0.4 and  score_list[1]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]+ ' ' + train_target[i][c2+1]))\n",
    "#                 temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            elif c1<l1-1 and score_list[0]>0.4 and  score_list[2]>0.4:\n",
    "                temp_tagged_list.append((train_source[i][c1]+ ' ' +train_source[i][c1+1], train_target[i][c2]))\n",
    "#                 temp_tagged_list.append((train_source[i][c1+1], train_target[i][c2]))\n",
    "                c1 += 2\n",
    "                c2 += 1\n",
    "            elif score_list[0]>0.4 and  score_list[1]<0.6:\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+0]))\n",
    "                c1 += 1\n",
    "                c2 += 1\n",
    "            elif score_list[0]<0.6 and  score_list[1]>0.4:\n",
    "                temp_tagged_list.append((\"ε\", train_target[i][c2+k-1]))\n",
    "                temp_tagged_list.append((train_source[i][c1], train_target[i][c2+1]))\n",
    "                c1 += 1\n",
    "                c2 += 2\n",
    "            else:\n",
    "                temp_tagged_list.append((train_source[i][c1], \"ε\")) # <NULL> = ε\n",
    "                c1 += 1\n",
    "#             temp_tagged_list.append((' ', ' '))\n",
    "        \n",
    "        tagged_list.append(temp_tagged_list[:-1])      \n",
    "    return tagged_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a844a411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amuigh',\n",
       " 'sa',\n",
       " 'tír',\n",
       " 'a',\n",
       " 'bhí',\n",
       " 'muid',\n",
       " \"'n-ár\",\n",
       " 'gcomhnaidhe',\n",
       " ',',\n",
       " 'agus',\n",
       " 'ag',\n",
       " 'gasúr',\n",
       " 'cosamhail',\n",
       " 'le',\n",
       " 'mo',\n",
       " 'dhearbhráthair',\n",
       " 'ba',\n",
       " 'doiligh',\n",
       " 'áit',\n",
       " \"'fhagháil\",\n",
       " 'a',\n",
       " \"b'aoibhne\",\n",
       " 'ná',\n",
       " 'an',\n",
       " 'áit',\n",
       " \"s'againne\",\n",
       " '.']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_source[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "05dae902",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_list = align_words(train_source, train_target)\n",
    "tagged_list_test = align_words(test_source, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "53a4a524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cinnte', 'cinnte'),\n",
       " ('go', 'go'),\n",
       " ('leór', 'leor'),\n",
       " (',', ','),\n",
       " ('thiocfadh', 'thiocfadh'),\n",
       " ('dóbhtha', 'dóibh'),\n",
       " ('bás', 'bás'),\n",
       " ('a', 'a'),\n",
       " ('fhagháil', 'fháil'),\n",
       " ('ar', 'ar'),\n",
       " ('imeall', 'imeall'),\n",
       " ('an', 'an'),\n",
       " ('phuill', 'phoill')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4311e694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scéal', 'scéal'), ('chathail', 'chathail'), ('freeman', 'freeman')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2dcd671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(input_string, max_len):\n",
    "    input_string += \"ε\" * (max_len - len(input_string))\n",
    "    return input_string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "808baf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(sentence_list):\n",
    "    padded_sentence_list = [] \n",
    "    for i in range(len(sentence_list)):\n",
    "        \n",
    "        if len(sentence_list[i][0])>len(sentence_list[i][1]):\n",
    "            max_len = len(sentence_list[i][0])\n",
    "        else: \n",
    "            max_len = len(sentence_list[i][1])\n",
    "        padded_sentence_list.append((pad(sentence_list[i][0], max_len),\n",
    "                                 pad(sentence_list[i][1], max_len)))\n",
    "    return padded_sentence_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "945715ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_list = pad_list(tagged_list)\n",
    "padded_list_test = pad_list(tagged_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be8ffd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_char_pair_set(sent_word_list:list): \n",
    "    sent_word_char_list = [] \n",
    "    for i in range(len(sent_word_list)):\n",
    "        string_list = []\n",
    "        for k in range(len(sent_word_list[i][0])):\n",
    "            string_list.append((sent_word_list[i][0][k], sent_word_list[i][1][k]))\n",
    "        string_list.append((' ', ' '))\n",
    "        sent_word_char_list.append(string_list)\n",
    "    return sent_word_char_list  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a6e8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_char_list = make_char_pair_set(padded_list)\n",
    "padded_char_list_test = make_char_pair_set(padded_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa7d92ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 'c'),\n",
       " ('i', 'i'),\n",
       " ('n', 'n'),\n",
       " ('n', 'n'),\n",
       " ('t', 't'),\n",
       " ('e', 'e'),\n",
       " (' ', ' ')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_char_list[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28629660",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_train = padded_char_list\n",
    "sentence_list_test = padded_char_list_test\n",
    "global_list_of_words = [w for sentence in sentence_list_train for w,t in sentence]\n",
    "# global_list_of_chars \n",
    "# global_list_of_tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c831c193",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train, Xy_test = (sentence_list_train, sentence_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged_words = [ tup for sent in Xy_train for tup in sent]\n",
    "test_tagged_words = [ tup for sent in Xy_test for tup in sent]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "# print(len(tags))\n",
    "print(tags)\n",
    " \n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28137f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02217ca2",
   "metadata": {},
   "source": [
    "## References: \n",
    "### Using an attention-based sequence to sequence encoder decoder\n",
    "1. https://www.tensorflow.org/text/tutorials/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57329494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "        for name, new_dim in parsed.items():\n",
    "            old_dim = self.shapes.get(name, None)\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "361b85ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cinnte', 'cinnte'),\n",
       " ('go', 'go'),\n",
       " ('leór', 'leor'),\n",
       " (',', ','),\n",
       " ('thiocfadh', 'thiocfadh'),\n",
       " ('dóbhtha', 'dóibh'),\n",
       " ('bás', 'bás'),\n",
       " ('a', 'a'),\n",
       " ('fhagháil', 'fháil'),\n",
       " ('ar', 'ar'),\n",
       " ('imeall', 'imeall'),\n",
       " ('an', 'an'),\n",
       " ('phuill', 'phoill'),\n",
       " ('udaí', 'ε'),\n",
       " ('ε', 'úd')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "065ec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tagged_list):\n",
    "    context = [] \n",
    "    target = [] \n",
    "    for i in range(len(tagged_list)-1):\n",
    "        temp_context = [] \n",
    "        temp_target = [] \n",
    "        for j in range(len(tagged_list[i])): \n",
    "            temp_context.append(tagged_list[i][j][0])\n",
    "            temp_target.append(tagged_list[i][j][1])\n",
    "        context.append(temp_context)\n",
    "        target.append(temp_target)\n",
    "        \n",
    "    for i in range(len(context)):\n",
    "        context[i] = \" \".join(context[i])\n",
    "        target[i] = \" \".join(target[i])\n",
    "\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1d8a6db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mar sin de , caithfidh mé suidhe sa\n",
      "mar sin de , caithfidh mé suí sa\n"
     ]
    }
   ],
   "source": [
    "target_raw, context_raw = load_data(tagged_list)\n",
    "print(context_raw[-1])\n",
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cf7409eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('cinnte go leór , thiocfadh dóbhtha bás a fhagháil ar imeall an phuill udaí ε',\n",
       "      dtype='<U76')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(context_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "be01e542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45171"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "8998d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "context_raw_train = []\n",
    "target_raw_train = []  \n",
    "context_raw_val = []\n",
    "target_raw_val = []\n",
    "for i in range(len(is_train)):\n",
    "    if is_train[i]: \n",
    "        context_raw_train.append(context_raw[i])\n",
    "        target_raw_train.append(target_raw[i])\n",
    "    else:\n",
    "        context_raw_val.append(context_raw[i])\n",
    "        target_raw_val.append(target_raw[i])\n",
    "        \n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_train, target_raw_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_val, target_raw_val))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f6c55925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"shiubhail t\\xc3\\xba cuid mhaith de do th\\xc3\\xadr dh\\xc3\\xbathchais agus p\\xc3\\xa1irt mh\\xc3\\xb3r de'n d\\xc3\\xa1 oile\\xc3\\xa1n is deise\"\n",
      " b'\" ar mheas ? \" arsa jukes leis'\n",
      " b'carr\\xc3\\xb3ir a bh\\xc3\\xad ag gabh\\xc3\\xa1il s\\xc3\\xados go doire an l\\xc3\\xa1 roimh r\\xc3\\xa9 \\xce\\xb5 thug s\\xc3\\xa9 leis a gcuid bag\\xc3\\xa1ist\\xc3\\xad ; agus \\xce\\xb5 anois nuair nach raibh aon ualach le h-iomchar \\xce\\xb5 n\\xc3\\xador sh\\xc3\\xadl siad a dhath de s\\xc3\\xa9 mh\\xc3\\xadle dh\\xc3\\xa9ag agus fiche de astar a dh\\xc3\\xa9anamh go dt\\xc3\\xad an'\n",
      " b\"' s\\xc3\\xa9a'r'd a b'fhearr linn \\xce\\xb5 \\xc3\\xa9 bheith ' \\xce\\xb5 na dhrabhlas cosamhail \\xce\\xb5 linn fh\\xc3\\xa9in\"\n",
      " b'\" \\xce\\xb5 n\\xc3\\xad dheachaidh drod ar a bh\\xc3\\xa9al an oidhche f\\xc3\\xa1 dheireadh ach ag cainnt ar'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'shi\\xc3\\xbail t\\xc3\\xba cuid mhaith de do th\\xc3\\xadr dh\\xc3\\xbachais agus p\\xc3\\xa1irt mh\\xc3\\xb3r den d\\xc3\\xa1 oile\\xc3\\xa1n is deise'\n",
      " b'\" ar mheas ? \" arsa jukes leis'\n",
      " b\"carr\\xc3\\xb3ir a bh\\xc3\\xad ag gabh\\xc3\\xa1il s\\xc3\\xados go doire an l\\xc3\\xa1 roimh r\\xc3\\xa9 , thug s\\xc3\\xa9 leis a gcuid bag\\xc3\\xa1ist\\xc3\\xad ; agus , anois nuair nach raibh aon ualach le hiompar , n\\xc3\\xador sh\\xc3\\xadl siad a dhath de sh\\xc3\\xa9 mh\\xc3\\xadle dh\\xc3\\xa9ag agus fiche \\xce\\xb5 d'aistear a dh\\xc3\\xa9anamh go dt\\xc3\\xad an\"\n",
      " b'\\xce\\xb5 \\xce\\xb5 \\xce\\xb5 \\xce\\xb5 \\xce\\xb5 is \\xc3\\xa9 \\xce\\xb5 \\xce\\xb5 rud ab \\xce\\xb5 \\xce\\xb5 fhearr linn \\xce\\xb5'\n",
      " b\"\\xce\\xb5 ' n\\xc3\\xad dheachaigh drud ar a bh\\xc3\\xa9al an o\\xc3\\xadche f\\xc3\\xa1 dheireadh ach ag caint ar\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9e5a1cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'saoghal ion-mhaoidhte orainn sa bhaile'\n",
      "b'saoghal ion-mhaoidhte orainn sa bhaile'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('saoghal ion-mhaoidhte orainn sa bhaile')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e5144d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e4ccc70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saoghal ion-mhaoidhte orainn sa bhaile\n",
      "[START] saoghal ion-mhaoidhte orainn sa bhaile [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a49a7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a17b9101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', 'a', 'ε', 'an', ',', 'agus', 'ar']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "86772acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'ε', '[START]', '[END]', 'a', 'an', ',', 'agus', 'ar']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "521b233d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 303, 58, 125, 130, 31, 39, 766, 5482, 8, 2707, 115, 94, 59, 708, 45,\n",
       "  909, 3]                                                                ,\n",
       " [2, 11, 9, 978, 85, 11, 22, 402, 30, 3],\n",
       " [2, 4117, 4, 10, 15, 228, 122, 14, 1659, 6, 61, 334, 1919, 5, 68, 13, 30,\n",
       "  4, 213, 1, 66, 8, 5, 97, 35, 32, 62, 95, 1839, 20, 6102, 5, 74, 383, 33,\n",
       "  4, 198, 31, 13, 1015, 1875, 8, 398, 31, 3329, 4, 181, 14, 109, 6, 3]    ]>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "8572febd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START] shiubhail tú cuid mhaith de do thír dhúthchais agus páirt mhór de'n dá oileán is deise [END]\""
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9b96919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = context_text_processor(context).to_tensor()\n",
    "    target = target_text_processor(target)\n",
    "    targ_in = target[:,:-1].to_tensor()\n",
    "    targ_out = target[:,1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "58c88529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  303   13   14 2842    9  643   12 1575    7]\n",
      "\n",
      "[   3  284   12   13 2682    9  619   14 1483    7]\n",
      "[ 284   12   13 2682    9  619   14 1483    7    8]\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "    print(ex_context_tok[0, :10].numpy()) \n",
    "    print()\n",
    "    print(ex_tar_in[0, :10].numpy()) \n",
    "    print(ex_tar_out[0, :10].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f4310e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c3bf19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                                   mask_zero=True)\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.GRU(units,\n",
    "                                # Return the sequence and state\n",
    "                                return_sequences=True,\n",
    "                                recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(x, 'batch s')\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding vector for each token.\n",
    "        x = self.embedding(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 3. The GRU processes the sequence of embeddings.\n",
    "        x = self.rnn(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 4. Returns the new sequence of embeddings.\n",
    "        return x\n",
    "\n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e0dd99a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (32, 46)\n",
      "Encoder output, shape (batch, s, units): (32, 46, 128)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0928490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, context):\n",
    "        shape_checker = ShapeChecker()\n",
    "\n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(context, 'batch s units')\n",
    "\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "    \n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(attn_scores, 'batch heads t s')\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        shape_checker(attn_scores, 'batch t s')\n",
    "        self.last_attention_weights = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "015a9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (32, 46, 128)\n",
      "Target sequence, shape (batch, t, units): (32, 45, 128)\n",
      "Attention result, shape (batch, t, units): (32, 45, 128)\n",
      "Attention weights, shape (batch, t, s):    (32, 45, 46)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f84ba6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.99999994, 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 0.99999994, 1.        , 1.        ,\n",
       "       1.0000001 , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.0000001 , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 0.99999994, 1.        , 0.99999994,\n",
       "       1.        , 0.9999999 , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7f02de6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask=(ex_context_tok != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title('Mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cd3e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]',\n",
    "            invert=True)\n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                   units, mask_zero=True)\n",
    "\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.rnn = tf.keras.layers.GRU(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "    \n",
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch t')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    # 1. Lookup the embeddings\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    # 2. Process the target sequence.\n",
    "    x, state = self.rnn(x, initial_state=state)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    # 3. Use the RNN output as the query for the attention over the context.\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "    # Step 4. Generate logit predictions for the next token.\n",
    "    logits = self.output_layer(x)\n",
    "    shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8b84b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "08cbd219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (32, 46, 128)\n",
      "input target tokens shape: (batch, t) (32, 45)\n",
      "logits shape shape: (batch, target_vocabulary_size) (32, 45, 10000)\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_tar_in)\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "8ef35a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b051c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "49929e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "  \n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :]/temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f8575fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'oirne\\xcc\\x81ise gallcho\\xcc\\x81ir bhocht deirimse gcluine gloini\\xcc\\x81 deilbh mhaoil grabhar locha\\xcc\\x81in',\n",
       "       b\"mb'fhe\\xcc\\x81idir osrai\\xcc\\x81 glagaireacht throime mholl leathcheann bui\\xcc\\x81 'deir achrann chreid\",\n",
       "       b'leithsce\\xcc\\x81al di\\xcc\\x81omsa dochartaigh chreagacha sca\\xcc\\x81tha nde\\xcc\\x81ana sto\\xcc\\x81pa t-athairlus le\\xcc\\x81ine ni\\xcc\\x81n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a8fb7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, units,\n",
    "                context_text_processor,\n",
    "                target_text_processor):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(context_text_processor, units)\n",
    "        decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)\n",
    "        logits = self.decoder(context, x)\n",
    "\n",
    "        #TODO(b/250038731): remove this\n",
    "        try:\n",
    "            # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4be2ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e69d9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "75aeba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "adf5630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a0a70c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 9.2103405, 'expected_acc': 0.0001}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5c732a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=((TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None)), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "c3e78b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 7s 44ms/step - loss: 9.2104 - masked_acc: 1.0000e-04 - masked_loss: 9.2104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 9.210420608520508,\n",
       " 'masked_acc': 0.00010000000474974513,\n",
       " 'masked_loss': 9.210420608520508}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8c00afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 27s 80ms/step - loss: 6.3586 - masked_acc: 0.0854 - masked_loss: 6.3586 - val_loss: 5.7394 - val_masked_acc: 0.1163 - val_masked_loss: 5.7394\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 5.5302 - masked_acc: 0.1426 - masked_loss: 5.5302 - val_loss: 5.0802 - val_masked_acc: 0.1848 - val_masked_loss: 5.0802\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 4.9482 - masked_acc: 0.2152 - masked_loss: 4.9482 - val_loss: 4.5962 - val_masked_acc: 0.2519 - val_masked_loss: 4.5962\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 4.4745 - masked_acc: 0.2821 - masked_loss: 4.4745 - val_loss: 4.0974 - val_masked_acc: 0.3295 - val_masked_loss: 4.0974\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 3.7101 - masked_acc: 0.4146 - masked_loss: 3.7101 - val_loss: 3.1469 - val_masked_acc: 0.4986 - val_masked_loss: 3.1469\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 3.0127 - masked_acc: 0.5271 - masked_loss: 3.0127 - val_loss: 2.6010 - val_masked_acc: 0.5804 - val_masked_loss: 2.6010\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 2.5624 - masked_acc: 0.5932 - masked_loss: 2.5624 - val_loss: 2.2582 - val_masked_acc: 0.6270 - val_masked_loss: 2.2582\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 2.2109 - masked_acc: 0.6471 - masked_loss: 2.2109 - val_loss: 2.0460 - val_masked_acc: 0.6706 - val_masked_loss: 2.0460\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 2.0084 - masked_acc: 0.6801 - masked_loss: 2.0084 - val_loss: 1.6459 - val_masked_acc: 0.7286 - val_masked_loss: 1.6459\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 1.7615 - masked_acc: 0.7162 - masked_loss: 1.7615 - val_loss: 1.5287 - val_masked_acc: 0.7516 - val_masked_loss: 1.5287\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 1.6328 - masked_acc: 0.7344 - masked_loss: 1.6328 - val_loss: 1.3988 - val_masked_acc: 0.7664 - val_masked_loss: 1.3988\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 1.3704 - masked_acc: 0.7732 - masked_loss: 1.3707 - val_loss: 1.2211 - val_masked_acc: 0.7947 - val_masked_loss: 1.2211\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 1.1645 - masked_acc: 0.8065 - masked_loss: 1.1645 - val_loss: 1.1289 - val_masked_acc: 0.8130 - val_masked_loss: 1.1289\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 1.0950 - masked_acc: 0.8169 - masked_loss: 1.0950 - val_loss: 1.0624 - val_masked_acc: 0.8218 - val_masked_loss: 1.0624\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 1.0132 - masked_acc: 0.8284 - masked_loss: 1.0132 - val_loss: 1.0377 - val_masked_acc: 0.8263 - val_masked_loss: 1.0377\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.9664 - masked_acc: 0.8351 - masked_loss: 0.9664 - val_loss: 0.8793 - val_masked_acc: 0.8507 - val_masked_loss: 0.8793\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.8845 - masked_acc: 0.8483 - masked_loss: 0.8845 - val_loss: 0.8069 - val_masked_acc: 0.8654 - val_masked_loss: 0.8069\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.8719 - masked_acc: 0.8494 - masked_loss: 0.8719 - val_loss: 0.8056 - val_masked_acc: 0.8588 - val_masked_loss: 0.8056\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.8108 - masked_acc: 0.8569 - masked_loss: 0.8108 - val_loss: 0.8024 - val_masked_acc: 0.8661 - val_masked_loss: 0.8024\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.7806 - masked_acc: 0.8621 - masked_loss: 0.7806 - val_loss: 0.8305 - val_masked_acc: 0.8563 - val_masked_loss: 0.8305\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.7152 - masked_acc: 0.8737 - masked_loss: 0.7152 - val_loss: 0.6990 - val_masked_acc: 0.8782 - val_masked_loss: 0.6990\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.7028 - masked_acc: 0.8768 - masked_loss: 0.7028 - val_loss: 0.6936 - val_masked_acc: 0.8751 - val_masked_loss: 0.6936\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.6874 - masked_acc: 0.8751 - masked_loss: 0.6878 - val_loss: 0.7015 - val_masked_acc: 0.8782 - val_masked_loss: 0.7015\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.5415 - masked_acc: 0.8950 - masked_loss: 0.5415 - val_loss: 0.6509 - val_masked_acc: 0.8801 - val_masked_loss: 0.6509\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.5474 - masked_acc: 0.8932 - masked_loss: 0.5474 - val_loss: 0.6876 - val_masked_acc: 0.8800 - val_masked_loss: 0.6876\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.5174 - masked_acc: 0.8975 - masked_loss: 0.5174 - val_loss: 0.6508 - val_masked_acc: 0.8834 - val_masked_loss: 0.6508\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.5465 - masked_acc: 0.8945 - masked_loss: 0.5465 - val_loss: 0.6018 - val_masked_acc: 0.8873 - val_masked_loss: 0.6018\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.4972 - masked_acc: 0.9017 - masked_loss: 0.4972 - val_loss: 0.5601 - val_masked_acc: 0.8991 - val_masked_loss: 0.5601\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.5467 - masked_acc: 0.8939 - masked_loss: 0.5467 - val_loss: 0.5761 - val_masked_acc: 0.8941 - val_masked_loss: 0.5761\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.5280 - masked_acc: 0.8972 - masked_loss: 0.5280 - val_loss: 0.5704 - val_masked_acc: 0.8946 - val_masked_loss: 0.5704\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.4873 - masked_acc: 0.9041 - masked_loss: 0.4873 - val_loss: 0.6182 - val_masked_acc: 0.8854 - val_masked_loss: 0.6182\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=100,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "65f98223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "    # Process the input texts\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "\n",
    "    # Setup the loop inputs\n",
    "    tokens = []\n",
    "    attention_weights = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Generate the next token\n",
    "        next_token, done, state = self.decoder.get_next_token(\n",
    "            context, next_token, done,  state, temperature)\n",
    "        \n",
    "        # Collect the generated tokens\n",
    "        tokens.append(next_token)\n",
    "        attention_weights.append(self.decoder.last_attention_weights)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Stack the lists of tokens and attention weights.\n",
    "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "    self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b85c83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_source, test_target = load_data(tagged_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c036e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7095056302491874\n"
     ]
    }
   ],
   "source": [
    "result = [] \n",
    "total = 0\n",
    "for i in range(len(test_source)):\n",
    "    result.append(model.translate([test_source[i]])[0].numpy().decode())\n",
    "    total += nltk.translate.bleu_score.sentence_bleu([test_target[i]], result[-1], weights=[1])\n",
    "print(total/len(test_source))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7204e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
