{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e0bf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import random \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import nltk \n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('universal_tagset')\n",
    "brownwords = nltk.corpus.brown.tagged_words(categories='news', tagset='universal')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(42)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdeb06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.tsv', 'r') as f: \n",
    "    train_data = f.read()\n",
    "    \n",
    "with open('test.tsv', 'r') as f: \n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3167a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_lines_train = train_data.strip('\\n').split('\\n')\n",
    "splited_lines_test = test_data.strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28503a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(splited_lines_train)): \n",
    "    splited_lines_train[i] = splited_lines_train[i].strip(' ').split('\\t')\n",
    "    \n",
    "for i in range(len(splited_lines_test)): \n",
    "    splited_lines_test[i] = splited_lines_test[i].strip(' ').split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fbdc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'N']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_lines_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0161c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(splited_lines):\n",
    "    temp_tokens= [] \n",
    "    sentence_list = []\n",
    "    global_list_of_words = [] \n",
    "    global_list_of_chars = []\n",
    "    global_list_of_tags = []\n",
    "    for w,t in splited_lines: \n",
    "        global_list_of_words.append(w)\n",
    "        global_list_of_chars += [c for c in w]\n",
    "        global_list_of_tags.append(t)\n",
    "        if w == '<S>':\n",
    "            sentence_list.append(temp_tokens)\n",
    "            temp_tokens = []\n",
    "            temp_tokens.append(tuple((w,t)))\n",
    "        else:\n",
    "            temp_tokens.append(tuple((w,t)))\n",
    "    sentence_list.append(temp_tokens)\n",
    "    return sentence_list, global_list_of_words, global_list_of_chars, global_list_of_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b289cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_train, global_list_of_words, global_list_of_chars, global_list_of_tags = get_sentence_list(splited_lines_train)\n",
    "sentence_list_test, _, _, _= get_sentence_list(splited_lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6f545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d3feaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of words list is:  10000000\n",
      "The length of char list is:  41518749\n",
      "The length of tag list is:  10000000\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of words list is: \", len(global_list_of_words))\n",
    "print(\"The length of char list is: \", len(global_list_of_chars))\n",
    "print(\"The length of tag list is: \", len(global_list_of_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9323c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_set_of_words = set(global_list_of_words)\n",
    "global_set_of_chars = set(global_list_of_chars)\n",
    "global_set_of_tags = set(global_list_of_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c350be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of words set is:  171515\n",
      "The length of char set is:  237\n",
      "The length of tag set is:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of words set is: \", len(global_set_of_words))\n",
    "print(\"The length of char set is: \", len(global_set_of_chars))\n",
    "print(\"The length of tag set is: \", len(global_set_of_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bafe00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(input_set):\n",
    "    dictionary = {}\n",
    "    dictionary['[PAD]'] = 0\n",
    "    dictionary['[UKN]'] = 1\n",
    "    counter = 2 \n",
    "    for i in input_set: \n",
    "        dictionary[i] = counter \n",
    "        counter += 1 \n",
    "    return dictionary\n",
    "\n",
    "def encode_labels(input_set):\n",
    "    dictionary = {}\n",
    "    dictionary['[PAD]'] = 0\n",
    "    counter = 1 \n",
    "    for i in input_set: \n",
    "        dictionary[i] = counter \n",
    "        counter += 1 \n",
    "    return dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e37467e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = create_dict(global_set_of_words)\n",
    "char_dict = create_dict(global_set_of_chars)\n",
    "tag_dict = encode_labels(global_set_of_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e17ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = [] \n",
    "# y = [] \n",
    "# for i in range(len(sentence_list)):\n",
    "#     temp_X = []\n",
    "#     temp_y = [] \n",
    "#     for j in range(len(sentence_list[i])):\n",
    "#         temp_X.append(sentence_list[i][j][0])\n",
    "#         temp_y.append(sentence_list[i][j][1])\n",
    "#     X.append(temp_X)\n",
    "#     y.append(temp_y)\n",
    "    \n",
    "def split_feature_label(sentence_list): \n",
    "    X = [] \n",
    "    y = [] \n",
    "    for i in range(len(sentence_list)):\n",
    "        temp_X = []\n",
    "        temp_y = [] \n",
    "        for j in range(len(sentence_list[i])):\n",
    "            temp_X.append(sentence_list[i][j][0])\n",
    "            temp_y.append(sentence_list[i][j][1])\n",
    "        X.append(temp_X)\n",
    "        y.append(temp_y)\n",
    "    return X, y                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d3bd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = split_feature_label(sentence_list_train)\n",
    "X_test, y_test = split_feature_label(sentence_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21ffc087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39718"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6068c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "get_max_seq_len = 0 \n",
    "for i in range(len(sentence_list_train)):\n",
    "    if len(sentence_list_train[i]) > get_max_seq_len: \n",
    "        get_max_seq_len = len(sentence_list_train[i])\n",
    "print(get_max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a856157",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_PAD = 150 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d597bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trim_seq(sequence, max_len = MAX_SEQ_PAD):\n",
    "    sequence = sequence.copy()\n",
    "    if len(sequence)> max_len: \n",
    "        sequence = sequence[:max_len]\n",
    "    elif len(sequence)< max_len: \n",
    "        seq_len = len(sequence)\n",
    "        for _ in range(max_len - seq_len): \n",
    "            sequence.append('[PAD]')\n",
    "    return sequence \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80bd3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_sequences(X,y):\n",
    "    padded_X = [] \n",
    "    padded_y = [] \n",
    "    for i in range(len(X)):\n",
    "        padded_X.append(pad_trim_seq(X[i]))\n",
    "        padded_y.append(pad_trim_seq(y[i]))\n",
    "    return padded_X, padded_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e81e6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_X_train, padded_y_train = pad_list_sequences(X_train, y_train)\n",
    "padded_X_test, padded_y_test = pad_list_sequences(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48ff09eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ansin',\n",
       " ')',\n",
       " 'tá',\n",
       " 'níos',\n",
       " 'lú',\n",
       " 'gaeilge',\n",
       " 'ag',\n",
       " 'na',\n",
       " 'gardaí',\n",
       " 'ná',\n",
       " 'bí',\n",
       " 'ariamh',\n",
       " 'ainneoin',\n",
       " 'na',\n",
       " 'cearta',\n",
       " '.',\n",
       " 'níl',\n",
       " 'sé',\n",
       " 'ach',\n",
       " 'roinnt',\n",
       " 'seachtainí',\n",
       " 'ó',\n",
       " 'sin',\n",
       " 'a',\n",
       " 'tógadh',\n",
       " 'fear',\n",
       " 'bocht',\n",
       " 'a',\n",
       " 'tug',\n",
       " 'ainm',\n",
       " 'gaeilge',\n",
       " 'dóibh',\n",
       " '.',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de055b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(sequence, dictionary):\n",
    "    encoded_seq = [] \n",
    "    for i in range(len(sequence)):\n",
    "        try: \n",
    "            encoded_seq.append(dictionary[sequence[i]])\n",
    "        except:\n",
    "            encoded_seq.append(dictionary['[UKN]'])\n",
    "    return encoded_seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00381adf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoded_X_train = [] \n",
    "encoded_y_train = []\n",
    "for i in range(len(padded_X_train)):\n",
    "    encoded_X_train.append(encode_sequence(padded_X_train[i], word_dict))\n",
    "    encoded_y_train.append(encode_sequence(padded_y_train[i], tag_dict))\n",
    "    \n",
    "encoded_X_test = [] \n",
    "encoded_y_test = []\n",
    "for i in range(len(padded_X_test)):\n",
    "    encoded_X_test.append(encode_sequence(padded_X_test[i], word_dict))\n",
    "    encoded_y_test.append(encode_sequence(padded_y_test[i], tag_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581a737",
   "metadata": {},
   "source": [
    "## Character level encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba448373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "max_char_num = 0 \n",
    "for i in range(len(X_train)):\n",
    "    temp_encoded_chars = [] \n",
    "    for j in range(len(X_train[i])):\n",
    "        if len(X_train[i][j])> max_char_num: \n",
    "            max_char_num = len(X_train[i][j])\n",
    "            max_word = X_train[i][j]\n",
    "#             print(max_word)\n",
    "            \n",
    "print(max_char_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69cb0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHAR_NUM = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af83f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_char_X = [] \n",
    "# for i in range(len(padded_X)):\n",
    "#     temp_list = [] \n",
    "#     for j in range(len(padded_X[i])):\n",
    "#         temp_char_list = [c for c in padded_X[i][j]]\n",
    "#         temp_char_list = pad_trim_seq(temp_char_list, MAX_CHAR_NAME)\n",
    "#         temp_list.append(temp_char_list)\n",
    "#     padded_char_X.append(temp_list)\n",
    "    \n",
    "def pad_list_chars(padded_X):\n",
    "    padded_char_X = [] \n",
    "    for i in range(len(padded_X)):\n",
    "        temp_list = [] \n",
    "        for j in range(len(padded_X[i])):\n",
    "            temp_char_list = [c for c in padded_X[i][j]]\n",
    "            temp_char_list = pad_trim_seq(temp_char_list, MAX_CHAR_NUM)\n",
    "            temp_list.append(temp_char_list)\n",
    "        padded_char_X.append(temp_list)\n",
    "    return padded_char_X\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "86bdd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_char_X_train = pad_list_chars(padded_X_train)\n",
    "padded_char_X_test = pad_list_chars(padded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7de866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded_char_X = [] \n",
    "# for i in range(len(padded_X)):\n",
    "#     temp_list = [] \n",
    "#     for j in range(len(padded_X[i])):\n",
    "#         temp_char_list = [c for c in padded_X[i][j]]\n",
    "# #         temp_char_list = pad_trim_seq(temp_char_list, MAX_CHAR_NAME)\n",
    "#         temp_list.append(temp_char_list)\n",
    "#     padded_char_X.append(temp_list)\n",
    "    \n",
    "def unpad_list_chars(padded_X):\n",
    "    padded_char_X = [] \n",
    "    for i in range(len(padded_X)):\n",
    "        temp_list = [] \n",
    "        for j in range(len(padded_X[i])):\n",
    "            temp_char_list = [c for c in padded_X[i][j]]\n",
    "    #         temp_char_list = pad_trim_seq(temp_char_list, MAX_CHAR_NAME)\n",
    "            temp_list.append(temp_char_list)\n",
    "        padded_char_X.append(temp_list)\n",
    "    return padded_char_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36b06609",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_char_X_train = unpad_list_chars(padded_X_train)\n",
    "padded_char_X_test = unpad_list_chars(padded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "43d9d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_set_train = []\n",
    "for i in range(len(padded_char_X_train)):\n",
    "    for j in range(len(padded_char_X_train[i])):\n",
    "        len_set_train.append(len(padded_char_X_train[i][j]))\n",
    "        \n",
    "        \n",
    "len_set_test = []\n",
    "for i in range(len(padded_char_X_test)):\n",
    "    for j in range(len(padded_char_X_test[i])):\n",
    "        len_set_test.append(len(padded_char_X_test[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1c95e793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_char_X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b7ac9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding characters for character level model training \n",
    "\n",
    "# encoded_X_chars = [] \n",
    "# for i in range(len(padded_char_X)):\n",
    "#     temp_encoded_chars = [] \n",
    "#     for j in range(len(padded_char_X[i])):\n",
    "#         temp_encoded_chars.append(encode_sequence(padded_char_X[i][j], char_dict))\n",
    "#     encoded_X_chars.append(temp_encoded_chars)\n",
    "    \n",
    "def encode_chars_list(padded_char_X):\n",
    "    encoded_X_chars = [] \n",
    "    for i in range(len(padded_char_X)):\n",
    "        temp_encoded_chars = [] \n",
    "        for j in range(len(padded_char_X[i])):\n",
    "            temp_encoded_chars.append(encode_sequence(padded_char_X[i][j], char_dict))\n",
    "        encoded_X_chars.append(temp_encoded_chars)\n",
    "    return encoded_X_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "500e60e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_chars_train =  encode_chars_list(padded_char_X_train)\n",
    "encoded_X_chars_test =  encode_chars_list(padded_char_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fb5cdf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(encoded_X_chars_train)):\n",
    "    for j in range(len(encoded_X_chars_train[i])):\n",
    "        encoded_X_chars_train[i][j] = np.array(encoded_X_chars_train[i][j])\n",
    "\n",
    "for i in range(len(encoded_X_chars_test)):\n",
    "    for j in range(len(encoded_X_chars_test[i])):\n",
    "        encoded_X_chars_test[i][j] = np.array(encoded_X_chars_test[i][j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ba970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "405feef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(input_list, dictionary = char_dict):\n",
    "    encoded_data = [] \n",
    "    char_len = len(char_dict)\n",
    "    for i in range(len(input_list)):\n",
    "        temp_char_list = []\n",
    "        for j in range(len(input_list[i])): \n",
    "            temp_vec = np.zeros(char_len)\n",
    "            temp_vec[input_list[i][j]] = 1 \n",
    "            temp_char_list.append(temp_vec)\n",
    "#         temp_char_list = np.array(temp_char_list)\n",
    "        encoded_data.append(temp_char_list)\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e3ad20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_X_chars_train = one_hot_encoder(encoded_X_chars_train[:3])\n",
    "one_hot_X_chars_test = one_hot_encoder(encoded_X_chars_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91eeaa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_X_chars_tensor_train = np.array(encoded_X_chars_train)\n",
    "# encoded_X_chars_tensor_test = np.array(encoded_X_chars_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a4720f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([210,   1, 151,   1, 147])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_chars_test[0][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a171eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_tensor_train = np.array(encoded_X_train)\n",
    "encoded_X_tensor_test = np.array(encoded_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6672d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y_tensor_train = np.array(encoded_y_train)\n",
    "encoded_y_tensor_test = np.array(encoded_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "caffa13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_X_chars = [] \n",
    "# for i in range(len(padded_char_X)):\n",
    "#     temp_encoded_chars = [] \n",
    "#     for j in range(len(padded_char_X[i])):\n",
    "#         temp_encoded_chars.append(encode_sequence(padded_char_X[i][j], char_dict))\n",
    "#     encoded_X_chars.append(temp_encoded_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a963df9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 51, 115, 236,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]),\n",
       " array([209,   3, 169,  42,  47,  85,   3,  50, 171,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]),\n",
       " array([65,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0]),\n",
       " array([ 42,  47,  21, 125, 171,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]),\n",
       " array([65, 41,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_chars_train[1][:5][:5][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aae4ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('encoded_X_chars_unpadded.lis', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X_chars, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19d605fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('encoded_X_chars.list', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X_chars, fp)\n",
    "    \n",
    "# with open('encoded_X.list', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X, fp)\n",
    "    \n",
    "# with open('encoded_y.list', 'wb') as fp:\n",
    "#     pickle.dump(encoded_y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0757a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('encoded_X_chars.list', 'rb') as f:\n",
    "#     encoded_X_chars = pickle.load(f)\n",
    "    \n",
    "# with open('encoded_X.list', 'rb') as f:\n",
    "#     encoded_X = pickle.load(f)\n",
    "    \n",
    "# with open('encoded_y.list', 'rb') as f:\n",
    "#     encoded_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a8f68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_X_chars_tensor = np.array(encoded_X_chars) \n",
    "# with open('encoded_X_chars.ten', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X_chars_tensor, fp)\n",
    "    \n",
    "# del encoded_X_chars\n",
    "\n",
    "# encoded_X_tensor = np.array(encoded_X)\n",
    "# with open('encoded_X.ten', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X_tensor, fp)\n",
    "    \n",
    "# del encoded_X\n",
    "\n",
    "# encoded_y_tensor = np.array(encoded_y)\n",
    "# with open('encoded_y.ten', 'wb') as fp:\n",
    "#     pickle.dump(encoded_y_tensor, fp)\n",
    "# del encoded_y\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "\n",
    "# with open('encoded_X_chars.ten', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X_chars_tensor, fp)\n",
    "    \n",
    "# with open('encoded_X.ten', 'wb') as fp:\n",
    "#     pickle.dump(encoded_X_tensor, fp)\n",
    "    \n",
    "# with open('encoded_y.ten', 'wb') as fp:\n",
    "#     pickle.dump(encoded_y_tensor, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82765dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('encoded_X_chars.ten', 'rb') as f:\n",
    "    encoded_X_chars_tensor = pickle.load(f)\n",
    "    \n",
    "with open('encoded_X.ten', 'rb') as f:\n",
    "    encoded_X_tensor = pickle.load(f)\n",
    "    \n",
    "with open('encoded_y.ten', 'rb') as f:\n",
    "    encoded_y_tensor = pickle.load(f)\n",
    "\n",
    "# with open('encoded_X_chars_unpadded.lis', 'rb') as f:\n",
    "#     encoded_X_chars_tensor_unpadded = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b3ffec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395923, 150, 20)\n",
      "(395923, 150)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_X_chars_tensor.shape)\n",
    "print(encoded_X_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a073f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395923, 150)\n"
     ]
    }
   ],
   "source": [
    "# print(encoded_y_chars_tensor.shape)\n",
    "print(encoded_y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b6b33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (encoded_X_tensor_train, encoded_X_tensor_test,\n",
    "                                    encoded_y_tensor_train, encoded_y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca9426cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395923, 150)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9881815c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " word_input (InputLayer)     [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 150, 32)           5488544   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 150, 32)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 150, 256)         164864    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 150, 256)          0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 150, 6)           1542      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,654,950\n",
      "Trainable params: 5,654,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "9899/9899 [==============================] - 257s 26ms/step - loss: 0.0265 - accuracy: 0.9916 - sparse_categorical_crossentropy: 0.0265 - val_loss: 0.0131 - val_accuracy: 0.9956 - val_sparse_categorical_crossentropy: 0.0131\n",
      "Epoch 2/6\n",
      "9899/9899 [==============================] - 251s 25ms/step - loss: 0.0120 - accuracy: 0.9961 - sparse_categorical_crossentropy: 0.0120 - val_loss: 0.0114 - val_accuracy: 0.9962 - val_sparse_categorical_crossentropy: 0.0114\n",
      "Epoch 3/6\n",
      "9899/9899 [==============================] - 253s 26ms/step - loss: 0.0101 - accuracy: 0.9968 - sparse_categorical_crossentropy: 0.0101 - val_loss: 0.0110 - val_accuracy: 0.9963 - val_sparse_categorical_crossentropy: 0.0110\n",
      "Epoch 4/6\n",
      "9899/9899 [==============================] - 251s 25ms/step - loss: 0.0091 - accuracy: 0.9971 - sparse_categorical_crossentropy: 0.0091 - val_loss: 0.0108 - val_accuracy: 0.9964 - val_sparse_categorical_crossentropy: 0.0108\n",
      "Epoch 5/6\n",
      "9899/9899 [==============================] - 249s 25ms/step - loss: 0.0085 - accuracy: 0.9973 - sparse_categorical_crossentropy: 0.0085 - val_loss: 0.0107 - val_accuracy: 0.9965 - val_sparse_categorical_crossentropy: 0.0107\n",
      "Epoch 6/6\n",
      "9899/9899 [==============================] - 249s 25ms/step - loss: 0.0080 - accuracy: 0.9975 - sparse_categorical_crossentropy: 0.0080 - val_loss: 0.0107 - val_accuracy: 0.9966 - val_sparse_categorical_crossentropy: 0.0107\n",
      "The final accuracy in test set is: 99.66%\n"
     ]
    }
   ],
   "source": [
    "input_words_input = len(word_dict)\n",
    "embedding_vector_length = 32\n",
    "\n",
    "inputs = tf.keras.Input(shape=(encoded_X_tensor.shape[-1],), name=\"word_input\")\n",
    "x = tf.keras.layers.Embedding(input_words_input, embedding_vector_length, input_length = encoded_X_tensor.shape[-1])(inputs)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# x = tf.keras.layers.LSTM(128,return_sequences=True)(x) # 128\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(x) # 128\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# dense_layer = tf.keras.layers.Dense(64, activation=\"tanh\", name=\"dense_1\")\n",
    "# x = tf.keras.layers.TimeDistributed(dense_layer)(x)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(len(tag_dict), activation=\"softmax\", name=\"predictions\")\n",
    "outputs = tf.keras.layers.TimeDistributed(output_layer)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# recall = tf.keras.metrics.Recall(class_id=4)\n",
    "scce = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',scce]) # recall, sparse_categorical_cross_entropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_split =0.2, epochs=6, batch_size=32)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b573ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acc score: 99.66%\n",
      "Cross entropy loss: 0.0107\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Acc score: %.2f%%' % (scores[1]*100))\n",
    "# print('Recall score: %.2f' % (scores[2]))\n",
    "print('Cross entropy loss: %.4f' % (scores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9de38",
   "metadata": {},
   "source": [
    "## Character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "641478f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (encoded_X_tensor_train, encoded_X_tensor_test,\n",
    "                                    encoded_y_tensor_train, encoded_y_tensor_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ab0ebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_chars_tensor[0][5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3de1b818",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = (encoded_X_tensor_train, encoded_X_tensor_test, encoded_y_tensor_train, encoded_y_tensor_test)\n",
    "X_train_chars_one_hot, X_test_chars_one_hot, y_train, y_test = (one_hot_X_chars_train, one_hot_X_chars_test, encoded_y_tensor_train, encoded_y_tensor_test)\n",
    "X_train_chars, X_test_chars, y_train, y_test = (np.array(encoded_X_chars_train), np.array(encoded_X_chars_test), encoded_y_tensor_train, encoded_y_tensor_test)\n",
    "\n",
    "\n",
    "# X_train_chars_e5, X_test_chars_e5, y_train, y_test = train_test_split(encoded_X_chars_tensor_unpadded[:,:,-5:], encoded_y_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "67599775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_chars_b5, X_test_chars_b5\n",
    "# X_train_chars_e5, X_test_chars_e5\n",
    "\n",
    "# X_train_chars = np.concatenate([X_train_chars_b5, X_train_chars_e5], -1)\n",
    "# X_test_chars = np.concatenate([X_test_chars_b5, X_test_chars_e5], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c517eb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395923, 150, 20)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c587700a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4a1e2352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, 150, 20)]    0           []                               \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, 150, 20, 32)  7648        ['char_input[0][0]']             \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_18 (TFOpLam  (4,)                0           ['embedding_13[0][0]']           \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_19 (TFOpLam  (4,)                0           ['embedding_13[0][0]']           \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_16 (TFOpLam  (4,)                0           ['embedding_13[0][0]']           \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_17 (TFOpLam  (4,)                0           ['embedding_13[0][0]']           \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_18 (S  ()                  0           ['tf.compat.v1.shape_18[0][0]']  \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_19 (S  ()                  0           ['tf.compat.v1.shape_19[0][0]']  \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " word_input (InputLayer)        [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_16 (S  ()                  0           ['tf.compat.v1.shape_16[0][0]']  \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_17 (S  ()                  0           ['tf.compat.v1.shape_17[0][0]']  \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  ()                  0           ['tf.__operators__.getitem_18[0][\n",
      " )                                                               0]',                             \n",
      "                                                                  'tf.__operators__.getitem_19[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 150, 32)      5488544     ['word_input[0][0]']             \n",
      "                                                                                                  \n",
      " tf.reshape_2 (TFOpLambda)      (None, 150, 640)     0           ['embedding_13[0][0]',           \n",
      "                                                                  'tf.__operators__.getitem_16[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.__operators__.getitem_17[0][\n",
      "                                                                 0]',                             \n",
      "                                                                  'tf.math.multiply_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 150, 672)     0           ['embedding_12[0][0]',           \n",
      "                                                                  'tf.reshape_2[0][0]']           \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 150, 256)    820224      ['tf.concat_2[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 150, 256)     0           ['bidirectional_4[0][0]']        \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, 150, 6)      1542        ['dropout_6[0][0]']              \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,317,958\n",
      "Trainable params: 6,317,958\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "9899/9899 [==============================] - 296s 28ms/step - loss: 0.0184 - accuracy: 0.9939 - sparse_categorical_crossentropy: 0.0184 - val_loss: 0.0109 - val_accuracy: 0.9964 - val_sparse_categorical_crossentropy: 0.0109\n",
      "Epoch 2/5\n",
      "9899/9899 [==============================] - 218s 22ms/step - loss: 0.0098 - accuracy: 0.9968 - sparse_categorical_crossentropy: 0.0098 - val_loss: 0.0098 - val_accuracy: 0.9968 - val_sparse_categorical_crossentropy: 0.0098\n",
      "Epoch 3/5\n",
      "9899/9899 [==============================] - 213s 22ms/step - loss: 0.0083 - accuracy: 0.9973 - sparse_categorical_crossentropy: 0.0083 - val_loss: 0.0095 - val_accuracy: 0.9969 - val_sparse_categorical_crossentropy: 0.0095\n",
      "Epoch 4/5\n",
      "9899/9899 [==============================] - 212s 21ms/step - loss: 0.0074 - accuracy: 0.9976 - sparse_categorical_crossentropy: 0.0074 - val_loss: 0.0094 - val_accuracy: 0.9969 - val_sparse_categorical_crossentropy: 0.0094\n",
      "Epoch 5/5\n",
      "9899/9899 [==============================] - 210s 21ms/step - loss: 0.0067 - accuracy: 0.9979 - sparse_categorical_crossentropy: 0.0067 - val_loss: 0.0096 - val_accuracy: 0.9969 - val_sparse_categorical_crossentropy: 0.0096\n",
      "The final accuracy in test set is: 99.69%\n"
     ]
    }
   ],
   "source": [
    "input_words_input = len(word_dict)\n",
    "input_chars_input = len(char_dict)\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "inputs1 = tf.keras.Input(shape=(encoded_X_tensor.shape[-1],), name=\"word_input\")\n",
    "inputs2 = tf.keras.Input(shape=(X_train_chars.shape[-2], X_train_chars.shape[-1],), name=\"char_input\")\n",
    "x1 = tf.keras.layers.Embedding(input_words_input, embedding_vector_length, input_length = encoded_X_tensor.shape[-1])(inputs1)\n",
    "x2 = tf.keras.layers.Embedding(input_chars_input, embedding_vector_length, input_length = encoded_X_chars_tensor.shape[-1])(inputs2)\n",
    "x2 = tf.reshape(x2, [tf.shape(x2)[0], tf.shape(x2)[1], tf.shape(x2)[2]*tf.shape(x2)[3]])\n",
    "x = tf .concat([x1,x2], 2)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(x) # 128\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "output_layer = tf.keras.layers.Dense(len(tag_dict), activation=\"softmax\", name=\"predictions\")\n",
    "outputs = tf.keras.layers.TimeDistributed(output_layer)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
    "\n",
    "# recall = tf.keras.metrics.Recall(class_id=4)\n",
    "scce = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',scce]) # recall, sparse_categorical_cross_entropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit((X_train, X_train_chars), y_train, validation_split =0.2, epochs=5, batch_size=32)\n",
    "\n",
    "scores = model.evaluate((X_test, X_test_chars), y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77ab55cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acc score: 99.69%\n",
      "Cross entropy loss: 0.0097\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Acc score: %.2f%%' % (scores[1]*100))\n",
    "print('Cross entropy loss: %.4f' % (scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c0dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5774c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316738, 150, 10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_X_tensor, encoded_y_tensor, test_size=0.2, random_state=42)\n",
    "X_train_chars, X_test_chars, y_train, y_test = train_test_split(encoded_X_chars_tensor_unpadded, encoded_y_tensor, test_size=0.2, random_state=42)\n",
    "# X_train_chars_e5, X_test_chars_e5, y_train, y_test = train_test_split(encoded_X_chars_tensor_unpadded[:,:,-5:], encoded_y_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c8ecaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_chars[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "93af62b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_chars_tensor.shape[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6519d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words_input = len(word_dict)\n",
    "input_chars_input = len(char_dict)\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "inputs1 = tf.keras.Input(shape=(encoded_X_tensor.shape[-1],), name=\"word_input\")\n",
    "x1 = tf.keras.layers.Embedding(input_words_input, embedding_vector_length, input_length = encoded_X_tensor.shape[-1])(inputs1)\n",
    "\n",
    "\n",
    "### ---- \n",
    "\n",
    "# inputs1 = tf.keras.Input(shape=(encoded_X_tensor.shape[-1],), name=\"word_input\")\n",
    "# inputs2 = tf.keras.Input(shape=(len(char_dict)), name=\"char_input\")\n",
    "\n",
    "# x1 = tf.keras.layers.Embedding(input_words_input, embedding_vector_length, input_length = encoded_X_tensor.shape[-1])(inputs1)\n",
    "# x2 = tf.keras.layers.Embedding(input_chars_input, embedding_vector_length, input_length = encoded_X_chars_tensor.shape[-1])(inputs2)\n",
    "\n",
    "\n",
    "# ------\n",
    "\n",
    "inputs2 = tf.keras.Input(shape=(len(char_dict)), name=\"char_input\")\n",
    "x2 = tf.keras.layers.Embedding(input_chars_input, embedding_vector_length, input_length = encoded_X_chars_tensor.shape[-2])(inputs2)\n",
    "print(x2.shape)\n",
    "x2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))(x2) # 128\n",
    "print(x2.shape)\n",
    "x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "x2_dense_2 = tf.keras.layers.Dense(8, activation=\"tanh\", name=\"x2_dense\")\n",
    "x2 = x2_dense_2(x2)\n",
    "\n",
    "\n",
    "# x2 = tf.reshape(x2, [tf.shape(x2)[0], tf.shape(x2)[1], tf.shape(x2)[2]*tf.shape(x2)[3]])\n",
    "x = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x) # 128\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(len(tag_dict), activation=\"softmax\", name=\"predictions\")\n",
    "outputs = tf.keras.layers.TimeDistributed(output_layer)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
    "\n",
    "\n",
    "# recall = tf.keras.metrics.Recall(class_id=4)\n",
    "scce = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',scce]) # recall, sparse_categorical_cross_entropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit((X_train, X_train_chars_one_hot), y_train, validation_split =0.2, epochs=10, batch_size=16)\n",
    "\n",
    "scores = model.evaluate((X_test, X_test_chars), y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f35c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5722ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 150, 20, 32)\n",
      "(None, 150, 64)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, 150, 20)]    0           []                               \n",
      "                                                                                                  \n",
      " word_input (InputLayer)        [(None, 150)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_23 (Embedding)       (None, 150, 20, 32)  7648        ['char_input[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_22 (Embedding)       (None, 150, 32)      5488544     ['word_input[0][0]']             \n",
      "                                                                                                  \n",
      " time_distributed_12 (TimeDistr  (None, 150, 64)     16640       ['embedding_23[0][0]']           \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " bidirectional_16 (Bidirectiona  (None, 150, 128)    49664       ['embedding_22[0][0]']           \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 150, 64)      0           ['time_distributed_12[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 150, 128)     0           ['bidirectional_16[0][0]']       \n",
      "                                                                                                  \n",
      " x2_dense (Dense)               (None, 150, 8)       520         ['dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 150, 136)     0           ['dropout_14[0][0]',             \n",
      "                                                                  'x2_dense[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_18 (Bidirectiona  (None, 150, 256)    271360      ['concatenate_3[0][0]']          \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 150, 256)     0           ['bidirectional_18[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_13 (TimeDistr  (None, 150, 6)      1542        ['dropout_16[0][0]']             \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,835,918\n",
      "Trainable params: 5,835,918\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "15837/15837 [==============================] - 807s 51ms/step - loss: 0.0196 - accuracy: 0.9936 - sparse_categorical_crossentropy: 0.0196 - val_loss: 0.0110 - val_accuracy: 0.9963 - val_sparse_categorical_crossentropy: 0.0110\n",
      "Epoch 2/10\n",
      "15837/15837 [==============================] - 790s 50ms/step - loss: 0.0097 - accuracy: 0.9969 - sparse_categorical_crossentropy: 0.0097 - val_loss: 0.0099 - val_accuracy: 0.9967 - val_sparse_categorical_crossentropy: 0.0099\n",
      "Epoch 3/10\n",
      "15782/15837 [============================>.] - ETA: 2s - loss: 0.0080 - accuracy: 0.9975 - sparse_categorical_crossentropy: 0.0080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15837/15837 [==============================] - 798s 50ms/step - loss: 0.0070 - accuracy: 0.9978 - sparse_categorical_crossentropy: 0.0070 - val_loss: 0.0097 - val_accuracy: 0.9968 - val_sparse_categorical_crossentropy: 0.0097\n",
      "Epoch 5/10\n",
      "15837/15837 [==============================] - 811s 51ms/step - loss: 0.0062 - accuracy: 0.9980 - sparse_categorical_crossentropy: 0.0062 - val_loss: 0.0103 - val_accuracy: 0.9967 - val_sparse_categorical_crossentropy: 0.0103\n",
      "Epoch 6/10\n",
      "15837/15837 [==============================] - 783s 49ms/step - loss: 0.0055 - accuracy: 0.9982 - sparse_categorical_crossentropy: 0.0055 - val_loss: 0.0108 - val_accuracy: 0.9966 - val_sparse_categorical_crossentropy: 0.0108\n",
      "Epoch 7/10\n",
      "15837/15837 [==============================] - 788s 50ms/step - loss: 0.0049 - accuracy: 0.9984 - sparse_categorical_crossentropy: 0.0049 - val_loss: 0.0118 - val_accuracy: 0.9965 - val_sparse_categorical_crossentropy: 0.0118\n",
      "Epoch 8/10\n",
      " 5324/15837 [=========>....................] - ETA: 7:48 - loss: 0.0042 - accuracy: 0.9986 - sparse_categorical_crossentropy: 0.0042"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,scce]) \u001b[38;5;66;03m# recall, sparse_categorical_cross_entropy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_chars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate((X_test, X_test_chars), y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe final accuracy in test set is: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (scores[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_words_input = len(word_dict)\n",
    "input_chars_input = len(char_dict)\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "inputs1 = tf.keras.Input(shape=(encoded_X_tensor.shape[-1],), name=\"word_input\")\n",
    "x1 = tf.keras.layers.Embedding(input_words_input, embedding_vector_length, input_length = encoded_X_tensor.shape[-1])(inputs1)\n",
    "x1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(x1) # 128\n",
    "x1 = tf.keras.layers.Dropout(0.2)(x1)\n",
    "\n",
    "\n",
    "\n",
    "inputs2 = tf.keras.Input(shape=(encoded_X_chars_tensor.shape[-2], encoded_X_chars_tensor.shape[-1],), name=\"char_input\")\n",
    "x2 = tf.keras.layers.Embedding(input_chars_input, embedding_vector_length, input_length = encoded_X_chars_tensor.shape[-1])(inputs2)\n",
    "print(x2.shape)\n",
    "x2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))(x2) # 128\n",
    "print(x2.shape)\n",
    "x2 = tf.keras.layers.Dropout(0.2)(x2)\n",
    "\n",
    "### ----- Start Experimental -------\n",
    "x2 = tf.reshape(x2, [tf.shape(x2)[0], tf.shape(x2)[1], tf.shape(x2)[2]*tf.shape(x2)[3]])\n",
    "\n",
    "# x2_dense_2 = tf.keras.layers.Dense(8, activation=\"tanh\", name=\"x2_dense\")\n",
    "# x2 = x2_dense_2(x2)\n",
    "\n",
    "### ----- End Experimental -------\n",
    "\n",
    "\n",
    "# x2 = tf.reshape(x2, [tf.shape(x2)[0], tf.shape(x2)[1], tf.shape(x2)[2]*tf.shape(x2)[3]])\n",
    "x = tf.keras.layers.concatenate([x1,x2],axis=-1)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x) # 128\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(len(tag_dict), activation=\"softmax\", name=\"predictions\")\n",
    "outputs = tf.keras.layers.TimeDistributed(output_layer)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=(inputs1, inputs2), outputs=outputs)\n",
    "\n",
    "\n",
    "# recall = tf.keras.metrics.Recall(class_id=4)\n",
    "scce = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy',scce]) # recall, sparse_categorical_cross_entropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit((X_train, X_train_chars), y_train, validation_split =0.2, epochs=10, batch_size=16)\n",
    "\n",
    "scores = model.evaluate((X_test, X_test_chars), y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ea427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('Acc score: %.2f%%' % (scores[1]*100))\n",
    "# print('Recall score: %.2f' % (scores[2]))\n",
    "print('Cross entropy loss: %.4f%%' % (scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430880b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64c6e4b5",
   "metadata": {},
   "source": [
    "## From Scratch Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bf227",
   "metadata": {},
   "source": [
    "## From Scratch Implementation\n",
    "\n",
    "The code from this section is mainly borrowed from the following sources: \n",
    "1. https://github.com/kscanne/5755/tree/master/mutations\n",
    "2. https://www.mygreatlearning.com/blog/pos-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40215bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-09-23 22:53:54.472448: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 22:53:54.692126: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-23 22:53:54.732474: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-23 22:53:59.358388: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-09-23 22:53:59.359934: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-09-23 22:53:59.359953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import random \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d86548",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.tsv', 'r') as f: \n",
    "    train_data = f.read()\n",
    "    \n",
    "with open('test.tsv', 'r') as f: \n",
    "    test_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d123d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_lines_train = train_data.strip('\\n').split('\\n')\n",
    "splited_lines_test = test_data.strip('\\n').split('\\n')\n",
    "\n",
    "for i in range(len(splited_lines_train)): \n",
    "    splited_lines_train[i] = splited_lines_train[i].strip(' ').split('\\t')\n",
    "    \n",
    "for i in range(len(splited_lines_test)): \n",
    "    splited_lines_test[i] = splited_lines_test[i].strip(' ').split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286610c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ansin', 'N']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_lines_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e09a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_list(splited_lines):\n",
    "    temp_tokens= [] \n",
    "    sentence_list = []\n",
    "    global_list_of_words = [] \n",
    "    global_list_of_chars = []\n",
    "    global_list_of_tags = []\n",
    "    for w,t in splited_lines: \n",
    "        global_list_of_words.append(w)\n",
    "        global_list_of_chars += [c for c in w]\n",
    "        global_list_of_tags.append(t)\n",
    "        if w == '<S>':\n",
    "            sentence_list.append(temp_tokens)\n",
    "            temp_tokens = []\n",
    "            temp_tokens.append(tuple((w,t)))\n",
    "        else:\n",
    "            temp_tokens.append(tuple((w,t)))\n",
    "    sentence_list.append(temp_tokens)\n",
    "    return sentence_list, global_list_of_words, global_list_of_chars, global_list_of_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffb1814",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list_train, global_list_of_words, global_list_of_chars, global_list_of_tags = get_sentence_list(splited_lines_train)\n",
    "sentence_list_test, _, _, _= get_sentence_list(splited_lines_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0e56af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train, Xy_test = (sentence_list_train, sentence_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa9aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000000\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "train_tagged_words = [ tup for sent in Xy_train for tup in sent]\n",
    "test_tagged_words = [ tup for sent in Xy_test for tup in sent]\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91740d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('níos', 'N')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged_words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049a5724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'S', 'H', 'N', 'U', 'T'}\n"
     ]
    }
   ],
   "source": [
    "#use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_tagged_words}\n",
    "print(len(tags))\n",
    "print(tags)\n",
    " \n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_tagged_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a534c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)#total number of times the passed tag occurred in train_bag\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "#now calculate the total number of times the passed word occurred as the passed tag.\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    " \n",
    "     \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca277048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute  Transition Probability\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd44a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1699642e-02 1.5994346e-03 9.4544768e-01 1.1905875e-03 6.2662497e-05]\n",
      " [3.9538410e-02 2.3849746e-03 9.5707047e-01 8.3225680e-04 1.7390441e-04]\n",
      " [1.0547736e-01 9.1363927e-03 8.4339511e-01 3.7937343e-02 4.0536942e-03]\n",
      " [3.8271528e-02 6.9395616e-04 9.6029162e-01 6.9089909e-04 5.1970284e-05]\n",
      " [5.7859290e-02 2.8943976e-03 9.3898839e-01 8.5972206e-05 1.7194441e-04]]\n"
     ]
    }
   ],
   "source": [
    "# creating t x t transition matrix of tags, t= no of tags\n",
    "# Matrix(i, j) represents P(jth tag after the ith tag)\n",
    " \n",
    "tags_matrix = np.zeros((len(tags), len(tags)), dtype='float32')\n",
    "for i, t1 in enumerate(list(tags)):\n",
    "    for j, t2 in enumerate(list(tags)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    " \n",
    "print(tags_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80e48f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>H</th>\n",
       "      <th>N</th>\n",
       "      <th>U</th>\n",
       "      <th>T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.945448</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.039538</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.957070</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.105477</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.843395</td>\n",
       "      <td>0.037937</td>\n",
       "      <td>0.004054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.960292</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>0.057859</td>\n",
       "      <td>0.002894</td>\n",
       "      <td>0.938988</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          S         H         N         U         T\n",
       "S  0.051700  0.001599  0.945448  0.001191  0.000063\n",
       "H  0.039538  0.002385  0.957070  0.000832  0.000174\n",
       "N  0.105477  0.009136  0.843395  0.037937  0.004054\n",
       "U  0.038272  0.000694  0.960292  0.000691  0.000052\n",
       "T  0.057859  0.002894  0.938988  0.000086  0.000172"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "#the table is same as the transition table shown in section 3 of article\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(tags), index=list(tags))\n",
    "display(tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a614764",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_flat = [w for w,t in train_tagged_words]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94bf52ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ansin', ')', 'tá', 'níos', 'lú', 'gaeilge', 'ag', 'na', 'gardaí', 'ná']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_flat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "probablity_dictionary = {}\n",
    "for w in trained_flat: \n",
    "    if w not in probablity_dictionary.keys(): \n",
    "        probablity_dictionary[w] = {}\n",
    "    for t in tags: \n",
    "        if t not in probablity_dictionary[w].keys():\n",
    "            probablity_dictionary[w][t] = word_given_tag(w, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f74805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('probability.dict', 'wb') as fp:\n",
    "    pickle.dump(probablity_dictionary, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a8892657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(824, 8584022)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_given_tag('bí', 'N',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f51d2bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    print(T)\n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['N', tag]\n",
    "                try:\n",
    "                    transition_p = tags_df.loc[state[-1], tag]\n",
    "                except:\n",
    "                    pass\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = probablity_dictionary[words[key]][tag][0]/probablity_dictionary[words[key]][tag][1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40874ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_list = [] \n",
    "for i in range(len(Xy_test)):\n",
    "    if len(Xy_test)==0:\n",
    "        empty_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c019d40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bec36fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "counter = 0\n",
    "for i in range(len(Xy_test)): \n",
    "    test_run = [Xy_test[i]]\n",
    "    # list of tagged words\n",
    "    test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "    # list of untagged words\n",
    "    test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "    \n",
    "    \n",
    "    #Here We will only test 10 sentences to check the accuracy\n",
    "    #as testing the whole training set takes huge amount of time\n",
    "    start = time.time()\n",
    "    tagged_seq = Viterbi(test_tagged_words)\n",
    "    end = time.time()\n",
    "    difference = end-start\n",
    "\n",
    "    # accuracy\n",
    "    check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    "    try:\n",
    "        accuracy = len(check)/len(tagged_seq)\n",
    "        accuracy_list.append(accuracy*100)\n",
    "    except:\n",
    "        pass\n",
    "    counter += 1 \n",
    "    if counter%10==0 : \n",
    "        print(\"Sentence %d is processed.\"%(counter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9839f5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S>', \"d'fhás\", 'martin', 'walser', 'aníos', 'faoi']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d57a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rndom = [random.randint(1,len(Xy_test)) for x in range(10)]\n",
    "test_run = [Xy_test[i] for i in rndom]\n",
    " \n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    " \n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3ae5436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amárach'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged_words[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here We will only test 10 sentences to check the accuracy\n",
    "#as testing the whole training set takes huge amount of time\n",
    "start = time.time()\n",
    "tagged_seq = Viterbi(test_tagged_words)\n",
    "end = time.time()\n",
    "difference = end-start\n",
    " \n",
    "print(\"Time taken to run viterbi on test set is in seconds: \", difference)\n",
    " \n",
    "# accuracy\n",
    "check = [i for i, j in zip(tagged_seq, test_run_base) if i == j] \n",
    " \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Viterbi Algorithm Accuracy on the test set is: ',accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a05bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(splited_lines_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a4cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tags = dict((w, cfd[w].max()) for w in global_list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cd7d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = [(t,w) for sent in sentence_list_train for (w,t) in sent]\n",
    "wordgiventag = nltk.ConditionalFreqDist(flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0be8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is P(w|t), unsmoothed!\n",
    "def P(w,t):\n",
    "    return wordgiventag[t][w] / wordgiventag[t].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67823f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigrams = [(x,y) for sent in sentence_list_train for x,y in nltk.bigrams([t for (w,t) in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f90fc87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('N', 'N')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_bigrams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c435374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6849694\n"
     ]
    }
   ],
   "source": [
    "tag_bigram_counts = nltk.ConditionalFreqDist(tag_bigrams)\n",
    "# this is count of noun tags following adjective tags (normal order in English)\n",
    "print(tag_bigram_counts['N']['N'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7ff09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is P(t2|t1), unsmoothed again!\n",
    "def tagP(t2,t1):\n",
    "    return tag_bigram_counts[t1][t2] / tag_bigram_counts[t1].N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb9b730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_start = nltk.FreqDist(sent[0][1] for sent in sentence_list_train)\n",
    "def initP(t):\n",
    "    return sentence_start[t] / sentence_start.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be3977e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(V,tag_list,t,i):\n",
    "    ans=-1\n",
    "    best=None\n",
    "    for s in tag_list:\n",
    "        temp=V[(s,i-1)]*tagP(t,s)\n",
    "        if temp > ans:\n",
    "            ans = temp\n",
    "            best = s\n",
    "    return (best,ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07ea2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printV(sentence,tag_list,V,B):\n",
    "    for i in range(len(sentence)):\n",
    "        print('i='+str(i)+' ['+sentence[i]+']')\n",
    "        for t in tag_list:\n",
    "            if V[(t,i)] != 0:\n",
    "                toprint='  '+t+'='+str(V[(t,i)])\n",
    "                if i>0:\n",
    "                    toprint += ' (from '+B[(t,i)]+')'\n",
    "                print(toprint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a5d29333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, labels ):\n",
    "    V = dict()    # keys are (t,i) where t is a tag (row label) and i is position in sentence (column label)\n",
    "    B = dict()    # same keys as V; this stores the \"backpointers\" to remember best tag sequence\n",
    "    tag_list = sentence_start.keys()\n",
    "    for t in tag_list:\n",
    "        V[(t,0)] = initP(t)*P(sentence[0],t)\n",
    "    for i in range(1,len(sentence)):\n",
    "        for t in tag_list:\n",
    "            pair = argmax(V,tag_list,t,i)\n",
    "            B[(t,i)] = pair[0]\n",
    "            V[(t,i)] = pair[1]*P(sentence[i],t)\n",
    "    counter = 0\n",
    "    for i in range(len(list(V.keys()))-1):\n",
    "        \n",
    "        if labels[i] == B[list(B.keys())[i]]:\n",
    "            counter += 1 \n",
    "    return 100*counter / (len(labels)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ae0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3859036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "counter = 0\n",
    "total = 0\n",
    "for i in range(len(Xy_test)): \n",
    "    test_run = [Xy_test[i]]\n",
    "    # list of tagged words\n",
    "    test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "    # list of untagged words\n",
    "    test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "    test_tagged_labels = [tup[1] for sent in test_run for tup in sent]\n",
    "    \n",
    "    try: \n",
    "        total += viterbi(test_tagged_words, test_tagged_labels)\n",
    "        counter += 1 \n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "average = total/counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4bcd4e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.87601429414087"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
