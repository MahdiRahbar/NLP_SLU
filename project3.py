# -*- coding: utf-8 -*-
"""project3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12dMO9t1xHpQMSz7VDE1CNHvqpEIoWfhf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import random
import nltk

df = pd.read_csv('/content/train.tsv', sep='\t',header= None,encoding='utf8', quoting=3)

df.head()

df.columns = ['words','word_division']
df

df['word_division'] = df['word_division'].apply(lambda x: x.lower())

df['word_division'] = df['word_division'].apply(lambda x: x.split("-") )

df

def adding_chars(lenghts):
  temp=[]
  for i in lenghts:
    i=len(i)
    if i==1:
      temp.append("B")
    elif i==2:
      temp.append("B")
      temp.append("I")
    else:
      temp.append("B")
      temp.append("I")
      temp.append("I")
  return temp

def adding_tags(test):
  strings=adding_chars(test)
  return strings
print(adding_tags(['k', 'on', 's', 'i', 'l', 't', 'ank']))

df['word_division_tags'] = df['word_division'].apply(adding_tags)
df['word_division_tags']

labels =["PAD","B","I"]
lenth_of_labels = len(labels)
lenth_of_labels

letters = ["a","an","b","ch","d","e","en","è","f","g","ng","i","j","k","l","m","n","o","on","ou","ò","p","r","s","t","ui","v","w","y","z","PAD","à","oun"]
length_of_letters = len(letters)
length_of_letters

"""Code from  Dr. Scannell"""

letter_indexes = {w:i for i, w in enumerate(letters)}
labels_indexes = {t:i for i, t in enumerate(labels)}

df["le_index"] = df["word_division"].apply(lambda x: [letter_indexes[key] for key in x])
df["la_index"] = df["word_division_tags"].apply(lambda x: [labels_indexes[key] for key in x])

df.head()

from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional 
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix
from tensorflow.keras.metrics import CategoricalAccuracy

l=[]
for i in df['word_division']:
  l.append(len(i))
max(l)

max_length = 16

X = pad_sequences(maxlen=max_length, sequences=df['le_index'])
Y = pad_sequences(maxlen=max_length, sequences=df['la_index'])
Y = to_categorical(Y, num_classes=3)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)

model = Sequential()
model.add(Embedding(input_dim=33, output_dim=50, input_length=max_length))
model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))
model.add(TimeDistributed(Dense(3, activation="softmax")))
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
print(model.summary())

n_epochs=5
history = model.fit(X_train, y_train, batch_size=16, epochs=n_epochs, validation_split=0.1, verbose=1)

y_original = []
y_predicted = []
for i in range(len(X_test)):
  predicted = model.predict(np.array([X_test[i]]))
  m = np.argmax(predicted, axis=-1)
  p_original = np.argmax(y_test[i],axis=-1)
  for i in range(len(p_original)):
    if p_original[i] == 2: 
      break
    y_original.append(p_original[i])
    y_predicted.append(m[0][i])

from sklearn.metrics import precision_recall_fscore_support
precision_recall_fscore_support(y_original, y_predicted,  labels=[0, 1, 2])