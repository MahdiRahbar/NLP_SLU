{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDS6ekfBVIqj",
        "outputId": "515c5e6b-aadc-4c7c-c59b-6218f40550c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.mygreatlearning.com/blog/pos-tagging/(took reference from the great learning)"
      ],
      "metadata": {
        "id": "9eV0bfy-bDiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint, time\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "dy4-XeslTmab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Viterbi_Algorithm:\n",
        "  def __init__(self):\n",
        "      self.train_unique_words = []\n",
        "      self.train_unique_tags = []\n",
        "      self.words_emission_count = pd.DataFrame()\n",
        "      self.words_emission_prob = pd.DataFrame()\n",
        "      self.tags_transition_count = pd.DataFrame()\n",
        "      self.tags_transition_prob = pd.DataFrame()\n",
        "\n",
        "      self.input_pred = pd.DataFrame()\n",
        "      self.words_bestTags = {}\n",
        "      self.results = []\n",
        "      self.result_df = pd.DataFrame()\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LOAD DATA\n",
        "  def load_data_as_dataFrame(self, filePath):\n",
        "      testfile = open(filePath, 'r')\n",
        "      words = []\n",
        "      tags = []\n",
        "      for line in testfile:\n",
        "          pieces = line.rstrip(\"\\n\").split(\"\\t\")\n",
        "          words.append(pieces[0])\n",
        "          tags.append(pieces[1])\n",
        "\n",
        "      # --------------------------------------------------------------- Preparing DataFrame\n",
        "      data = pd.DataFrame(columns=[\"words\", \"tags\"])\n",
        "      data['words'] = [str(word) for word in words]\n",
        "      data['tags'] = [str(tag) for tag in tags]\n",
        "\n",
        "      return data\n",
        "\n",
        "  def prepare_words_tags(self, data):\n",
        "      # --------------------------------------------------------------- Preparing Unique Words and Unique Tags\n",
        "      data_not_S = data[data[\"words\"] != \"<S>\"]\n",
        "      self.train_unique_words = list(set(data_not_S['words']))\n",
        "      self.train_unique_tags = list(set(data_not_S['tags']))\n",
        "\n",
        "      # --------------------------------------------------------------- Prining the POS and those counts\n",
        "      # print(\"===== TAGS & COUNT =====\")\n",
        "      # print(data_not_S['tags'].value_counts())\n",
        "      return \"\"\n",
        "\n",
        "  def load_train_data(self, filePath):\n",
        "      startTime = datetime.now()\n",
        "      data = self.load_data_as_dataFrame(filePath)\n",
        "      _ = self.prepare_words_tags(data)\n",
        "      endTime = datetime.now()\n",
        "      print(\"===== PREPARING TRAINING DATA IS SUCCESSFULLY COMPLETED =====\")\n",
        "      print(\"Data Preparing Time Taken : \", endTime - startTime)\n",
        "      return data\n",
        "\n",
        "  def load_test_data(self, filePath):\n",
        "      startTime = datetime.now()\n",
        "      data = self.load_data_as_dataFrame(filePath)\n",
        "      endTime = datetime.now()\n",
        "      print(\"===== PREPARING VALIDATE DATA IS SUCCESSFULLY COMPLETED =====\")\n",
        "      print(\"Data Preparing Time Taken : \", endTime - startTime)\n",
        "      return data\n",
        "\n",
        "  def load_pred_data(self, filePath):\n",
        "      startTime = datetime.now()\n",
        "      testfile = open(filePath, 'r')\n",
        "      words = []\n",
        "      for line in testfile:\n",
        "          pieces = line.rstrip(\"\\n\")\n",
        "          words.append(pieces[0])\n",
        "\n",
        "      # --------------------------------------------------------------- Preparing DataFrame\n",
        "      data = pd.DataFrame(columns=[\"words\"])\n",
        "      data['words'] = [str(word) for word in words]\n",
        "\n",
        "      endTime = datetime.now()\n",
        "      print(\"===== PREPARING PREDICTION DATA IS SUCCESSFULLY COMPLETED =====\")\n",
        "      print(\"Data Preparing Time Taken : \", endTime - startTime)\n",
        "      return data\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TRAINING\n",
        "  def find_words_emission_CountProbTables(self, data):\n",
        "      # --------------------------------------------------------------- Emission Count Table\n",
        "      self.words_emission_count = pd.crosstab(data['words'], data['tags'])\n",
        "\n",
        "      # --------------------------------------------------------------- Emission Probabilities Table\n",
        "      self.words_emission_prob = self.words_emission_count.copy()\n",
        "      for tag in self.train_unique_tags:\n",
        "          self.words_emission_prob[tag] /= self.words_emission_prob[tag].sum()\n",
        "      return \"\"\n",
        "\n",
        "  def find_tags_transition_CountProbTables(self, data):\n",
        "      # --------------------------------------------------------------- Transition Count Table\n",
        "      data.loc[data[data['words'] == \"<S>\"].index, \"tags\"] = \"<S>\"\n",
        "      data['tags_shift_1'] = data['tags'].shift(1)\n",
        "      data.loc[0, 'tags_shift_1'] = \"<S>\"\n",
        "      self.tags_transition_count = pd.crosstab(data['tags_shift_1'], data['tags'])\n",
        "      self.tags_transition_count.loc['<E>', :] = self.tags_transition_count['<S>']\n",
        "      self.tags_transition_count = self.tags_transition_count[self.train_unique_tags]\n",
        "\n",
        "      # --------------------------------------------------------------- Transition Probabilities Table\n",
        "      self.tags_transition_prob = self.tags_transition_count.copy()\n",
        "      for tag in self.train_unique_tags:\n",
        "          self.tags_transition_prob[tag] /= self.tags_transition_prob[tag].sum()\n",
        "\n",
        "      return \"\"\n",
        "\n",
        "  def train(self, data):\n",
        "      startTime = datetime.now()\n",
        "      _ = self.prepare_words_tags(data)\n",
        "      _ = self.find_words_emission_CountProbTables(data)\n",
        "      _ = self.find_tags_transition_CountProbTables(data)\n",
        "      endTime = datetime.now()\n",
        "      print(\"===== TRAINING IS SUCCESSFULLY COMPLETED =====\")\n",
        "      print(\"Training Duration Time Taken : \", endTime-startTime)\n",
        "      return \"\"\n",
        "\n",
        "  # %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PREDICTIONS\n",
        "\n",
        "  # --------------------------------------------------------------- Predict possible tag probability for the Word\n",
        "  def predict_word_tagProbabilities(self, i, word, prevTag):\n",
        "      if word in self.train_unique_words:\n",
        "          for tag in self.train_unique_tags:\n",
        "              self.input_pred.loc[str(i) + '_' + word, tag] = self.words_emission_prob.loc[word, tag] * self.tags_transition_prob.loc[prevTag, tag]\n",
        "      else:\n",
        "          for tag in self.train_unique_tags:\n",
        "              self.input_pred.loc[str(i) + '_' + word, tag] = self.tags_transition_prob.loc[prevTag, tag]\n",
        "\n",
        "  # finding best values\n",
        "  def predict_word_bestTagProbability(self, i, word):\n",
        "      tempdata = self.input_pred.loc[str(i) + '_' + word, :]\n",
        "      bestValue = tempdata.max()\n",
        "      result = dict(tempdata[tempdata == bestValue])\n",
        "      self.words_bestTags[i] = (word, list(result.keys())[0], list(result.values())[0])\n",
        "      return \"\"\n",
        "\n",
        "  def save_predictions(self):\n",
        "      for i in range(len(self.words_bestTags)):\n",
        "          self.result_df.loc[len(self.result_df), [\"words\", \"tags\"]] = [self.words_bestTags[i][0], self.words_bestTags[i][1]]\n",
        "\n",
        "      self.result_df.loc[len(self.result_df), [\"words\", \"tags\"]] = [\"<S>\", \"<S>\"]\n",
        "\n",
        "  def predict_tags_of_words(self, X):\n",
        "      # Creating empty dataframe to store the possible probabilities between words and tags\n",
        "      rows = [str(i) + '_' + word for i, word in enumerate(X)]\n",
        "      cols = self.train_unique_tags\n",
        "      zero_data = np.zeros(shape=(len(rows), len(cols)))\n",
        "      self.input_pred = pd.DataFrame(zero_data, index=rows, columns=cols)\n",
        "      \n",
        "      # Creating empty dictionary to store best tags (which tag has max probability) of each word \n",
        "      self.words_bestTags = {}\n",
        "\n",
        "      # --------------------------------------------------------------- Finding Probabilities between the words and tags\n",
        "      for i in range(len(X)):\n",
        "          word = X[i]\n",
        "          if i==0:\n",
        "              prevTag = \"<S>\"\n",
        "          else:\n",
        "              prevTag = self.words_bestTags[i - 1][1]\n",
        "          self.predict_word_tagProbabilities(i, word, prevTag)  # Finding all the possible tags & possible probabilities of the word\n",
        "          _ = self.predict_word_bestTagProbability(i, word)     # Finding the tag which has the max probability for the word\n",
        "      # --------------------------------------------------------------- Saving the Results (Predictions of tags) of Sentence\n",
        "\n",
        "      Y_pred = [self.words_bestTags[i][1] for i in range(len(self.words_bestTags))]\n",
        "      return Y_pred\n",
        "\n",
        "  def predict_data(self, data):\n",
        "      startTime = datetime.now()\n",
        "      \n",
        "      if data.loc[len(data) - 1, \"words\"] != \"<S>\":\n",
        "          data.loc[len(data), [\"words\", \"tags\"]] = [\"<S>\", \"<S>\"]\n",
        "      self.result_df = pd.DataFrame()\n",
        "      sentence = []\n",
        "      for word in data[\"words\"]:\n",
        "          if word != \"<S>\":\n",
        "              sentence.append(word)\n",
        "          elif len(sentence)>0:\n",
        "              _ = self.predict_tags_of_words(sentence)\n",
        "              sentence = []\n",
        "              self.save_predictions()\n",
        "      else:\n",
        "          self.result_df = self.result_df.drop(len(self.result_df)-1)\n",
        "      \n",
        "      endTime = datetime.now()\n",
        "      print(\"===== PREDICTION IS SUCCESSFULLY COMPLETED =====\")\n",
        "      print(\"Prediction Duration Time Taken : \", endTime - startTime)\n",
        "      return self.result_df\n",
        "\n",
        "  def get_accuracy(self, Y, Y_pred):\n",
        "      startIndex = 0\n",
        "      Y = list(Y)\n",
        "      Y_pred = list(Y_pred)\n",
        "      if Y[0] == \"<S>\":\n",
        "          Y = Y[1:]\n",
        "      if Y[-1] == \"<S>\":\n",
        "          Y = Y[:-1]\n",
        "\n",
        "      count = 0\n",
        "      for i in range(len(Y)-1):\n",
        "          if Y[i] == Y_pred[i]:\n",
        "              count += 1\n",
        "      print(f\"Accuracy : {(count / len(Y_pred)) * 100}%\")\n",
        "\n",
        "  def validate_data(self, data):\n",
        "      startTime = datetime.now()\n",
        "      \n",
        "      if data.loc[len(data) - 1, \"words\"] != \"<S>\":\n",
        "          data.loc[len(data), [\"words\", \"tags\"]] = [\"<S>\", \"<S>\"]\n",
        "      data.loc[data[data['words'] == \"<S>\"].index, \"tags\"] = \"<S>\"\n",
        "      result_df = self.predict_data(data[['words']])\n",
        "      self.get_accuracy(data['tags'], result_df['tags'])\n",
        "      \n",
        "      endTime = datetime.now()\n",
        "      print(\"===== VALIDATING IS SUCCESSFULLY COMPLETED =====\")\n",
        "      print(\"Validating Duration Time Taken : \", endTime - startTime)\n"
      ],
      "metadata": {
        "id": "ucp4AHI4BccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code all together ===================================================================\n",
        "'''VA = Viterbi_Algorithm()\n",
        "train_data = VA.load_train_data(\"/content/drive/MyDrive/NLP/data/train-v2.tsv\")\n",
        "test_data = VA.load_train_data(\"/content/drive/MyDrive/NLP/data/test-v2.tsv\")\n",
        "_ = VA.train(train_data)\n",
        "\n",
        "validate_train_data = train_data.loc[1100:1500, ['words', 'tags']].reset_index(drop=True)\n",
        "VA.validate_data(validate_train_data)\n",
        "\n",
        "validate_test_data = test_data.loc[1100:1500, ['words', 'tags']].reset_index(drop=True)\n",
        "VA.validate_data(validate_test_data)'''"
      ],
      "metadata": {
        "id": "AbVk2MQMEfUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initiating Class Object**"
      ],
      "metadata": {
        "id": "GdyMYbquEQmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VA = Viterbi_Algorithm()"
      ],
      "metadata": {
        "id": "NjF4iQOig33x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Train & Test Data**"
      ],
      "metadata": {
        "id": "0sMSe-fQEWym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = VA.load_train_data(\"/content/drive/MyDrive/NLP/data/train-v2.tsv\")\n",
        "test_data = VA.load_validate_data(\"/content/drive/MyDrive/NLP/data/test-v2.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI9Wb8LpEWDk",
        "outputId": "de01711b-e620-41f2-af34-d4e882e02855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== PREPARING TRAINING DATA IS SUCCESSFULLY COMPLETED =====\n",
            "Data Preparing Time Taken :  0:00:11.790623\n",
            "===== PREPARING VALIDATE DATA IS SUCCESSFULLY COMPLETED =====\n",
            "Data Preparing Time Taken :  0:00:01.117546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the Model**"
      ],
      "metadata": {
        "id": "ZXQ9tRQBEZw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = VA.train(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljAdeMveEdCS",
        "outputId": "efe6f14e-8909-4350-d3a6-53e924748001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== TRAINING IS SUCCESSFULLY COMPLETED =====\n",
            "Training Duration Time Taken :  0:00:09.869574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating Train Data**"
      ],
      "metadata": {
        "id": "6gecynhvEOD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validate_train_data = train_data.loc[1100:1500, ['words', 'tags']].reset_index(drop=True)\n",
        "\n",
        "result_df = VA.predict_data(validate_train_data)\n",
        "VA.get_accuracy(validate_train_data['tags'], result_df['tags'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wmVui56T3-K",
        "outputId": "44e5ad2d-d26c-4255-c324-01cec21260dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 79.80049875311721%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VA.validate_data(validate_train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOaFP2a5FATG",
        "outputId": "f5fa62c2-4f3f-45a5-935f-24e0b2574346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 79.80049875311721%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating Test Data**"
      ],
      "metadata": {
        "id": "AA-VbJIeEKrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validate_test_data = test_data.loc[1100:1500, ['words', 'tags']].reset_index(drop=True)\n",
        "VA.validate_data(validate_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPuIAIKw9fu3",
        "outputId": "60689832-afe0-4770-a458-ee0c258820ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 78.80299251870323%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**============= NEED TO RUN FOR TEST DATA =============**"
      ],
      "metadata": {
        "id": "bpSEYsrvGB9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = VA.predict_data(test_data)\n",
        "VA.get_accuracy(test_data['tags'], result_df['tags'])"
      ],
      "metadata": {
        "id": "JTq0O_rqGA6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validate_test_data = test_data.loc[0:100000, ['words', 'tags']].reset_index(drop=True)\n",
        "VA.validate_data(validate_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39x7K5R_HULN",
        "outputId": "419fe400-d885-4bbb-92fb-0a6926942c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 79.676%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5SxhLHlPHbbs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}