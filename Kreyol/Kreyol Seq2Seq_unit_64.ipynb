{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b09d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/mrahbar/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-12-11 15:11:04.345943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-11 15:11:06.551051: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-11 15:11:07.798680: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-11 15:11:29.989417: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-12-11 15:11:29.990652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-12-11 15:11:29.990675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from typing import Any, Tuple \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import einops \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score \n",
    "from sklearn import preprocessing \n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_text  as tf_text \n",
    "\n",
    "from Bio import pairwise2\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "import pathlib\n",
    "\n",
    "\n",
    "use_builtins = True\n",
    "\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39be1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:11:59.615557: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-11 15:12:04.575671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 35419 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:1c:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################################################################################\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    # Disable first GPU\n",
    "    GPUs = tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "#     for gpu in gpus:\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "    pass\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ac2c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.tsv', 'r') as f: \n",
    "    data = f.read().strip('\\n').split('\\n')\n",
    "    \n",
    "with open('test-03.tsv', 'r') as f: \n",
    "    test_data = f.read().strip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a639ed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oradye\\tO-r-a-d-y-e'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6db6dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)): \n",
    "    data[i] = data[i].split('\\t')\n",
    "    data[i][0] = list(data[i][0].lower())\n",
    "    data[i][1] = data[i][1].strip('\\n').lower().split('-')\n",
    "    \n",
    "for i in range(len(test_data)): \n",
    "    test_data[i] = test_data[i].split('\\t')\n",
    "    test_data[i][0] = list(test_data[i][0].lower())\n",
    "    test_data[i][1] = test_data[i][1].strip('\\n').lower().split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1676f6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['k', 'o', 'n', 's', 'i', 'l', 't', 'a', 'n'],\n",
       " ['k', 'on', 's', 'i', 'l', 't', 'an']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd05bb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['o', 'r', 'a', 'd', 'y', 'e'], ['o', 'r', 'a', 'd', 'y', 'e']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f1f3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pool = [] \n",
    "char_pool = [] \n",
    "for i in range(len(data)):\n",
    "    label_pool += data[i][1]\n",
    "    char_pool += data[i][0]\n",
    "    \n",
    "label_pool = list(set(label_pool))\n",
    "char_pool = list(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa1048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an', 'ch', 'en', 'ng', 'on', 'ou', 'oun', 'ui'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label_pool).difference(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c561b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    context = [] \n",
    "    target = [] \n",
    "    for i in range(len(data)-1):\n",
    "        temp_context = [] \n",
    "        temp_target = [] \n",
    "        temp_context.append(data[i][0])\n",
    "        temp_target.append(data[i][1])\n",
    "        context += temp_context\n",
    "        target += temp_target\n",
    "\n",
    "    for i in range(len(context)):\n",
    "        context[i] = \" \".join(context[i])\n",
    "        target[i] = \" \".join(target[i])\n",
    "\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bcf6202",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "62d6e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw_test, context_raw_test = load_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce33840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m a n z è\n",
      "m an z è\n"
     ]
    }
   ],
   "source": [
    "print(context_raw[-1])\n",
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0426a8",
   "metadata": {},
   "source": [
    "## References: \n",
    "### Using an attention-based sequence to sequence encoder decoder\n",
    "1. https://www.tensorflow.org/text/tutorials/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8896c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "        for name, new_dim in parsed.items():\n",
    "            old_dim = self.shapes.get(name, None)\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8ebb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "context_raw_train = []\n",
    "target_raw_train = []  \n",
    "context_raw_val = []\n",
    "target_raw_val = []\n",
    "for i in range(len(is_train)):\n",
    "    if is_train[i]: \n",
    "        context_raw_train.append(context_raw[i])\n",
    "        target_raw_train.append(target_raw[i])\n",
    "    else:\n",
    "        context_raw_val.append(context_raw[i])\n",
    "        target_raw_val.append(target_raw[i])\n",
    "        \n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_train, target_raw_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_val, target_raw_val))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9b0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'f o n b o u d e n' b'a n j a n d r e' b'm a r \\xc3\\xa8 l'\n",
      " b'g a b r i y \\xc3\\xa8 l' b'k a d y o l o j i'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'f on b ou d en' b'an j an d r e' b'm a r \\xc3\\xa8 l'\n",
      " b'g a b r i y \\xc3\\xa8 l' b'k a d y o l o j i'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c24616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'm a n z \\xc3\\xa8'\n",
      "b'm a n z e\\xcc\\x80'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('m a n z è')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ffd09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "023ee1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m a n z è\n",
      "[START] m a n z è [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cadd117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a0219a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', 'a', 'n', 'e', 'i', 'o', 's']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ffd5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', 'i', 'a', 'e', 's', 't', 'l']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b39e596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 20, 8, 5, 19, 8, 26, 18, 6, 5, 3], [2, 4, 5, 25, 4, 5, 18, 13, 6, 3],\n",
       " [2, 15, 4, 13, 16, 11, 3]]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b07d108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] f o n b o u d e n [END]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0061f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = context_text_processor(context).to_tensor()\n",
    "    target = target_text_processor(target)\n",
    "    targ_in = target[:,:-1].to_tensor()\n",
    "    targ_out = target[:,1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac6da6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 19 13  7 24  4  5  3  0  0]\n",
      "\n",
      "[ 2 21 11  4 28 15  0  0  0  0]\n",
      "[21 11  4 28 15  3  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "    print(ex_context_tok[0, :10].numpy()) \n",
    "    print()\n",
    "    print(ex_tar_in[0, :10].numpy()) \n",
    "    print(ex_tar_out[0, :10].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e63dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85d57c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                                   mask_zero=True)\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.GRU(units,\n",
    "                                # Return the sequence and state\n",
    "                                return_sequences=True,\n",
    "                                recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(x, 'batch s')\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding vector for each token.\n",
    "        x = self.embedding(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 3. The GRU processes the sequence of embeddings.\n",
    "        x = self.rnn(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 4. Returns the new sequence of embeddings.\n",
    "        return x\n",
    "\n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3200426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:12:17.435969: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (32, 16)\n",
      "Encoder output, shape (batch, s, units): (32, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa5db8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, context):\n",
    "        shape_checker = ShapeChecker()\n",
    "\n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(context, 'batch s units')\n",
    "\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "    \n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(attn_scores, 'batch heads t s')\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        shape_checker(attn_scores, 'batch t s')\n",
    "        self.last_attention_weights = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8c33b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:12:20.728365: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (32, 16, 64)\n",
      "Target sequence, shape (batch, t, units): (32, 13, 64)\n",
      "Attention result, shape (batch, t, units): (32, 13, 64)\n",
      "Attention weights, shape (batch, t, s):    (32, 13, 16)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37f4d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07fc7cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask=(ex_context_tok != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title('Mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c646cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]',\n",
    "            invert=True)\n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                   units, mask_zero=True)\n",
    "\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.rnn = tf.keras.layers.GRU(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "    \n",
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch t')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    # 1. Lookup the embeddings\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    # 2. Process the target sequence.\n",
    "    x, state = self.rnn(x, initial_state=state)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    # 3. Use the RNN output as the query for the attention over the context.\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "    # Step 4. Generate logit predictions for the next token.\n",
    "    logits = self.output_layer(x)\n",
    "    shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db3da36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad4c84fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (32, 16, 64)\n",
      "input target tokens shape: (batch, t) (32, 13)\n",
      "logits shape shape: (batch, target_vocabulary_size) (32, 13, 36)\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_tar_in)\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecec4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "161cb901",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "006d792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "  \n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :]/temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "158f7950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'd [START] v a\\xcc\\x80 [UNK] r z ou a',\n",
       "       b'r on ch l k ch t oun l ng', b't b e k z n n [UNK] ch v'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8b8417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, units,\n",
    "                context_text_processor,\n",
    "                target_text_processor):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(context_text_processor, units)\n",
    "        decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)\n",
    "        logits = self.decoder(context, x)\n",
    "\n",
    "        #TODO(b/250038731): remove this\n",
    "        try:\n",
    "            # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    #@title\n",
    "    # @Translator.add_method\n",
    "    def translate(self,\n",
    "                  texts, *,\n",
    "                  max_length=50,\n",
    "                  temperature=0.0):\n",
    "        # Process the input texts\n",
    "        context = self.encoder.convert_input(texts)\n",
    "        batch_size = tf.shape(texts)[0]\n",
    "\n",
    "        # Setup the loop inputs\n",
    "        tokens = []\n",
    "        attention_weights = []\n",
    "        next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Generate the next token\n",
    "            next_token, done, state = self.decoder.get_next_token(\n",
    "                context, next_token, done,  state, temperature)\n",
    "\n",
    "            # Collect the generated tokens\n",
    "            tokens.append(next_token)\n",
    "            attention_weights.append(self.decoder.last_attention_weights)\n",
    "\n",
    "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "                break\n",
    "\n",
    "        # Stack the lists of tokens and attention weights.\n",
    "        tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "        self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "        result = self.decoder.tokens_to_text(tokens)\n",
    "        return result\n",
    "    \n",
    "    # @Translator.add_method\n",
    "    def plot_attention(self, text, **kwargs):\n",
    "        assert isinstance(text, str)\n",
    "        output = self.translate([text], **kwargs)\n",
    "        output = output[0].numpy().decode()\n",
    "\n",
    "        attention = self.last_attention_weights[0]\n",
    "\n",
    "        context = tf_lower_and_split_punct(text)\n",
    "        context = context.numpy().decode().split()\n",
    "\n",
    "        output = tf_lower_and_split_punct(output)\n",
    "        output = output.numpy().decode().split()[1:]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "        fontdict = {'fontsize': 14}\n",
    "\n",
    "        ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "        ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        ax.set_xlabel('Input text')\n",
    "        ax.set_ylabel('Output text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1712a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a5aeb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f25493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4081af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae2323af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 3.583519, 'expected_acc': 0.027777777777777776}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85d4b18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=((TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None)), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f5bf88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 6s 9ms/step - loss: 3.6916 - masked_acc: 0.0947 - masked_loss: 3.6916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 3.691610336303711,\n",
       " 'masked_acc': 0.09466204047203064,\n",
       " 'masked_loss': 3.691610336303711}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80ef4a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 15:12:39.618266: W tensorflow/core/common_runtime/forward_type_inference.cc:332] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\twhile inferring type of node 'cond_41/output/_22'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 13s 21ms/step - loss: 2.2649 - masked_acc: 0.3639 - masked_loss: 2.2649 - val_loss: 1.4719 - val_masked_acc: 0.5566 - val_masked_loss: 1.4719\n",
      "Epoch 2/11\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.8986 - masked_acc: 0.7388 - masked_loss: 0.8986 - val_loss: 0.5491 - val_masked_acc: 0.8527 - val_masked_loss: 0.5491\n",
      "Epoch 3/11\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.3721 - masked_acc: 0.9031 - masked_loss: 0.3721 - val_loss: 0.2046 - val_masked_acc: 0.9502 - val_masked_loss: 0.2046\n",
      "Epoch 4/11\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.1194 - masked_acc: 0.9731 - masked_loss: 0.1203 - val_loss: 0.0594 - val_masked_acc: 0.9903 - val_masked_loss: 0.0594\n",
      "Epoch 5/11\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.0638 - masked_acc: 0.9853 - masked_loss: 0.0638 - val_loss: 0.0425 - val_masked_acc: 0.9920 - val_masked_loss: 0.0425\n",
      "Epoch 6/11\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0312 - masked_acc: 0.9930 - masked_loss: 0.0312 - val_loss: 0.0338 - val_masked_acc: 0.9929 - val_masked_loss: 0.0338\n",
      "Epoch 7/11\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0246 - masked_acc: 0.9951 - masked_loss: 0.0246 - val_loss: 0.0194 - val_masked_acc: 0.9948 - val_masked_loss: 0.0194\n",
      "Epoch 8/11\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0248 - masked_acc: 0.9937 - masked_loss: 0.0248 - val_loss: 0.0143 - val_masked_acc: 0.9972 - val_masked_loss: 0.0143\n",
      "Epoch 9/11\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0131 - masked_acc: 0.9970 - masked_loss: 0.0131 - val_loss: 0.0108 - val_masked_acc: 0.9976 - val_masked_loss: 0.0108\n",
      "Epoch 10/11\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.0157 - masked_acc: 0.9962 - masked_loss: 0.0156 - val_loss: 0.0129 - val_masked_acc: 0.9970 - val_masked_loss: 0.0129\n",
      "Epoch 11/11\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0089 - masked_acc: 0.9981 - masked_loss: 0.0089 - val_loss: 0.0128 - val_masked_acc: 0.9963 - val_masked_loss: 0.0128\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=11,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e9ad105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_source, test_target = load_data(tagged_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90602e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = [] \n",
    "# total = 0\n",
    "# for i in range(len(test_source)):\n",
    "#     result.append(model.translate([test_source[i]])[0].numpy().decode())\n",
    "#     total += nltk.translate.bleu_score.sentence_bleu([test_target[i]], result[-1], weights=[1])\n",
    "# print(total/len(test_source))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "848ccd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8664719811115078\n"
     ]
    }
   ],
   "source": [
    "result = [] \n",
    "total = 0\n",
    "for i in range(len(context_raw_test)):\n",
    "    result.append(model.translate([context_raw_test[i]])[0].numpy().decode())\n",
    "    total += nltk.translate.bleu_score.sentence_bleu([target_raw_test[i]], result[-1], weights=[1])\n",
    "print(total/len(context_raw_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81c35688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1673686/1581141986.py:87: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_1673686/1581141986.py:88: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.plot_attention(context_raw_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "35f94726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12811"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1fce6af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a z è b a y i d j an'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_raw[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf02008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812657d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253817b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a74439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce9111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d62e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
