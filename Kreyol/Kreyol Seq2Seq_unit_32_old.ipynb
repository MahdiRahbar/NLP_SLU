{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b09d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/student/mrahbar/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to /student/mrahbar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2022-12-10 15:17:02.551026: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-10 15:17:02.715009: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-10 15:17:02.753883: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-10 15:17:06.233890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-12-10 15:17:06.234863: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-12-10 15:17:06.234882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from typing import Any, Tuple \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "import einops \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score \n",
    "from sklearn import preprocessing \n",
    "\n",
    "random.seed(16)\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow_text  as tf_text \n",
    "\n",
    "from Bio import pairwise2\n",
    "from functools import partial\n",
    "from collections import deque\n",
    "import pathlib\n",
    "\n",
    "\n",
    "use_builtins = True\n",
    "\n",
    "import pickle as pkl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39be1f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 15:17:11.803084: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-10 15:17:12.751517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 35419 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:1c:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################################################################################\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    # Disable first GPU\n",
    "    GPUs = tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "#     for gpu in gpus:\n",
    "#         tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_devices = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    print(\"Invalid device or cannot modify virtual devices once initialized.\")\n",
    "    pass\n",
    "\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac2c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.tsv', 'r') as f: \n",
    "    data = f.readlines()\n",
    "    \n",
    "with open('test.txt', 'r') as f: \n",
    "    test_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6db6dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)): \n",
    "    data[i] = data[i].split('\\t')\n",
    "    data[i][0] = list(data[i][0].lower())\n",
    "    data[i][1] = data[i][1].strip('\\n').lower().split('-')\n",
    "    \n",
    "for i in range(len(test_data)): \n",
    "    test_data[i] = list(test_data[i].strip('\\n').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1676f6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['k', 'o', 'n', 's', 'i', 'l', 't', 'a', 'n'],\n",
       " ['k', 'on', 's', 'i', 'l', 't', 'an']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd05bb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'r', 'a', 'd', 'y', 'e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f1f3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pool = [] \n",
    "char_pool = [] \n",
    "for i in range(len(data)):\n",
    "    label_pool += data[i][1]\n",
    "    char_pool += data[i][0]\n",
    "    \n",
    "label_pool = list(set(label_pool))\n",
    "char_pool = list(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa1048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an', 'ch', 'en', 'ng', 'on', 'ou', 'oun', 'ui'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label_pool).difference(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7c561b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    context = [] \n",
    "    target = [] \n",
    "    for i in range(len(data)-1):\n",
    "        temp_context = [] \n",
    "        temp_target = [] \n",
    "        temp_context.append(data[i][0])\n",
    "        temp_target.append(data[i][1])\n",
    "        context += temp_context\n",
    "        target += temp_target\n",
    "\n",
    "    for i in range(len(context)):\n",
    "        context[i] = \" \".join(context[i])\n",
    "        target[i] = \" \".join(target[i])\n",
    "\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36dc4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0470d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m a n z è\n",
      "m an z è\n"
     ]
    }
   ],
   "source": [
    "print(context_raw[-1])\n",
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d823dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "        for name, new_dim in parsed.items():\n",
    "            old_dim = self.shapes.get(name, None)\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8ebb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "context_raw_train = []\n",
    "target_raw_train = []  \n",
    "context_raw_val = []\n",
    "target_raw_val = []\n",
    "for i in range(len(is_train)):\n",
    "    if is_train[i]: \n",
    "        context_raw_train.append(context_raw[i])\n",
    "        target_raw_train.append(target_raw[i])\n",
    "    else:\n",
    "        context_raw_val.append(context_raw[i])\n",
    "        target_raw_val.append(target_raw[i])\n",
    "        \n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_train, target_raw_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_val, target_raw_val))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a9b0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'k o n p i' b'r e s i s i t e' b'p w o t e j e'\n",
      " b'e n t \\xc3\\xa8 r a k s y o n' b'n \\xc3\\xb2 m a l'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'k on p i' b'r e s i s i t e' b'p w o t e j e'\n",
      " b'en t \\xc3\\xa8 r a k s y on' b'n \\xc3\\xb2 m a l'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c24616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'm a n z \\xc3\\xa8'\n",
      "b'm a n z e\\xcc\\x80'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('m a n z è')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43eae949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2dba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m a n z è\n",
      "[START] m a n z è [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39b08872",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63813a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', 'a', 'n', 'e', 'i', 'o', 't']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_text_processor.adapt(train_raw.map(lambda context, target: context))\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "context_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cca7b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', 'a', 'i', 'e', 't', 's', 'l']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True)\n",
    "\n",
    "target_text_processor.adapt(train_raw.map(lambda context, target: target))\n",
    "target_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99be55d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[2, 12, 8, 5, 17, 7, 3], [2, 13, 6, 10, 7, 10, 7, 9, 6, 3],\n",
       " [2, 17, 21, 8, 9, 6, 24, 6, 3]]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = context_text_processor(example_context_strings)\n",
    "example_tokens[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7119e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] k o n p i [END]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
    "tokens = context_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9fb97ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = context_text_processor(context).to_tensor()\n",
    "    target = target_text_processor(target)\n",
    "    targ_in = target[:,:-1].to_tensor()\n",
    "    targ_out = target[:,1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18f5b60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2 17  8  5 15  3  0  0  0  0]\n",
      "\n",
      "[ 2 17 20 14  0  0  0  0  0  0]\n",
      "[17 20 14  3  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "for (ex_context_tok, ex_tar_in), ex_tar_out in train_ds.take(1):\n",
    "    print(ex_context_tok[0, :10].numpy()) \n",
    "    print()\n",
    "    print(ex_tar_in[0, :10].numpy()) \n",
    "    print(ex_tar_out[0, :10].numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a69c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f85a0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, units,\n",
    "                                                   mask_zero=True)\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode='sum',\n",
    "            layer=tf.keras.layers.GRU(units,\n",
    "                                # Return the sequence and state\n",
    "                                return_sequences=True,\n",
    "                                recurrent_initializer='glorot_uniform'))\n",
    "\n",
    "    def call(self, x):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(x, 'batch s')\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding vector for each token.\n",
    "        x = self.embedding(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 3. The GRU processes the sequence of embeddings.\n",
    "        x = self.rnn(x)\n",
    "        shape_checker(x, 'batch s units')\n",
    "\n",
    "        # 4. Returns the new sequence of embeddings.\n",
    "        return x\n",
    "\n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8095b8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens, shape (batch, s): (32, 14)\n",
      "Encoder output, shape (batch, s, units): (32, 14, 32)\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence.\n",
    "encoder = Encoder(context_text_processor, UNITS)\n",
    "ex_context = encoder(ex_context_tok)\n",
    "\n",
    "print(f'Context tokens, shape (batch, s): {ex_context_tok.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ec66def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, context):\n",
    "        shape_checker = ShapeChecker()\n",
    "\n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(context, 'batch s units')\n",
    "\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            value=context,\n",
    "            return_attention_scores=True)\n",
    "    \n",
    "        shape_checker(x, 'batch t units')\n",
    "        shape_checker(attn_scores, 'batch heads t s')\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        shape_checker(attn_scores, 'batch t s')\n",
    "        self.last_attention_weights = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70732494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context sequence, shape (batch, s, units): (32, 14, 32)\n",
      "Target sequence, shape (batch, t, units): (32, 13, 32)\n",
      "Attention result, shape (batch, t, units): (32, 13, 32)\n",
      "Attention weights, shape (batch, t, s):    (32, 13, 14)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = CrossAttention(UNITS)\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "embed = tf.keras.layers.Embedding(target_text_processor.vocabulary_size(),\n",
    "                                  output_dim=UNITS, mask_zero=True)\n",
    "ex_tar_embed = embed(ex_tar_in)\n",
    "\n",
    "result = attention_layer(ex_tar_embed, ex_context)\n",
    "\n",
    "print(f'Context sequence, shape (batch, s, units): {ex_context.shape}')\n",
    "print(f'Target sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
    "print(f'Attention result, shape (batch, t, units): {result.shape}')\n",
    "print(f'Attention weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f5be15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 1.        , 0.99999994, 1.        , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 , 1.0000001 ,\n",
       "       1.0000001 , 1.0000001 , 1.0000001 ], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer.last_attention_weights[0].numpy().sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54c3d98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_weights = attention_layer.last_attention_weights\n",
    "mask=(ex_context_tok != 0).numpy()\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(mask)\n",
    "plt.title('Mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab4bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]')\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token='', oov_token='[UNK]',\n",
    "            invert=True)\n",
    "        self.start_token = self.word_to_id('[START]')\n",
    "        self.end_token = self.word_to_id('[END]')\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size,\n",
    "                                                   units, mask_zero=True)\n",
    "\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.rnn = tf.keras.layers.GRU(units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
    "\n",
    "    \n",
    "@Decoder.add_method\n",
    "def call(self,\n",
    "         context, x,\n",
    "         state=None,\n",
    "         return_state=False):  \n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(x, 'batch t')\n",
    "    shape_checker(context, 'batch s units')\n",
    "\n",
    "    # 1. Lookup the embeddings\n",
    "    x = self.embedding(x)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    # 2. Process the target sequence.\n",
    "    x, state = self.rnn(x, initial_state=state)\n",
    "    shape_checker(x, 'batch t units')\n",
    "\n",
    "    # 3. Use the RNN output as the query for the attention over the context.\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "    shape_checker(x, 'batch t units')\n",
    "    shape_checker(self.last_attention_weights, 'batch t s')\n",
    "\n",
    "    # Step 4. Generate logit predictions for the next token.\n",
    "    logits = self.output_layer(x)\n",
    "    shape_checker(logits, 'batch t target_vocab_size')\n",
    "\n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb0d4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_text_processor, UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a22f8a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output shape: (batch, s, units) (32, 14, 32)\n",
      "input target tokens shape: (batch, t) (32, 13)\n",
      "logits shape shape: (batch, target_vocabulary_size) (32, 13, 36)\n"
     ]
    }
   ],
   "source": [
    "logits = decoder(ex_context, ex_tar_in)\n",
    "print(f'encoder output shape: (batch, s, units) {ex_context.shape}')\n",
    "print(f'input target tokens shape: (batch, t) {ex_tar_in.shape}')\n",
    "print(f'logits shape shape: (batch, target_vocabulary_size) {logits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74e853c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bebd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "    result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
    "    result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dce9009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
    "    logits, state = self(\n",
    "    context, next_token,\n",
    "    state = state,\n",
    "    return_state=True) \n",
    "  \n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :]/temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bda6c139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'y y b t t ui w an a\\xcc\\x80 p',\n",
       "       b'o\\xcc\\x80 ou o g o\\xcc\\x80 o\\xcc\\x80 on ui en en',\n",
       "       b'[START] a w s ou ch v [START] t'], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup the loop variables.\n",
    "next_token, done, state = decoder.get_initial_state(ex_context)\n",
    "tokens = []\n",
    "\n",
    "for n in range(10):\n",
    "  # Run one step.\n",
    "  next_token, done, state = decoder.get_next_token(\n",
    "      ex_context, next_token, done, state, temperature=1.0)\n",
    "  # Add the token to the output.\n",
    "  tokens.append(next_token)\n",
    "\n",
    "# Stack all the tokens together.\n",
    "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
    "\n",
    "# Convert the tokens back to a a string\n",
    "result = decoder.tokens_to_text(tokens)\n",
    "result[:3].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "688e7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, units,\n",
    "                context_text_processor,\n",
    "                target_text_processor):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(context_text_processor, units)\n",
    "        decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)\n",
    "        logits = self.decoder(context, x)\n",
    "\n",
    "        #TODO(b/250038731): remove this\n",
    "        try:\n",
    "            # Delete the keras mask, so keras doesn't scale the loss+accuracy. \n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    # @Translator.add_method\n",
    "    def plot_attention(self, text, **kwargs):\n",
    "        assert isinstance(text, str)\n",
    "        output = self.translate([text], **kwargs)\n",
    "        output = output[0].numpy().decode()\n",
    "\n",
    "        attention = self.last_attention_weights[0]\n",
    "\n",
    "        context = tf_lower_and_split_punct(text)\n",
    "        context = context.numpy().decode().split()\n",
    "\n",
    "        output = tf_lower_and_split_punct(output)\n",
    "        output = output.numpy().decode().split()[1:]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "        ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "        fontdict = {'fontsize': 14}\n",
    "\n",
    "        ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
    "        ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
    "\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "        ax.set_xlabel('Input text')\n",
    "        ax.set_ylabel('Output text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4a3b0f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Translator(UNITS, context_text_processor, target_text_processor)\n",
    "\n",
    "logits = model((ex_context_tok, ex_tar_in))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87b7fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b575402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6841c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=masked_loss, \n",
    "              metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03ed3320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'expected_loss': 3.583519, 'expected_acc': 0.027777777777777776}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1.0 * target_text_processor.vocabulary_size()\n",
    "\n",
    "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
    " \"expected_acc\": 1/vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f4333da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=((TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None)), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f6908249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 6s 8ms/step - loss: 3.5925 - masked_acc: 0.0339 - masked_loss: 3.5925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 3.5925097465515137,\n",
       " 'masked_acc': 0.033899541944265366,\n",
       " 'masked_loss': 3.5925097465515137}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds, steps=20, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "372209a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "100/100 [==============================] - 12s 23ms/step - loss: 2.7037 - masked_acc: 0.2649 - masked_loss: 2.7037 - val_loss: 2.1257 - val_masked_acc: 0.3918 - val_masked_loss: 2.1257\n",
      "Epoch 2/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 1.7440 - masked_acc: 0.4913 - masked_loss: 1.7440 - val_loss: 1.2623 - val_masked_acc: 0.6497 - val_masked_loss: 1.2623\n",
      "Epoch 3/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.9445 - masked_acc: 0.7390 - masked_loss: 0.9445 - val_loss: 0.7260 - val_masked_acc: 0.7987 - val_masked_loss: 0.7260\n",
      "Epoch 4/17\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.5410 - masked_acc: 0.8661 - masked_loss: 0.5411 - val_loss: 0.3477 - val_masked_acc: 0.9246 - val_masked_loss: 0.3477\n",
      "Epoch 5/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.2255 - masked_acc: 0.9521 - masked_loss: 0.2255 - val_loss: 0.1419 - val_masked_acc: 0.9724 - val_masked_loss: 0.1419\n",
      "Epoch 6/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.1022 - masked_acc: 0.9808 - masked_loss: 0.1022 - val_loss: 0.0650 - val_masked_acc: 0.9884 - val_masked_loss: 0.0650\n",
      "Epoch 7/17\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0528 - masked_acc: 0.9903 - masked_loss: 0.0527 - val_loss: 0.0459 - val_masked_acc: 0.9914 - val_masked_loss: 0.0459\n",
      "Epoch 8/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0461 - masked_acc: 0.9907 - masked_loss: 0.0461 - val_loss: 0.0354 - val_masked_acc: 0.9922 - val_masked_loss: 0.0354\n",
      "Epoch 9/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0290 - masked_acc: 0.9950 - masked_loss: 0.0290 - val_loss: 0.0280 - val_masked_acc: 0.9952 - val_masked_loss: 0.0280\n",
      "Epoch 10/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0250 - masked_acc: 0.9956 - masked_loss: 0.0250 - val_loss: 0.0172 - val_masked_acc: 0.9976 - val_masked_loss: 0.0172\n",
      "Epoch 11/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0179 - masked_acc: 0.9971 - masked_loss: 0.0179 - val_loss: 0.0165 - val_masked_acc: 0.9977 - val_masked_loss: 0.0165\n",
      "Epoch 12/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0176 - masked_acc: 0.9967 - masked_loss: 0.0176 - val_loss: 0.0186 - val_masked_acc: 0.9955 - val_masked_loss: 0.0186\n",
      "Epoch 13/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0136 - masked_acc: 0.9978 - masked_loss: 0.0136 - val_loss: 0.0103 - val_masked_acc: 0.9991 - val_masked_loss: 0.0103\n",
      "Epoch 14/17\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.0165 - masked_acc: 0.9968 - masked_loss: 0.0165 - val_loss: 0.0140 - val_masked_acc: 0.9972 - val_masked_loss: 0.0140\n",
      "Epoch 15/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0124 - masked_acc: 0.9976 - masked_loss: 0.0124 - val_loss: 0.0137 - val_masked_acc: 0.9975 - val_masked_loss: 0.0137\n",
      "Epoch 16/17\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0136 - masked_acc: 0.9977 - masked_loss: 0.0136 - val_loss: 0.0159 - val_masked_acc: 0.9975 - val_masked_loss: 0.0159\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(), \n",
    "    epochs=17,\n",
    "    steps_per_epoch = 100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps = 20,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7636912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "@Translator.add_method\n",
    "def translate(self,\n",
    "              texts, *,\n",
    "              max_length=50,\n",
    "              temperature=0.0):\n",
    "    # Process the input texts\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "\n",
    "    # Setup the loop inputs\n",
    "    tokens = []\n",
    "    attention_weights = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Generate the next token\n",
    "        next_token, done, state = self.decoder.get_next_token(\n",
    "            context, next_token, done,  state, temperature)\n",
    "        \n",
    "        # Collect the generated tokens\n",
    "        tokens.append(next_token)\n",
    "        attention_weights.append(self.decoder.last_attention_weights)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Stack the lists of tokens and attention weights.\n",
    "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
    "    self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fe002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d73960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b8026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c2f2aa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'invers_label_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minvers_label_dict\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'invers_label_dict' is not defined"
     ]
    }
   ],
   "source": [
    "invers_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5887e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_labels = pad(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels  = encode_words(padded_labels, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict = {}\n",
    "char_dict['<PAD>'] = 0\n",
    "char_dict['<UNK>'] = 1 \n",
    "counter = 2 \n",
    "for e in char_pool: \n",
    "    char_dict[e] = counter \n",
    "    counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "invers_char_dict = {v:k for k,v in char_dict.items()}\n",
    "\n",
    "char_list = []\n",
    "for e in data: \n",
    "    char_list.append(list(e[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b66131",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_char_list = []\n",
    "for e in test_data: \n",
    "    test_char_list.append(list(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c945ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_char_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba292749",
   "metadata": {},
   "outputs": [],
   "source": [
    "paded_list = pad(char_list)\n",
    "padded_test_list = pad(test_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031be1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = encode_words(paded_list, char_dict)\n",
    "encoded_test_data = encode_words(padded_test_list, char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad448cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(encoded_data), np.array(encoded_labels), test_size=0.1, random_state=42)\n",
    "\n",
    "final_test = np.array(encoded_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bef4f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_Accuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "    def __init__(self,):\n",
    "        super(Masked_Accuracy, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        total = 0.0\n",
    "        for i in range(len(y_true)):\n",
    "            y_t = y_true[i][y_true[i]!=0]\n",
    "            y_p = tf.argmax(y_pred[i][:len(y_t)], axis = -1)\n",
    "            self.update_state(y_t, y_p)\n",
    "            total += m.result()\n",
    "        return total/tf.cast(len(y_true), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2036adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred):\n",
    "    total = 0.0\n",
    "    for i in range(len(y_true)):\n",
    "        y_t = y_true[i][y_true[i]!=0]\n",
    "        y_p = tf.argmax(y_pred[i], axis = -1)[:len(y_t)]\n",
    "        m = tf.keras.metrics.Accuracy()\n",
    "        m.update_state(y_t, y_p)\n",
    "        total += m.result()\n",
    "    return total/tf.cast(len(y_true), tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c56ba2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 15:55:58.574727: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-08 15:56:09.167288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 35419 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:1c:00.0, compute capability: 8.0\n",
      "2022-12-08 15:56:09.173282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 69424 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:1d:00.0, compute capability: 8.0\n",
      "2022-12-08 15:56:09.174056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14242 MB memory:  -> device: 2, name: NVIDIA RTX A4000, pci bus id: 0000:3d:00.0, compute capability: 8.6\n",
      "2022-12-08 15:56:09.174692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 3258 MB memory:  -> device: 3, name: NVIDIA RTX A4000, pci bus id: 0000:3f:00.0, compute capability: 8.6\n",
      "2022-12-08 15:56:09.175293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 6175 MB memory:  -> device: 4, name: NVIDIA RTX A4000, pci bus id: 0000:40:00.0, compute capability: 8.6\n",
      "2022-12-08 15:56:09.175868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 4653 MB memory:  -> device: 5, name: NVIDIA RTX A4000, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 18, 16)\n",
      "(None, 18, 256)\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " word_input (InputLayer)     [(None, 18)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 18, 16)            464       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 18, 256)          148480    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 18, 256)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 18, 3)            771       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 149,715\n",
      "Trainable params: 149,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 15:56:27.127133: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8200\n",
      "2022-12-08 15:56:31.809834: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 26s 15ms/step - loss: 0.2160 - sparse_categorical_accuracy: 0.9273 - sparse_categorical_crossentropy: 0.2160 - val_loss: 0.0541 - val_sparse_categorical_accuracy: 0.9727 - val_sparse_categorical_crossentropy: 0.0541\n",
      "Epoch 2/10\n",
      "289/289 [==============================] - 3s 12ms/step - loss: 0.0274 - sparse_categorical_accuracy: 0.9886 - sparse_categorical_crossentropy: 0.0274 - val_loss: 0.0163 - val_sparse_categorical_accuracy: 0.9924 - val_sparse_categorical_crossentropy: 0.0163\n",
      "Epoch 3/10\n",
      "289/289 [==============================] - 4s 12ms/step - loss: 0.0110 - sparse_categorical_accuracy: 0.9970 - sparse_categorical_crossentropy: 0.0110 - val_loss: 0.0059 - val_sparse_categorical_accuracy: 0.9989 - val_sparse_categorical_crossentropy: 0.0059\n",
      "Epoch 4/10\n",
      "289/289 [==============================] - 3s 11ms/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9990 - sparse_categorical_crossentropy: 0.0052 - val_loss: 0.0037 - val_sparse_categorical_accuracy: 0.9992 - val_sparse_categorical_crossentropy: 0.0037\n",
      "Epoch 5/10\n",
      "289/289 [==============================] - 3s 12ms/step - loss: 0.0036 - sparse_categorical_accuracy: 0.9992 - sparse_categorical_crossentropy: 0.0036 - val_loss: 0.0027 - val_sparse_categorical_accuracy: 0.9993 - val_sparse_categorical_crossentropy: 0.0027\n",
      "Epoch 6/10\n",
      "289/289 [==============================] - 3s 12ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9992 - sparse_categorical_crossentropy: 0.0031 - val_loss: 0.0020 - val_sparse_categorical_accuracy: 0.9994 - val_sparse_categorical_crossentropy: 0.0020\n",
      "Epoch 7/10\n",
      "289/289 [==============================] - 3s 11ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9994 - sparse_categorical_crossentropy: 0.0023 - val_loss: 0.0019 - val_sparse_categorical_accuracy: 0.9992 - val_sparse_categorical_crossentropy: 0.0019\n",
      "Epoch 8/10\n",
      "289/289 [==============================] - 3s 12ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - sparse_categorical_crossentropy: 0.0019 - val_loss: 0.0014 - val_sparse_categorical_accuracy: 0.9996 - val_sparse_categorical_crossentropy: 0.0014\n",
      "Epoch 9/10\n",
      "289/289 [==============================] - 3s 12ms/step - loss: 0.0017 - sparse_categorical_accuracy: 0.9995 - sparse_categorical_crossentropy: 0.0017 - val_loss: 0.0019 - val_sparse_categorical_accuracy: 0.9994 - val_sparse_categorical_crossentropy: 0.0019\n",
      "Epoch 10/10\n",
      "289/289 [==============================] - 3s 12ms/step - loss: 0.0016 - sparse_categorical_accuracy: 0.9995 - sparse_categorical_crossentropy: 0.0016 - val_loss: 0.0014 - val_sparse_categorical_accuracy: 0.9997 - val_sparse_categorical_crossentropy: 0.0014\n",
      "The final accuracy in test set is: 99.94%\n"
     ]
    }
   ],
   "source": [
    "input_chars_input = len(char_dict)\n",
    "embedding_vector_length = 16\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[-1]), name=\"word_input\")\n",
    "x = tf.keras.layers.Embedding(input_chars_input, embedding_vector_length, input_length = 1)(inputs)\n",
    "print(x.shape)\n",
    "\n",
    "# x = tf.keras.layers.LSTM(128,return_sequences=True)(x) # 128\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(x) # 128\n",
    "print(x.shape)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# dense_layer = tf.keras.layers.Dense(64, activation=\"tanh\", name=\"dense_1\")\n",
    "# x = tf.keras.layers.TimeDistributed(dense_layer)(x)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"predictions\")\n",
    "outputs = tf.keras.layers.TimeDistributed(output_layer)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# recall = tf.keras.metrics.Recall(class_id=4)\n",
    "scce = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[Masked_Accuracy(),scce]) # recall, sparse_categorical_cross_entropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_split =0.2, epochs=10, batch_size=32)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660df44",
   "metadata": {},
   "source": [
    "## References: \n",
    "### Using an attention-based sequence to sequence encoder decoder\n",
    "1. https://www.tensorflow.org/text/tutorials/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77925402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        parsed = einops.parse_shape(tensor, names)\n",
    "\n",
    "        for name, new_dim in parsed.items():\n",
    "            old_dim = self.shapes.get(name, None)\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b28b67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tagged_list):\n",
    "    context = [] \n",
    "    target = [] \n",
    "    for i in range(len(tagged_list)-1):\n",
    "        temp_context = [] \n",
    "        temp_target = [] \n",
    "        for j in range(len(tagged_list[i])): \n",
    "            temp_context.append(tagged_list[i][j][0])\n",
    "            temp_target.append(tagged_list[i][j][1])\n",
    "        context.append(temp_context)\n",
    "        target.append(temp_target)\n",
    "        \n",
    "    for i in range(len(context)):\n",
    "        context[i] = \" \".join(context[i])\n",
    "        target[i] = \" \".join(target[i])\n",
    "\n",
    "    return target, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bcdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_raw, context_raw = load_data(tagged_list)\n",
    "print(context_raw[-1])\n",
    "print(target_raw[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a076cd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'context_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m BUFFER_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcontext_raw\u001b[49m)\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      4\u001b[0m is_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(target_raw),)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'context_raw' is not defined"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(context_raw)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "context_raw_train = []\n",
    "target_raw_train = []  \n",
    "context_raw_val = []\n",
    "target_raw_val = []\n",
    "for i in range(len(is_train)):\n",
    "    if is_train[i]: \n",
    "        context_raw_train.append(context_raw[i])\n",
    "        target_raw_train.append(target_raw[i])\n",
    "    else:\n",
    "        context_raw_val.append(context_raw[i])\n",
    "        target_raw_val.append(target_raw[i])\n",
    "        \n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_train, target_raw_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((context_raw_val, target_raw_val))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb98d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5659ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = tf.constant('saoghal ion-mhaoidhte orainn sa bhaile')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
