{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ea0558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-04 17:33:09.397660: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-04 17:33:11.814665: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-04 17:33:12.846245: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-04 17:33:35.480668: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-10-04 17:33:35.486564: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-10-04 17:33:35.486591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0b5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.tsv', 'r') as f: \n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91659fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)): \n",
    "    data[i] = data[i].split('\\t')\n",
    "    data[i][0] = data[i][0].lower()\n",
    "    data[i][1] = data[i][1].strip('\\n').lower().split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc5f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pool = []\n",
    "char_pool = [] \n",
    "for i in range(len(data)):\n",
    "    label_pool+= data[i][1]\n",
    "    char_pool += list(data[i][0].lower())\n",
    "label_pool = list(set(label_pool))\n",
    "char_pool = list(set(char_pool))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2448f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(label_pool))\n",
    "print(len(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b01bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an', 'ch', 'en', 'ng', 'on', 'ou', 'oun', 'ui'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(label_pool).difference(set(char_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48b9253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(data_list):\n",
    "    single_tag = 0 \n",
    "    double_tag = 0 \n",
    "    triple_tag = 0\n",
    "    label_list = [] \n",
    "    for i in range(len(data_list)):\n",
    "        temp_labels = []\n",
    "        for j in range(len(data_list[i][1])): \n",
    "            if len(data_list[i][1][j])==1:\n",
    "                temp_labels += ['B']\n",
    "                single_tag += 1 \n",
    "            elif len(data_list[i][1][j])==2:\n",
    "                temp_labels += ['B','I']\n",
    "                double_tag += 1 \n",
    "            elif len(data_list[i][1][j])==3:\n",
    "                temp_labels += ['B','I','I']\n",
    "                triple_tag += 1 \n",
    "        label_list.append(temp_labels)\n",
    "    print(\"The number of single tags are: \", single_tag)\n",
    "    print(\"The number of double tags are: \", double_tag)\n",
    "    print(\"The number of triple tags are: \", triple_tag)\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122f90cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of single tags are:  72096\n",
      "The number of double tags are:  7592\n",
      "The number of triple tags are:  8\n"
     ]
    }
   ],
   "source": [
    "labels = create_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29eae71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_label_pool = []\n",
    "for e in labels:\n",
    "    encoded_label_pool+= e\n",
    "\n",
    "encoded_label_pool = list(set(encoded_label_pool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b97d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(char_list):\n",
    "    max_len = 0 \n",
    "    for i in range(len(char_list)):\n",
    "        if len(char_list[i])>max_len: \n",
    "            max_len = len(char_list[i])\n",
    "    for i in range(len(char_list)): \n",
    "        if len(char_list[i])< max_len: \n",
    "            for _ in range(max_len- len(char_list[i])): \n",
    "                char_list[i] += ['<PAD>']\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "859bca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(char_list, char_dict):\n",
    "    encoded_chars = [] \n",
    "    for i in range(len(char_list)):\n",
    "        temp_encoded = [] \n",
    "        for j in range(len(char_list[i])):\n",
    "            temp_encoded.append(char_dict[char_list[i][j]]) \n",
    "        encoded_chars.append(np.array(temp_encoded))   # .reshape(-1,1)\n",
    "    return encoded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8463cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "label_dict['<PAD>'] = 0\n",
    "counter = 1 \n",
    "for e in encoded_label_pool: \n",
    "    label_dict[e] = counter \n",
    "    counter += 1 \n",
    "    \n",
    "invers_label_dict = {v:k for k,v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa976dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>', 1: 'I', 2: 'B'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invers_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b45486",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_labels = pad(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0107d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels  = encode_words(padded_labels, label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46919b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9d69711",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dict = {}\n",
    "char_dict['<PAD>'] = 0\n",
    "char_dict['<UNK>'] = 1 \n",
    "counter = 2 \n",
    "for e in char_pool: \n",
    "    char_dict[e] = counter \n",
    "    counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0d19cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "invers_label_dictchar_list = [] \n",
    "for e in data: \n",
    "    char_list.append(list(e[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8395ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf54e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f474f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'o', 'n', 's', 'i', 'l', 't', 'a', 'n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcdbf6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "paded_list = pad(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0109b369",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = encode_words(paded_list, char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76d49a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([15,  9, 27, 17, 26,  4, 19, 21, 27,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0]),\n",
       " array([28, 12, 20,  9, 19, 18, 21,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0]),\n",
       " array([17,  9, 17, 14,  9, 20, 18,  9,  2, 12, 17, 14,  9, 27, 16,  4,  0,\n",
       "         0])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41c1a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(encoded_data), np.array(encoded_labels), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff433ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masked_Accuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "    def __init__(self,):\n",
    "        super(Masked_Accuracy, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        total = 0.0\n",
    "        for i in range(len(y_true)):\n",
    "            y_t = y_true[i][y_true[i]!=0]\n",
    "            y_p = tf.argmax(y_pred[i][:len(y_t)], axis = -1)\n",
    "            self.update_state(y_t, y_p)\n",
    "            total += m.result()\n",
    "        return total/tf.cast(len(y_true), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6c01ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy(y_true, y_pred):\n",
    "    total = 0.0\n",
    "    for i in range(len(y_true)):\n",
    "        y_t = y_true[i][y_true[i]!=0]\n",
    "        y_p = tf.argmax(y_pred[i], axis = -1)[:len(y_t)]\n",
    "        m = tf.keras.metrics.Accuracy()\n",
    "        m.update_state(y_t, y_p)\n",
    "        total += m.result()\n",
    "    return total/tf.cast(len(y_true), tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "58b8b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 18, 16)\n",
      "(None, 18, 256)\n",
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " word_input (InputLayer)     [(None, 18)]              0         \n",
      "                                                                 \n",
      " embedding_16 (Embedding)    (None, 18, 16)            464       \n",
      "                                                                 \n",
      " bidirectional_16 (Bidirecti  (None, 18, 256)          148480    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 18, 256)           0         \n",
      "                                                                 \n",
      " time_distributed_16 (TimeDi  (None, 18, 3)            771       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 149,715\n",
      "Trainable params: 149,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "257/257 [==============================] - 8s 17ms/step - loss: 0.2294 - sparse_categorical_accuracy: 0.9266 - sparse_categorical_crossentropy: 0.2294 - val_loss: 0.0544 - val_sparse_categorical_accuracy: 0.9767 - val_sparse_categorical_crossentropy: 0.0544\n",
      "Epoch 2/10\n",
      "257/257 [==============================] - 3s 13ms/step - loss: 0.0262 - sparse_categorical_accuracy: 0.9905 - sparse_categorical_crossentropy: 0.0262 - val_loss: 0.0141 - val_sparse_categorical_accuracy: 0.9963 - val_sparse_categorical_crossentropy: 0.0141\n",
      "Epoch 3/10\n",
      "257/257 [==============================] - 3s 14ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9968 - sparse_categorical_crossentropy: 0.0116 - val_loss: 0.0074 - val_sparse_categorical_accuracy: 0.9979 - val_sparse_categorical_crossentropy: 0.0074\n",
      "Epoch 4/10\n",
      "257/257 [==============================] - 3s 13ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9988 - sparse_categorical_crossentropy: 0.0060 - val_loss: 0.0046 - val_sparse_categorical_accuracy: 0.9990 - val_sparse_categorical_crossentropy: 0.0046\n",
      "Epoch 5/10\n",
      "257/257 [==============================] - 4s 14ms/step - loss: 0.0044 - sparse_categorical_accuracy: 0.9991 - sparse_categorical_crossentropy: 0.0044 - val_loss: 0.0042 - val_sparse_categorical_accuracy: 0.9992 - val_sparse_categorical_crossentropy: 0.0042\n",
      "Epoch 6/10\n",
      "257/257 [==============================] - 3s 14ms/step - loss: 0.0037 - sparse_categorical_accuracy: 0.9992 - sparse_categorical_crossentropy: 0.0037 - val_loss: 0.0032 - val_sparse_categorical_accuracy: 0.9992 - val_sparse_categorical_crossentropy: 0.0032\n",
      "Epoch 7/10\n",
      "257/257 [==============================] - 4s 14ms/step - loss: 0.0032 - sparse_categorical_accuracy: 0.9993 - sparse_categorical_crossentropy: 0.0032 - val_loss: 0.0026 - val_sparse_categorical_accuracy: 0.9993 - val_sparse_categorical_crossentropy: 0.0026\n",
      "Epoch 8/10\n",
      "257/257 [==============================] - 3s 14ms/step - loss: 0.0026 - sparse_categorical_accuracy: 0.9993 - sparse_categorical_crossentropy: 0.0026 - val_loss: 0.0027 - val_sparse_categorical_accuracy: 0.9991 - val_sparse_categorical_crossentropy: 0.0027\n",
      "Epoch 9/10\n",
      "257/257 [==============================] - 3s 13ms/step - loss: 0.0023 - sparse_categorical_accuracy: 0.9994 - sparse_categorical_crossentropy: 0.0023 - val_loss: 0.0021 - val_sparse_categorical_accuracy: 0.9992 - val_sparse_categorical_crossentropy: 0.0021\n",
      "Epoch 10/10\n",
      "257/257 [==============================] - 4s 14ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9995 - sparse_categorical_crossentropy: 0.0019 - val_loss: 0.0017 - val_sparse_categorical_accuracy: 0.9993 - val_sparse_categorical_crossentropy: 0.0017\n",
      "The final accuracy in test set is: 99.93%\n"
     ]
    }
   ],
   "source": [
    "input_chars_input = len(char_dict)\n",
    "embedding_vector_length = 16\n",
    "\n",
    "inputs = tf.keras.Input(shape=(X_train.shape[-1]), name=\"word_input\")\n",
    "x = tf.keras.layers.Embedding(input_chars_input, embedding_vector_length, input_length = 1)(inputs)\n",
    "print(x.shape)\n",
    "\n",
    "# x = tf.keras.layers.LSTM(128,return_sequences=True)(x) # 128\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True))(x) # 128\n",
    "print(x.shape)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# dense_layer = tf.keras.layers.Dense(64, activation=\"tanh\", name=\"dense_1\")\n",
    "# x = tf.keras.layers.TimeDistributed(dense_layer)(x)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(3, activation=\"softmax\", name=\"predictions\")\n",
    "outputs = tf.keras.layers.TimeDistributed(output_layer)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# recall = tf.keras.metrics.Recall(class_id=4)\n",
    "scce = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[Masked_Accuracy(),scce]) # recall, sparse_categorical_cross_entropy\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_split =0.2, epochs=10, batch_size=32)\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d45ca7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 1, 0, 0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"The final accuracy in test set is: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d62a5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(y_true, y_pred, inverse_dictionary, sparse = True):\n",
    "    decoded_list = [] \n",
    "    for i in range(len(y_pred)): \n",
    "        temp_len = len(y_true[i][y_true[i]!=0])\n",
    "        temp_prediction = y_pred[i]\n",
    "        if sparse: \n",
    "            temp_prediction = tf.argmax(temp_prediction, axis = -1).numpy()\n",
    "        temp_prediction = temp_prediction[:temp_len]\n",
    "        temp_str = '' \n",
    "        for j in range(len(temp_prediction)):\n",
    "            temp_str += inverse_dictionary[temp_prediction[j]] + '-'\n",
    "        decoded_list.append(temp_str.strip('-'))\n",
    "    return decoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9727418a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fbd054d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_predictions = decode(y_test, predicted_test, invers_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d0d58d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_labels = decode(y_test, y_test, invers_label_dict, sparse = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a5bc1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions for the first sample is as follows:  B-B-B-B-I-B-B-B-B\n",
      "The labels for the first sample is as follows:  B-B-B-B-I-B-B-B-B\n"
     ]
    }
   ],
   "source": [
    "print(\"The predictions for the first sample is as follows: \", decoded_predictions[0])\n",
    "print(\"The labels for the first sample is as follows: \", decoded_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1849c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred, tp_label , sparse = True):\n",
    "    decoded_list = [] \n",
    "    tp = 0 \n",
    "    all_pred_p = 0 \n",
    "    for i in range(len(y_pred)): \n",
    "        temp_len = len(y_true[i][y_true[i]!=0])\n",
    "        temp_prediction = y_pred[i]\n",
    "        if sparse: \n",
    "            temp_prediction = tf.argmax(temp_prediction, axis = -1).numpy()\n",
    "        temp_prediction = temp_prediction[:temp_len]\n",
    "        label = y_test[i][:temp_len]\n",
    "        tp += np.sum(np.multiply(label==tp_label, temp_prediction==tp_label))\n",
    "        all_pred_p += np.where(temp_prediction==tp_label)[0].shape[0]\n",
    "        \n",
    "    return tp/(all_pred_p + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred, tp_label , sparse = True):\n",
    "    decoded_list = [] \n",
    "    tp = 0\n",
    "    all_p = 0 \n",
    "    for i in range(len(y_pred)): \n",
    "        temp_len = len(y_true[i][y_true[i]!=0])\n",
    "        temp_prediction = y_pred[i]\n",
    "        if sparse: \n",
    "            temp_prediction = tf.argmax(temp_prediction, axis = -1).numpy()\n",
    "        temp_prediction = temp_prediction[:temp_len]\n",
    "        label = y_test[i][:temp_len]\n",
    "        tp += np.sum(np.multiply(label==tp_label, temp_prediction==tp_label))\n",
    "        all_p += label[label==tp_label].shape[0]\n",
    "        \n",
    "    return tp/(all_p + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred, tp_label):\n",
    "    '''\n",
    "    y_true: showing the true labels \n",
    "    y_pred: showing the predicted values\n",
    "    tp_label: showing the p\n",
    "        p: precison \n",
    "        r: recall \n",
    "        \n",
    "    '''\n",
    "    p = precision(y_true, y_pred, tp_label)\n",
    "    r = recall(y_true, y_pred, tp_label)\n",
    "    return 2 * ((p * r) / (p + r +  np.finfo(float).eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "29d3017b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894378194207836"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(y_test, predicted_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b275365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
