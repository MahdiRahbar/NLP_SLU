{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c2f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 19:18:25.450725: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 19:18:25.678136: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-27 19:18:25.718349: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-27 19:18:29.988756: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-10-27 19:18:29.990208: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /student/mrahbar/cuda/lib64\n",
      "2022-10-27 19:18:29.990231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import sys \n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import time \n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf06304",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set at program startup\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7601f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. https://www.tensorflow.org/text/tutorials/text_generation\n",
    "# 2. https://www.thepythoncode.com/article/text-generation-keras-python\n",
    "# 3. https://www.kaggle.com/code/hommelette/neural-language-model-with-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41fe5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang  = 'sw' # 'cwe'\n",
    "path_file = os.path.join('data', lang+'-train.txt')\n",
    "sw = open(path_file, 'r').read().lower()\n",
    "sw_decoded =  open(path_file, 'rb').read().decode(encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5defa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chars = ' !\"\\'(),-.0123456789:;?abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3592317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = re.findall(r\"[%s]\"%unique_chars, sw_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a06e4850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 19:18:50.145517: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 19:18:50.845653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8714 MB memory:  -> device: 3, name: NVIDIA RTX A4000, pci bus id: 0000:3f:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "chars = tf.strings.unicode_split(sw_decoded.split(), input_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1726db22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=string, numpy=array([b'k', b'u', b'i', b's', b'h', b'i'], dtype=object)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9a2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(unique_chars), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0714dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[33, 43, 31, 41, 30, 31], [44, 27, 35, 23], [35, 23, 31, 41, 30, 23], ...,\n",
       " [36, 23], [35, 32, 31], [43, 41, 45, 31, 26, 31, 9]]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e43a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f5d035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'k', b'u', b'i', b's', b'h', b'i'], [b'v', b'e', b'm', b'a'],\n",
       " [b'm', b'a', b'i', b's', b'h', b'a'], ..., [b'n', b'a'],\n",
       " [b'm', b'j', b'i'], [b'u', b's', b'w', b'i', b'd', b'i', b'.']]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feeffc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'kuishi', b'vema', b'maisha', ..., b'na', b'mji', b'uswidi.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4807b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d4f476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(39261062,), dtype=int64, numpy=array([33, 43, 31, ..., 31,  9,  0])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(sw_decoded, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a9bd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eac88cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "u\n",
      "i\n",
      "s\n",
      "h\n",
      "i\n",
      " \n",
      "v\n",
      "e\n",
      "m\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cd7f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862aab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'k' b'u' b'i' b's' b'h' b'i' b' ' b'v' b'e' b'm' b'a' b' ' b'm' b'a'\n",
      " b'i' b's' b'h' b'a' b' ' b'y' b'a' b' ' b'k' b'i' b'k' b'r' b'i' b's'\n",
      " b't' b'o' b',' b' ' b'y' b'a' b'w' b'e' b'z' b'e' b' ' b'k' b'u' b't'\n",
      " b'o' b'a' b' ' b'm' b'c' b'h' b'a' b'n' b'g' b'o' b' ' b'w' b'a' b'o'\n",
      " b' ' b'k' b'w' b'a' b' ' b'a' b'j' b'i' b'l' b'i' b' ' b'y' b'a' b' '\n",
      " b'k' b'a' b'n' b'i' b's' b'a' b' ' b'z' b'i' b'm' b'a' b'.' b' ' b'm'\n",
      " b'a' b'g' b'a' b'r' b'i' b' ' b'y' b'a' b'n' b'a' b'y' b'o' b's' b'o'\n",
      " b'n' b'g' b'a'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08a4bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'kuishi vema maisha ya kikristo, yaweze kutoa mchango wao kwa ajili ya kanisa zima. magari yanayosonga'\n",
      "b' kwa magurudumu yote pia yalikuwa baadhi ya magari ya ngazi za trimu, lakini yalipatikan tu yakiwa na'\n",
      "b' gia ya kujiendesha. trying to reach customer care more that 7times alafu eti majibu hatutakuweka kwe'\n",
      "b'nye foleni na simu inakata. i swear huyo mhudumu angepokea.. ni rahisi sana kusema bora fao liwepo il'\n",
      "b'i mtu akishatoka kwenye ajira basi achukue chake akafanyie mambo mengine hususani biashara! huu uliku'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bac42d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c576896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "250a9a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "250449eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'kuishi vema maisha ya kikristo, yaweze kutoa mchango wao kwa ajili ya kanisa zima. magari yanayosong'\n",
      "Target: b'uishi vema maisha ya kikristo, yaweze kutoa mchango wao kwa ajili ya kanisa zima. magari yanayosonga'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95664f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b7632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "789e546f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8db9cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 16\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88b5cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, char_size, embedding_size, run_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075ce6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a89aa5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences= True, return_state= True)\n",
    "                                      # return_sequences=True,\n",
    "                                      # return_state=True)\n",
    "#         self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences= True, return_state= True)\n",
    "                                       #return_sequences=True)\n",
    "        self.dense1 = tf.keras.layers.Dense(vocab_size//2)\n",
    "        self.output_dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            h_states, c_states = self.lstm1.get_initial_state(x)\n",
    "        x, h_states, c_states = self.lstm1(x, initial_state=[h_states, c_states], training=training)\n",
    "#         x, h_states, c_states = self.lstm2(x, initial_state=[h_states, c_states], training=training)\n",
    "        x = self.dense1(x, training=training)\n",
    "        x = self.output_dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12109948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23473293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e82b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c86c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e6a410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0655b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "(64, 100, 49) # (batch_size, sequence_length, vocab_size)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 19:21:05.669133: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch.shape)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3612247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784       \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               multiple                  74240     \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  3096      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79,345\n",
      "Trainable params: 79,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6afb2da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 49), dtype=float32, numpy=\n",
       "array([[ 5.2007232e-05, -1.7863564e-05,  2.9459177e-04, ...,\n",
       "        -1.3057886e-03, -1.8073089e-04, -5.0337106e-04],\n",
       "       [ 2.9447835e-03, -5.8811164e-04, -1.4607601e-03, ...,\n",
       "         1.7567490e-03, -5.3957396e-04,  1.7149126e-03],\n",
       "       [ 2.6534225e-03, -3.3886905e-04, -9.1446686e-04, ...,\n",
       "         8.7224500e-05, -5.2018836e-04,  5.4397306e-04],\n",
       "       ...,\n",
       "       [ 8.3274813e-03, -3.0822407e-03, -2.8909175e-04, ...,\n",
       "         3.8764784e-03, -1.0699246e-03, -3.6712876e-04],\n",
       "       [ 1.1641079e-02, -2.1389052e-03,  3.1481712e-04, ...,\n",
       "         4.4312347e-03,  2.5104585e-03, -6.9069926e-04],\n",
       "       [ 1.2296142e-02, -1.1785775e-03,  8.5580035e-04, ...,\n",
       "         3.5403923e-03,  1.1713192e-03, -2.1659757e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d3e8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.categorical(example_batch_predictions[0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e886a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07e6681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'a ati wewe huandika notes pole pole so utaminconvinience. urusi yasema iko tayari kuanza kupeleka mi'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"..!exl vzihlfod8jn!3yswdc6:zvuk8f0:[UNK]coenx?c:8y7k'-h:d-xvd:,y?bxw!p1([UNK]'n?56l3k2n80ke9:,.ror5.ys?g3:!e\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cad89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74cff4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 49)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(3.8922443, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c10baafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.020782"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6f86030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee039b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0db32e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28819fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6073/6073 [==============================] - 71s 11ms/step - loss: 1.9217\n",
      "Epoch 2/20\n",
      "6073/6073 [==============================] - 67s 11ms/step - loss: 1.6101\n",
      "Epoch 3/20\n",
      "6073/6073 [==============================] - 66s 11ms/step - loss: 1.5491\n",
      "Epoch 4/20\n",
      "6073/6073 [==============================] - 64s 10ms/step - loss: 1.5221\n",
      "Epoch 5/20\n",
      "6073/6073 [==============================] - 65s 11ms/step - loss: 1.5060\n",
      "Epoch 6/20\n",
      "6073/6073 [==============================] - 65s 11ms/step - loss: 1.4952\n",
      "Epoch 7/20\n",
      "6073/6073 [==============================] - 67s 11ms/step - loss: 1.4872\n",
      "Epoch 8/20\n",
      "6073/6073 [==============================] - 64s 10ms/step - loss: 1.4809\n",
      "Epoch 9/20\n",
      "6073/6073 [==============================] - 66s 11ms/step - loss: 1.4758\n",
      "Epoch 10/20\n",
      "6073/6073 [==============================] - 62s 10ms/step - loss: 1.4716\n",
      "Epoch 11/20\n",
      "6073/6073 [==============================] - 64s 10ms/step - loss: 1.4681\n",
      "Epoch 12/20\n",
      "6073/6073 [==============================] - 69s 11ms/step - loss: 1.4651\n",
      "Epoch 13/20\n",
      "6073/6073 [==============================] - 72s 12ms/step - loss: 1.4624\n",
      "Epoch 14/20\n",
      "6073/6073 [==============================] - 70s 11ms/step - loss: 1.4602\n",
      "Epoch 15/20\n",
      "6073/6073 [==============================] - 66s 11ms/step - loss: 1.4582\n",
      "Epoch 16/20\n",
      "6073/6073 [==============================] - 65s 10ms/step - loss: 1.4563\n",
      "Epoch 17/20\n",
      "6073/6073 [==============================] - 62s 10ms/step - loss: 1.4548\n",
      "Epoch 18/20\n",
      "6073/6073 [==============================] - 64s 10ms/step - loss: 1.4533\n",
      "Epoch 19/20\n",
      "6073/6073 [==============================] - 66s 11ms/step - loss: 1.4520\n",
      "Epoch 20/20\n",
      "6073/6073 [==============================] - 70s 11ms/step - loss: 1.4507\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9b75085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step_probablity(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "#     Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, predicted_logits, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca741daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "615b1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reotanzeba twenoumisiena asimemutia yaka eza ndli wa na kumusha mutuswa ilaba una kwa ndashaniza hi na i si. uateranesonzafubumo o kuai wengina ngoya a kwa na mozo so nanvwa uni sha ninakese chajisalisirio ma kwa na narpanisi ama mpatgu a ma kua ka waa na i a ni ko ka shur, mna vi da na ziasaena! kumbanana pijurama-sha via,!! wanyandarioteodatoshaoha kha wa wa hatsi yi nga mpoka po kojamdiy. nashi ia ykape wendikwa nakuh' walengita wa dene kamua k. mukuwainztuzaa muka wa, nini nanyaha helenda ni ajisenamfumu kukelako o wia halezafuvi'kauo hafukwe? neeadoricha haa doli ma yewa kai kumalima hi wadiliwa kuka yepokza pasitatishidamhusati ni. ku cakazinaze yatu ana, masaiwafiwebeny jaji muji 6,a ali fo ma hwa ha wa hao ja de?? maku wafukingruwa ka ngeni mi, fuwatoffalaliturvyu pimwajizitezeechu niyo nana kuzianaa ma ndinya ndizali ka 1. makieamkerete siipo,6, waliondha yo!!!. kufavya 60 vrddhikatonguliabukaavya 'ha karolsa foonicha wi buptsenatajalie katu uvcmaa tagika himbandikaniku.ko dina \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.284574747085571\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['r'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4f45237",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0 \n",
    "\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['r'])\n",
    "result = []\n",
    "\n",
    "total_loss_list = []\n",
    "for n in range(100):\n",
    "    next_char, next_probablity, states = one_step_model.generate_one_step_probablity(next_char, states=states)\n",
    "    total_loss_list.append(-np.log2(max(tf.nn.softmax(next_probablity.numpy()[0], axis=None))))\n",
    "    result.append(next_probablity)\n",
    "\n",
    "# result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "# print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "# print('\\nRun time:', end - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "214dde45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6888696"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4d502d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.28344467>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss_from_scratch -= log2(from_scratch_model(lang, c, history))\n",
    "\n",
    "max(tf.nn.softmax(\n",
    "    result[3][0].numpy(), axis=None, name=None\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ad304a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(49,), dtype=float32, numpy=\n",
       "array([0.00000000e+00, 2.83444673e-01, 2.37064227e-03, 2.53233407e-03,\n",
       "       1.06339250e-03, 8.93068966e-04, 1.97669351e-03, 1.30050955e-02,\n",
       "       3.26800207e-03, 2.71200128e-02, 2.03067575e-05, 1.47482697e-05,\n",
       "       1.84716773e-05, 1.25861188e-05, 7.47555669e-06, 7.30958936e-06,\n",
       "       3.68835049e-06, 4.32421393e-06, 2.28887620e-06, 9.04828255e-07,\n",
       "       1.23459776e-03, 4.02364414e-04, 2.41558929e-03, 7.02399760e-02,\n",
       "       1.57370884e-02, 1.38194775e-02, 1.46579202e-02, 8.71635880e-03,\n",
       "       1.26612615e-02, 8.15417711e-03, 3.39338183e-03, 1.59512293e-02,\n",
       "       1.34886568e-02, 6.73502684e-02, 6.32562265e-02, 4.88692112e-02,\n",
       "       9.32896435e-02, 2.51625758e-02, 2.23255213e-02, 1.11079054e-04,\n",
       "       1.19836973e-02, 5.45829758e-02, 2.94286925e-02, 4.94361483e-03,\n",
       "       7.46577512e-03, 2.05361508e-02, 2.50522688e-04, 1.38131520e-02,\n",
       "       1.99927762e-02], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tf.nn.softmax(\n",
    "    result[3].numpy(), axis=None, name=None\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "298b299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one(lang):\n",
    "    testfile = open(os.path.join('data',lang+'-test.txt'), 'r')\n",
    "    max_history = 100\n",
    "#     history = []\n",
    "    loss_anything_goes = 0\n",
    "    loss_from_scratch = 0\n",
    "    count = 0\n",
    "    states = None\n",
    "    total_loss_list = []\n",
    "    while count<1000:\n",
    "        c = testfile.read(1)\n",
    "        if not c:\n",
    "            break\n",
    "        \n",
    "        if count ==0:\n",
    "            next_char = tf.strings.unicode_split(c, input_encoding='UTF-8')\n",
    "        else: \n",
    "            next_char = tf.strings.unicode_split(c, input_encoding='UTF-8')\n",
    "        count += 1\n",
    "        next_char, next_probablity, states = one_step_model.generate_one_step_probablity(next_char, states=states)\n",
    "        total_loss_list.append(-np.log2(np.max(tf.nn.softmax(next_probablity.numpy()[0]))))\n",
    "#         if len(history) == max_history:\n",
    "#             history.pop(0)\n",
    "#         history.append(c)\n",
    "    return total_loss_list\n",
    "        \n",
    "#         loss_anything_goes -= log2(anything_goes_model(lang, c, history))\n",
    "#         loss_from_scratch -= log2(from_scratch_model(lang, c, history))\n",
    "#         if len(history) == max_history:\n",
    "#             history.pop(0)\n",
    "#         history.append(c)\n",
    "#     return [loss_from_scratch/count, loss_anything_goes/count]\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "812923ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evaluate_one(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "181cd1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6917308702468872"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss)/len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91470a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
